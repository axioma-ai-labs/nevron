{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Developer Documentation","text":"<p>Learn how to build your first Nevron, an autonomous AI agent in Python.</p> <p>This framework is designed to be a modular and extensible framework for building autonomous AI agents, which can perform tasks idependently on their own.</p> <p>Follow this documentation to learn how to create your first Nevron, build your own tools &amp; workflows, and integrate with external services.</p>"},{"location":"#overview","title":"Overview","text":"<p>Nevron is an open-source framework that support the development, deployment and management of autonomous AI agents.</p> <p>This framework is built on top of:</p> <ul> <li>Python programming language</li> <li>Q-learning algorithm for decision making</li> <li>State of the art LLM-powered intelligence</li> <li>Modular architecture with planning, feedback, and memory components</li> <li>Integration with external services (Telegram, Twitter, Discord, etc.)</li> <li>Vector-based memory storage using Chroma or Qdrant</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Autonomous Decision Making: Nevron uses Q-learning algorithm for intelligent decision making</li> <li>LLM Integration: Powered by a wide range of Large Language Models (e.g., OpenAI, Anthropic, xAI, etc.)</li> <li>Modular Workflows: Predefined autonomous agent task execution patterns<ul> <li>Analyze signal workflow</li> <li>Research news workflow</li> </ul> </li> <li>Memory Management: Qdrant-based vector storage for context retention</li> <li>External Integrations:<ul> <li>Telegram messaging</li> <li>Twitter interaction</li> <li>News API integration (in progress)</li> <li>Perplexity research integration (in progress)</li> </ul> </li> </ul>"},{"location":"#nevron-vs-other-frameworks","title":"Nevron vs. Other Frameworks","text":""},{"location":"#core-components","title":"Core Components","text":""},{"location":"#1-planning-module","title":"1. Planning Module","text":"<p>Handles decision-making using Q-learning algorithm to determine optimal actions for the agent.</p> <ul> <li>Q-Learning<ul> <li>Uses state-action value mapping for decision making</li> <li>Configurable parameters:<ul> <li>Learning rate (PLANNING_ALPHA)</li> <li>Discount factor (PLANNING_GAMMA) </li> <li>Exploration rate (PLANNING_EPSILON)</li> </ul> </li> </ul> </li> </ul>"},{"location":"#2-memory-module","title":"2. Memory Module","text":"<p>Manages agent's memory using vector storage for efficient context retrieval, which enables the agent to remember and recall previous interactions and events.</p> <ul> <li> <p>Multiple Vector Databases Support</p> <ul> <li>Chroma (default)</li> <li>Qdrant (alternative vector database)</li> </ul> </li> <li> <p>Features</p> <ul> <li>Vector embeddings via OpenAI's text-embedding-3-small model</li> <li>Semantic similarity search</li> <li>Metadata storage for context</li> <li>Configurable collection management</li> </ul> </li> <li> <p>Backend</p> <ul> <li>Abstract memory backend interface</li> <li>Modular backend architecture for optimal performance &amp; customization</li> <li>Async storage and retrieval operations</li> </ul> </li> </ul> <p>More about memory module can be found in the Memory section.</p>"},{"location":"#3-feedback-module","title":"3. Feedback Module","text":"<p>Feedback module is responsible for processing action results and updating the Q-learning model for improved decision making.</p> <ul> <li> <p>Functions</p> <ul> <li>Collects feedback from action execution</li> <li>Evaluates action outcomes</li> <li>Updates Q-learning parameters</li> <li>Maintains feedback history</li> </ul> </li> <li> <p>Integration</p> <ul> <li>Direct integration with Planning Module</li> <li>Performance metrics tracking</li> </ul> </li> </ul> <p>More about feedback module can be found in the Planning section.</p>"},{"location":"#4-tools","title":"4. Tools","text":"<p>Nevron supports integrations with external services and APIs for extended functionality &amp; integrations in diferent platforms.</p> <p>For development purposes, Nevron comes with a set of tools that can be used as a starting point for building your own tools and integrating more complex functionality to your AI agent.</p> <ul> <li> <p>Telegram</p> <ul> <li>Telegram Bot integration</li> <li>Channel/group support</li> <li>HTML message formatting</li> </ul> </li> <li> <p>Twitter</p> <ul> <li>Tweet posting</li> <li>Media handling</li> <li>Thread creation</li> </ul> </li> <li> <p>Research</p> <ul> <li>Perplexity API integration (in progress)</li> <li>News API integration (in progress)</li> </ul> </li> </ul> <p>More about tools can be found in the Tools section.</p>"},{"location":"#5-llm-integration","title":"5. LLM Integration","text":"<p>Powers the agent's intelligence and natural language capabilities.</p> <ul> <li>Supported Providers<ul> <li>OpenAI (primary)</li> <li>gpt-4o for decision making</li> <li>text-embedding-3-small for embeddings</li> <li>Anthropic (alternative)</li> </ul> </li> </ul> <p>More about LLM integration can be found in the LLM section.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For setup and development instructions, please refer to our Quickstart and setup your first AI agent in minutes.</p>"},{"location":"#creators","title":"Creators","text":"<p>Nevron was created by Neurobro team. If you want to learn more about Nevron, our core values &amp; team behind it, please visit About page.</p> <p>Support by upvoting Nevron on Product Hunt.</p> <p> </p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"about/","title":"About Nevron","text":"<p>Nevron is an open-source AI Agent framework developed by Neurobro, dedicated to creating intelligent, modular agents for automation and decision-making.</p> <p>Our mission is to make AI innovation accessible, decentralized, and collaborative to empower individuals and organizations globally. We envision a future where AI is democratized, transparent, and seamlessly integrated into everyday life to drive meaningful impact and sustainable growth.</p> <p>Nevron is part of Neurobro's commitment to open-source decentralized artificial intelligence, fostering innovation through community-driven development.</p> <p>To learn more about our vision, mission &amp; belief, feel free to read our Litepaper.</p> <p>More about our AI Agents can be found here</p>"},{"location":"about/#our-principles","title":"Our Principles","text":"<ul> <li> <p>Accessibility: AI tools designed for everyone, from students to enterprises, ensuring broad adoption and usability.</p> </li> <li> <p>Transparency: Open-source code, processes, and decision-making to build trust and foster collaboration.</p> </li> <li> <p>Sustainability: A balanced approach to fostering innovation while ensuring long-term project growth and community health.</p> </li> <li> <p>Decentralization: Building tools that prioritize autonomy and reduce reliance on centralized entities, in line with the ethos of blockchain and Web3 technologies.</p> </li> <li> <p>Collaboration: Actively engaging with a global community of developers and researchers to advance AI's potential.</p> </li> <li> <p>Ethical AI: Developing AI systems that respect privacy, promote fairness, and align with human values.</p> </li> </ul>"},{"location":"about/#get-in-touch","title":"Get In Touch","text":"<p>Follow us on social media to stay updated on the latest developments and get involved in the community.</p> <ul> <li>Neurobro Website</li> <li>Neurobro on Twitter</li> <li>Neurobro on Telegram</li> <li>Neurobro on YouTube</li> <li>Neurobro on TikTok</li> </ul> <p>Stay updated and contribute to the evolution of AI with Nevron. </p> <p>Together, we can push the boundaries of what's possible \u26a1\ufe0f</p>"},{"location":"deployment/","title":"Deployment","text":"<p>This guide explains how to deploy Nevron using Docker and various cloud platforms.</p>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":"<p>Nevron comes with a pre-configured Dockerfile that allows you to run the agent in a containerized environment. This provides consistency across different deployment environments and makes it easy to deploy the agent anywhere that supports Docker.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed on your system</li> <li><code>.env</code> file with proper configuration</li> <li>Sufficient disk space for logs</li> </ul>"},{"location":"deployment/#building-the-docker-image","title":"Building the Docker Image","text":"<p>To build the Docker image, run the following command from the root directory of the project:</p> <pre><code>docker build -t nevron:latest .\n</code></pre>"},{"location":"deployment/#running-the-container","title":"Running the Container","text":"<p>To run the Nevron container, you need to: 1. Mount the logs directory 2. Provide environment variables</p> <p>Here's the basic command:</p> <pre><code>docker run -d \\\n  --name nevron \\\n  -v $(pwd)/logs:/nevron/logs \\\n  --env-file .env \\\n  nevron:latest\n</code></pre>"},{"location":"deployment/#important-notes","title":"Important Notes:","text":""},{"location":"deployment/#logs-volume","title":"Logs Volume","text":"<ul> <li>The container creates a <code>/nevron/logs</code> directory</li> <li>You should mount this directory to persist logs</li> <li>Example: <code>-v $(pwd)/logs:/nevron/logs</code></li> </ul>"},{"location":"deployment/#environment-variables","title":"Environment Variables","text":"<ul> <li>The container requires a properly configured <code>.env</code> file</li> <li>You can copy <code>.env.dev</code> as a template: <code>cp .env.dev .env</code></li> <li>Make sure to set all required API keys and configurations</li> <li>Pass the env file using <code>--env-file .env</code></li> </ul>"},{"location":"deployment/#container-management","title":"Container Management","text":"<pre><code># Stop the container\ndocker stop nevron\n\n# Start the container\ndocker start nevron\n\n# View logs\ndocker logs -f nevron\n</code></pre>"},{"location":"deployment/#cloud-deployment-options","title":"Cloud Deployment Options","text":"<p>You can deploy the Nevron Docker container to various cloud platforms:</p>"},{"location":"deployment/#aws","title":"AWS","text":"<ol> <li>Push the image to Amazon ECR</li> <li>Deploy using:<ul> <li>ECS (Elastic Container Service)</li> <li>EKS (Elastic Kubernetes Service)</li> <li>EC2 with Docker installed</li> </ul> </li> </ol>"},{"location":"deployment/#google-cloud","title":"Google Cloud","text":"<ol> <li>Push the image to Google Container Registry</li> <li>Deploy using:<ul> <li>Google Kubernetes Engine (GKE)</li> <li>Cloud Run</li> <li>Compute Engine with Docker installed</li> </ul> </li> </ol>"},{"location":"deployment/#azure","title":"Azure","text":"<ol> <li>Push the image to Azure Container Registry</li> <li>Deploy using:<ul> <li>Azure Kubernetes Service (AKS)</li> <li>Azure Container Instances</li> <li>VM with Docker installed</li> </ul> </li> </ol>"},{"location":"deployment/#digital-ocean","title":"Digital Ocean","text":"<ol> <li>Push the image to Digital Ocean Container Registry</li> <li>Deploy using:<ul> <li>Digital Ocean Kubernetes</li> <li>Droplet with Docker installed</li> </ul> </li> </ol>"},{"location":"deployment/#best-practices","title":"Best Practices","text":""},{"location":"deployment/#security","title":"Security","text":"<ul> <li>Never include sensitive data in the Docker image</li> <li>Use environment variables for all sensitive information</li> <li>Follow the Security Policy guidelines</li> </ul>"},{"location":"deployment/#monitoring","title":"Monitoring","text":"<ul> <li>Mount the logs directory to persist logs</li> <li>Consider implementing container monitoring</li> <li>Set up alerts for container health</li> </ul>"},{"location":"deployment/#updates","title":"Updates","text":"<ul> <li>Use specific version tags for production deployments</li> <li>Implement a strategy for updating the container</li> <li>Keep base images updated for security patches</li> </ul>"},{"location":"deployment/#resource-management","title":"Resource Management","text":"<ul> <li>Monitor container resource usage</li> <li>Set appropriate resource limits</li> <li>Scale based on your needs </li> </ul>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will help you get Nevron, your autonomous AI agent, running in 5 simple steps.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>To be able to run the agent, you need to have the following:</p> <ul> <li>Python 3.12</li> <li>Pipenv</li> <li>Docker (optional, for Qdrant memory backend)</li> <li>OpenAI API key (or any other API key of an LLM provider of your choice)</li> </ul>"},{"location":"quickstart/#setup-in-5-steps","title":"Setup in 5 Steps","text":""},{"location":"quickstart/#1-clone-install","title":"1. Clone &amp; Install","text":"<pre><code># clone the repository\ngit clone https://github.com/axioma-ai-labs/nevron.git\ncd nevron\n\n# install dependencies\nmake deps\n</code></pre>"},{"location":"quickstart/#2-configure-environment","title":"2. Configure Environment","text":"<pre><code># copy example environment file\ncp .env.dev .env\n</code></pre> <p>Required environment variables: <pre><code>ENVIRONMENT=development         # Set environment (development or production)\nOPENAI_API_KEY=your_key_here    # Required for embeddings\n</code></pre></p> <p>Note, there're many more environment variables you can set! Please, explore the .env.dev file.</p>"},{"location":"quickstart/#3-choose-memory-backend","title":"3. Choose Memory Backend","text":""},{"location":"quickstart/#option-a-chroma-default","title":"Option A: Chroma (Default)","text":"<p>No additional setup required. Uses local file storage.</p>"},{"location":"quickstart/#option-b-qdrant","title":"Option B: Qdrant","text":"<pre><code># create storage directory\nmkdir qdrant_storage\n\n# run qdrant container\ndocker run -p 6333:6333 -p 6334:6334 \\\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n    qdrant/qdrant\n</code></pre> <p>Update <code>.env</code>: <pre><code>MEMORY_BACKEND_TYPE=qdrant\n</code></pre></p>"},{"location":"quickstart/#4-configure-nevrons-personality","title":"4. Configure Nevron's Personality","text":"<p>Setup the personality, goals and rest time of your agent depending on your needs.</p> <p>In <code>.env</code>: <pre><code>AGENT_PERSONALITY=\"A helpful AI assistant focused on research and analysis\"\nAGENT_GOAL=\"To assist with information gathering and analysis\"\nAGENT_REST_TIME=300  # seconds between actions\n</code></pre></p>"},{"location":"quickstart/#5-run-nevron","title":"5. Run Nevron","text":"<pre><code>make run\n</code></pre>"},{"location":"quickstart/#available-workflows","title":"Available Workflows","text":"<p>Nevron comes with two pre-configured workflows which can be used as a starting point:</p> <ul> <li><code>Analyze signal</code>: Processes and analyzes incoming signal data</li> <li><code>Research news</code>: Gathers and analyzes news using Perplexity API</li> </ul> <p>If you want to create your own workflows, or want to learn more about how the workflows work, please refer to the Workflows documentation.</p>"},{"location":"quickstart/#customization","title":"Customization","text":"<p>For more customization you can add your custom workflows and tools, adjust planning parameters, switch LLM providers, fine-tune the hyper-parameters, etc.</p> <p>Please refer to the Agent for more information on how to customize the agent, its behavior &amp; personality.</p>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Ensure all required API keys are set in <code>.env</code></li> <li>Check logs in the console for detailed error messages</li> <li>Verify Python version: <code>python --version</code></li> <li>Confirm dependencies: <code>pipenv graph</code></li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/feedback/","title":"Feedback Module","text":""},{"location":"agent/feedback/#overview","title":"Overview","text":"<p>The Feedback Module is an integral component designed to collect and process feedback for actions performed by the agent. It enables the system to evaluate outcomes, track performance, and adapt behavior based on historical data. This module is vital for improving the agent's decision-making and maintaining accountability.</p> <p>The Feedback Module is implemented in <code>src/feedback/feedback_module.py</code> and provides structured methods for collecting, storing, and analyzing feedback related to agent actions.</p>"},{"location":"agent/feedback/#how-it-works","title":"How It Works","text":"<p>The Feedback Module operates as follows:</p> <ol> <li> <p>Feedback Collection:    Captures feedback based on the action performed and its outcome, assigning a feedback score to evaluate success or failure.</p> </li> <li> <p>Feedback Storage:    Maintains an internal history of feedback entries, including details about the action, its outcome, and the assigned feedback score.</p> </li> <li> <p>Feedback Retrieval:    Allows querying of recent feedback entries for analysis and monitoring.</p> </li> <li> <p>Feedback Reset:    Provides functionality to clear feedback history, ensuring the module can be reset when needed.</p> </li> </ol>"},{"location":"agent/feedback/#technical-features","title":"Technical Features","text":""},{"location":"agent/feedback/#1-feedback-collection","title":"1. Feedback Collection","text":"<p>The <code>collect_feedback</code> method records feedback for a specific action and its outcome. It assigns a feedback score based on predefined criteria:</p> <ul> <li>Failure: Assigned a score of <code>-1.0</code> if the outcome is <code>None</code>.</li> <li>Success: Assigned a score of <code>1.0</code> for successful outcomes.</li> </ul>"},{"location":"agent/feedback/#implementation","title":"Implementation:","text":"<pre><code>feedback_score = -1.0 if outcome is None else 1.0\n</code></pre> <ul> <li> <p>Inputs:</p> </li> <li> <p><code>action</code> (str): The name of the action performed.</p> </li> <li> <p><code>outcome</code> (Any): The outcome of the action; <code>None</code> for failure or a value indicating success.</p> </li> <li> <p>Output:</p> </li> <li> <p>Returns a feedback score (<code>float</code>) for the action.</p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>feedback_score = feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre>"},{"location":"agent/feedback/#2-feedback-history-retrieval","title":"2. Feedback History Retrieval","text":"<p>The <code>get_feedback_history</code> method retrieves the most recent feedback entries, limited by the specified count.</p>"},{"location":"agent/feedback/#features","title":"Features:","text":"<ul> <li>Enables monitoring of agent performance.</li> <li>Default limit is set to 10 entries.</li> </ul>"},{"location":"agent/feedback/#example","title":"Example:","text":"<pre><code>recent_feedback = feedback_module.get_feedback_history(limit=5)\n</code></pre>"},{"location":"agent/feedback/#3-feedback-reset","title":"3. Feedback Reset","text":"<p>The <code>reset_feedback_history</code> method clears the internal feedback history, resetting the module's state. This is useful for testing or reinitializing feedback tracking.</p>"},{"location":"agent/feedback/#example_1","title":"Example:","text":"<pre><code>feedback_module.reset_feedback_history()\n</code></pre>"},{"location":"agent/feedback/#key-methods","title":"Key Methods","text":""},{"location":"agent/feedback/#collect_feedback","title":"<code>collect_feedback</code>","text":"<p>Captures feedback for a given action and outcome.</p> <p>Arguments:</p> <ul> <li><code>action</code> (str): The action name.</li> <li><code>outcome</code> (Optional[Any]): The outcome of the action.</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Feedback score.</li> </ul>"},{"location":"agent/feedback/#get_feedback_history","title":"<code>get_feedback_history</code>","text":"<p>Retrieves recent feedback entries.</p> <p>Arguments:</p> <ul> <li><code>limit</code> (int): Number of entries to retrieve (default: 10).</li> </ul> <p>Returns:</p> <ul> <li><code>List[Dict[str, Any]]</code>: Recent feedback entries.</li> </ul>"},{"location":"agent/feedback/#reset_feedback_history","title":"<code>reset_feedback_history</code>","text":"<p>Clears the feedback history.</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"agent/feedback/#example-workflow","title":"Example Workflow","text":"<ol> <li>An action is performed by the agent (e.g., <code>fetch_data</code>).</li> <li>The outcome of the action is evaluated, and feedback is collected:    <pre><code>feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre></li> <li>The collected feedback is stored in the feedback history for analysis.</li> <li>The feedback history can be retrieved for review:    <pre><code>recent_feedback = feedback_module.get_feedback_history(limit=5)\n</code></pre></li> <li>Feedback history is reset when needed:    <pre><code>feedback_module.reset_feedback_history()\n</code></pre></li> </ol>"},{"location":"agent/feedback/#benefits","title":"Benefits","text":"<ul> <li>Performance Monitoring: Tracks agent actions and their outcomes, enabling better performance evaluation.</li> <li>Adaptability: Facilitates improvements by learning from historical data.</li> <li>Simplicity: Provides clear and structured methods for feedback collection and retrieval.</li> </ul>"},{"location":"agent/feedback/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Feedback Collection: Ensure feedback is collected for all critical actions to build a comprehensive history.</li> <li>Periodic Reset: Use <code>reset_feedback_history</code> to clear stale feedback during testing or major updates.</li> <li>Logging: Leverage <code>loguru</code> for detailed insights into feedback processing.</li> </ol>"},{"location":"agent/feedback/#known-limitations","title":"Known Limitations","text":"<ul> <li>Scoring Granularity: Feedback scores are currently binary (<code>-1.0</code> or <code>1.0</code>). Future versions can introduce more nuanced scoring.</li> <li>Debugging Notes: Debugging comments in <code>collect_feedback</code> indicate areas for improvement.</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/llm/","title":"LLM Integration","text":"<p>Large Language Models are the backbone of the Autonomous Agent. They are the core component that allows the agent to understand and respond to natural language.</p>"},{"location":"agent/llm/#implementation","title":"Implementation","text":"<p>The LLM integration is primarily handled through the <code>src/llm</code> directory, which provides:</p> <ul> <li>API interaction with OpenAI/Anthropic/xAI models</li> <li>Embeddings generation for memory storage</li> <li>Context management</li> <li>Response processing &amp; generation</li> </ul>"},{"location":"agent/llm/#overview","title":"Overview","text":""},{"location":"agent/llm/#1-embeddings","title":"1. Embeddings","text":"<p>For memory storage, the agent uses OpenAI's embedding models to generate vector representations of the memories. These vectors are then stored in a vector database for efficient retrieval and semantic search. To generate embeddings, the agent uses the <code>src/llm/embeddings.py</code> module.</p> <p>For embeddings generation we recommend using OpenAI's <code>text-embedding-3-large</code> model.</p>"},{"location":"agent/llm/#2-response-processing-generation","title":"2. Response Processing &amp; Generation","text":"<p>The agent uses the LLM class to generate responses through different providers. The <code>src/llm/llm.py</code> module provides:</p> <ul> <li>Unified interface for multiple LLM providers through the <code>LLM</code> class</li> <li>Automatic system message injection with agent personality and goals</li> <li>Async response generation via <code>generate_response()</code> method</li> <li>Support for additional parameters like model and temperature</li> <li>OpenAI client initialization helper via <code>get_oai_client()</code></li> </ul>"},{"location":"agent/llm/#configuration","title":"Configuration","text":"<p>Currently, the agent is configured to use OpenAI's <code>gpt-4o</code> model, but it can be easily configured to use other models (e.g. <code>gpt-4o-mini</code>, <code>gpt-4</code>, Anthropic's models like <code>claude-3-5-sonnet</code> or xAI's <code>grok-2-latest</code> model).</p>"},{"location":"agent/llm/#environment-variables","title":"Environment Variables","text":"<p>To choose model of your choice, set the following environment variables: <pre><code>OPENAI_API_KEY=your-api-key\nOPENAI_MODEL=gpt-4  # or other supported models (e.g. gpt-4o-mini, gpt-4o, claude-3-5-sonnet, grok-2-latest)\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large # or other supported embedding models\n</code></pre></p>"},{"location":"agent/llm/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Token Management    Monitor and track token consumption across API calls. Implement rate limiting mechanisms to prevent exceeding quotas. Establish proper API quota management systems to maintain service availability.</p> </li> <li> <p>Error Handling    Implement graceful fallback mechanisms when API calls fail. Set up automatic retry logic with exponential backoff. Maintain comprehensive error logging to track and debug issues.</p> </li> <li> <p>Cost Optimization    Select appropriate model tiers based on task requirements. Implement response caching for frequently requested prompts. Track and analyze API usage patterns to optimize costs.</p> </li> </ol>"},{"location":"agent/llm/#future-enhancements","title":"Future Enhancements","text":"<p>We're planning to add support for additional LLM providers, advanced prompt engineering, fine-tuning capabilities, and enhanced error handling in the nearest future.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/memory/","title":"Memory Module","text":""},{"location":"agent/memory/#overview","title":"Overview","text":"<p>The Memory Module is a critical component designed to store and retrieve structured memory entries for the agent. It leverages vector embedding and similarity search to maintain a contextual memory, allowing the agent to make decisions based on historical events and outcomes. This module supports flexible backends to meet different scalability and performance needs.</p> <p>The Memory Module is implemented in <code>src/memory/memory_module.py</code> and integrates with vector storage systems like Qdrant and ChromaDB.</p>"},{"location":"agent/memory/#how-it-works","title":"How It Works","text":"<p>The Memory Module operates as follows:</p> <ol> <li> <p>Embedding Generation: Converts textual data into vector embeddings using OpenAI's embedding models.</p> </li> <li> <p>Memory Storage: Stores embeddings and associated metadata in a vector store backend.</p> </li> <li> <p>Memory Retrieval: Performs similarity searches to retrieve related memories based on a query.</p> </li> <li> <p>Backend Flexibility: Supports multiple backends, such as Qdrant and ChromaDB, to accommodate various storage and performance requirements.</p> </li> </ol>"},{"location":"agent/memory/#technical-features","title":"Technical Features","text":""},{"location":"agent/memory/#1-embedding-generation","title":"1. Embedding Generation","text":"<p>The module uses an <code>EmbeddingGenerator</code> to convert textual descriptions (e.g., events, actions, outcomes) into vector embeddings. These embeddings form the basis for similarity searches.</p>"},{"location":"agent/memory/#key-features","title":"Key Features:","text":"<ul> <li>Supports asynchronous embedding generation using OpenAI models.</li> <li>Combines event, action, and outcome data to create meaningful embeddings.</li> </ul>"},{"location":"agent/memory/#example","title":"Example:","text":"<pre><code>embedding = await embedding_generator.get_embedding(\"event action outcome\")\n</code></pre>"},{"location":"agent/memory/#2-memory-storage","title":"2. Memory Storage","text":"<p>The <code>store</code> method saves memory entries in the vector store backend.</p>"},{"location":"agent/memory/#features","title":"Features:","text":"<ul> <li>Combines textual data and embeddings for efficient storage.</li> <li>Includes metadata for additional context.</li> </ul>"},{"location":"agent/memory/#implementation","title":"Implementation:","text":"<pre><code>await memory_module.store(\n    event=\"User Login\",\n    action=\"Verify Credentials\",\n    outcome=\"Success\",\n    metadata={\"user_id\": 1234}\n)\n</code></pre>"},{"location":"agent/memory/#3-memory-search","title":"3. Memory Search","text":"<p>The <code>search</code> method retrieves similar memory entries by performing a vector similarity search.</p>"},{"location":"agent/memory/#features_1","title":"Features:","text":"<ul> <li>Asynchronous search capability.</li> <li>Configurable <code>top_k</code> parameter to control the number of results.</li> </ul>"},{"location":"agent/memory/#example_1","title":"Example:","text":"<pre><code>results = await memory_module.search(query=\"User Login\", top_k=5)\n</code></pre>"},{"location":"agent/memory/#4-backend-flexibility","title":"4. Backend Flexibility","text":"<p>The module supports multiple backends for vector storage (by default Chroma):</p> <ul> <li> <p>Chroma:</p> <ul> <li>Local &amp; lightweight vector store.</li> <li>Uses persistent storage on disk.</li> </ul> </li> <li> <p>Qdrant:</p> <ul> <li>High-performance distributed vector store.</li> <li>Requires configuration for host, port, and vector size.</li> </ul> </li> </ul>"},{"location":"agent/memory/#backend-initialization","title":"Backend Initialization:","text":"<pre><code>backend = QdrantBackend(\n    collection_name=\"memory_collection\",\n    host=\"localhost\",\n    port=6333,\n    vector_size=512\n)\n</code></pre> <p>Please note: If you want to use Qdrant as a memory backend, you need to have Docker installed for running the Qdrant container. Install this software first (follow the official documentation).</p>"},{"location":"agent/memory/#example-workflow","title":"Example Workflow","text":"<ol> <li> <p>Store Memory:    <pre><code>await memory_module.store(\n    event=\"User Registration\",\n    action=\"Send Confirmation Email\",\n    outcome=\"Email Sent\",\n    metadata={\"user_email\": \"example@example.com\"}\n)\n</code></pre></p> </li> <li> <p>Search Memory:    <pre><code>similar_memories = await memory_module.search(\n    query=\"User Registration\",\n    top_k=3\n)\n</code></pre></p> </li> <li> <p>Backend Initialization:    <pre><code>memory_module = get_memory_module(\n    backend_type=\"qdrant\"\n)\n</code></pre></p> </li> </ol>"},{"location":"agent/memory/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Consistent Data Structure:    Ensure all memory entries follow a standardized structure for consistency.</p> </li> <li> <p>Choose the Right Backend:    Use Qdrant for distributed, scalable setups and ChromaDB for lightweight, local use cases.</p> </li> <li> <p>Asynchronous Operations:    Take advantage of asynchronous methods for efficient execution.</p> </li> </ol> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/overview/","title":"Nevron Overview","text":""},{"location":"agent/overview/#architecture","title":"Architecture","text":"<p>Nevron is an autonomous AI agent built with a modular architecture consisting of several key components that work together to enable intelligent decision-making and task execution.</p> <p></p>"},{"location":"agent/overview/#1-planning-module-q-learning","title":"1. Planning Module (Q-Learning)","text":"<p>The Planning Module serves as the decision-making engine, leveraging Q-Learning, a model-free reinforcement learning algorithm. It determines intelligent actions based on the current system state and optimal action planning through learned patterns. The module continuously adapts to new data via feedback loops while maintaining state-action value mappings for consistency.</p>"},{"location":"agent/overview/#2-feedback-module","title":"2. Feedback Module","text":"<p>The Feedback Module bridges planning and execution by processing the outcomes of system actions. It evaluates the outcomes of the actions and updates the Q-learning model to improve decision-making.</p>"},{"location":"agent/overview/#3-workflows","title":"3. Workflows","text":"<p>Workflows act as the execution layer, translating high-level plans into actionable steps. This module manages task sequences and dependencies. It provides standardized operation patterns, maintaining consistency and efficiency throughout the system.</p> <p>Using workflows, Nevron can perform tasks such as signal analysis or news research.</p>"},{"location":"agent/overview/#4-tools","title":"4. Tools","text":"<p>The Tools module represents the operational toolkit, executing workflow tasks via integrations. It interfaces seamlessly with external services and APIs to deliver concrete implementation capabilities. </p> <p>This enables effective real-world interactions such as sending messages to Telegram or Twitter.</p>"},{"location":"agent/overview/#5-memory-module-chroma","title":"5. Memory Module (Chroma)","text":"<p>The Memory Module, powered by Chroma, serves as a sophisticated vector storage system for the platform. </p> <p>It maintains a persistent history of states and actions, facilitating efficient context retrieval. By storing rewards and learning patterns, it optimizes performance through Qdrant\u2019s advanced vector database capabilities.</p> <p>Alternative to Qdrant, Chroma is a vector database that is easier to setup and use.</p>"},{"location":"agent/overview/#decision-making-process","title":"Decision Making Process","text":"<ol> <li> <p>State Assessment</p> <ul> <li>Nevron evaluates current context</li> <li>Retrieves relevant memories</li> <li>Analyzes available actions</li> </ul> </li> <li> <p>Action Selection</p> <ul> <li>Q-learning model selects optimal action</li> <li>Based on historical performance</li> <li>Considers current state</li> </ul> </li> <li> <p>Execution</p> <ul> <li>Selected workflow is triggered</li> <li>Tools are utilized as needed</li> <li>Results are captured</li> </ul> </li> <li> <p>Feedback Loop</p> <ul> <li>Action outcomes are evaluated</li> <li>Q-learning model is updated</li> <li>Memory is stored for future reference</li> </ul> </li> </ol>"},{"location":"agent/overview/#configuration","title":"Configuration","text":"<p>The agent's behavior can be configured via:</p> <ul> <li>Environment variables</li> <li>Configuration files</li> <li>Q-learning parameters</li> <li>Workflows</li> <li>Tools</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/planning/","title":"Planning Module","text":""},{"location":"agent/planning/#overview","title":"Overview","text":"<p>The Planning Module is a critical component of the Nevron autonomous AI agent. It leverages Q-learning, a model-free reinforcement learning algorithm, to make intelligent decisions based on the current system state.</p> <p>Key capabilities include:</p> <ul> <li>Learning from experience: Adapts behavior based on feedback from the environment.</li> <li>Balancing exploration and exploitation: Chooses between trying new actions (exploration) and relying on known effective actions (exploitation).</li> <li>Planning for the future: Prioritizes long-term success while considering immediate rewards.</li> </ul> <p>The module continuously evolves by adapting to new data through feedback loops, while maintaining consistent state-action value mappings.</p>"},{"location":"agent/planning/#the-planning-module-is-implemented-in-srcplanningplanning_modulepy","title":"The Planning Module is implemented in <code>src/planning/planning_module.py</code>.","text":""},{"location":"agent/planning/#how-it-works","title":"How It Works","text":"<p>The Planning Module empowers the AI agent with sophisticated decision-making capabilities. It utilizes a Q-table to record and update state-action value mappings, which guide the agent's decisions. This process enables the agent to refine its behavior through accumulated experience.</p> <p>Key functionalities include:</p> <ol> <li> <p>Dynamic Adaptability: When environmental conditions change, the module quickly updates the Q-table to learn new behavioral patterns.</p> </li> <li> <p>Autonomous Operations: The agent independently makes informed decisions, reducing the need for constant human intervention.</p> </li> <li> <p>Managing Complexity: Effectively navigates and prioritizes decisions in environments with numerous states and actions.</p> </li> </ol> <p>The Q-table acts as the module's memory, tracking:</p> <ul> <li>States encountered by the agent.</li> <li>Available actions for each state.</li> <li>Expected rewards for each action.</li> </ul> <p>Over time, this table becomes the primary reference for decision-making, ensuring the agent operates effectively in both familiar and new situations.</p>"},{"location":"agent/planning/#core-components","title":"Core Components","text":"<p>The Planning Module's behavior is controlled by several configuration parameters that define its learning dynamics and decision-making capabilities. These parameters are categorized into:</p>"},{"location":"agent/planning/#core-parameters","title":"Core Parameters","text":"<ul> <li><code>actions</code>: Defines the action space available to the agent, typically using the <code>AgentAction</code> enumeration to ensure a comprehensive list of permitted actions.</li> <li><code>q_table_path</code>: Specifies the file path for saving and loading the Q-table, enabling state preservation across sessions.</li> </ul>"},{"location":"agent/planning/#learning-parameters","title":"Learning Parameters","text":"<ul> <li> <p><code>planning_alpha</code> (Learning Rate):</p> <ul> <li>Controls how quickly the agent learns from new experiences.</li> <li>Higher values (e.g., 0.9): Faster learning but potentially less stability.</li> <li>Lower values (e.g., 0.1): Slower but more stable learning.</li> </ul> </li> <li> <p><code>planning_gamma</code> (Discount Factor):</p> <ul> <li>Balances the importance of future rewards versus immediate ones.</li> <li>Values closer to 1: Focus on long-term consequences.</li> <li>Lower values (e.g., 0.5): Emphasis on immediate rewards.</li> </ul> </li> <li> <p><code>planning_epsilon</code> (Exploration Rate):</p> <ul> <li>Determines the balance between exploration (trying new actions) and exploitation (sticking to known strategies).</li> <li>Higher values: Encourages trying new actions in uncertain environments.</li> <li>Lower values: Relies on proven strategies in well-understood scenarios.</li> </ul> </li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/tools/","title":"Tools","text":""},{"location":"agent/tools/#overview","title":"Overview","text":"<p>Tools are integral components that empower Nevron to interact with external services and execute specific actions. Each tool is designed to handle distinct functionality, enhancing the agent's versatility and capability. Tools act as modular utilities that can be seamlessly integrated into workflows to perform specialized tasks. They enable connectivity with external services, third-party APIs, search engines, and custom functions, making Nevron highly adaptable and efficient.</p> <p>All tools are organized within the <code>src/tools/</code> directory.</p>"},{"location":"agent/tools/#how-it-works","title":"How It Works","text":"<p>Tools in Nevron are specialized modules that handle specific tasks within the system, integrating with workflows to provide seamless interactions with external platforms. Here's how the tools work at a high level:</p> <ol> <li> <p>Integration with Workflows:    Tools serve as reusable components that workflows rely on for executing key tasks such as publishing content, fetching data, or processing input.</p> </li> <li> <p>Purpose-Built Functionality:    Each tool is uniquely designed to address a specific need, such as interacting with Twitter, Telegram, or external APIs. This ensures workflows remain focused and efficient.</p> </li> <li> <p>Technical Features:    All tools share the following core capabilities:</p> </li> <li>Error Handling: Tools catch and log errors clearly, ensuring smooth operation.</li> <li>Logging: Use <code>loguru</code> for consistent, detailed logs.</li> <li>Configuration Management: Centralized settings allow easy updates and customization.</li> <li>Asynchronous Execution: Async/await ensures non-blocking performance.</li> <li>Custom Exceptions: Each tool defines specific error types for clarity (e.g., <code>TwitterError</code>, <code>TelegramError</code>, <code>APIError</code>).</li> </ol>"},{"location":"agent/tools/#available-tools","title":"Available Tools","text":""},{"location":"agent/tools/#1-twitter-integration-twitterpy","title":"1. Twitter Integration (<code>twitter.py</code>)","text":"<p>The Twitter tool automates content publishing to Twitter using both v1.1 and v2 of the Twitter API.</p>"},{"location":"agent/tools/#features","title":"Features:","text":"<ul> <li>Supports image uploads and tweet threads.</li> <li>Combines the strengths of v1.1 (better for media) and v2 (better for posting).</li> <li>Smart rate-limiting with 3-second delays between tweets.</li> </ul>"},{"location":"agent/tools/#how-it-works_1","title":"How It Works:","text":"<ol> <li>For Media:<ul> <li>Downloads the image.</li> <li>Converts it to grayscale.</li> <li>Uploads it to Twitter.</li> </ul> </li> <li>For Tweet Threads:<ul> <li>Posts tweets sequentially, linking them together as a thread.</li> <li>Delays are added between posts to avoid API rate limits.</li> </ul> </li> <li>Output:<ul> <li>Returns the status of each posted tweet.</li> </ul> </li> </ol>"},{"location":"agent/tools/#2-telegram-integration-tgpy","title":"2. Telegram Integration (<code>tg.py</code>)","text":"<p>The Telegram tool simplifies posting content to a specific Telegram channel while managing message length constraints.</p>"},{"location":"agent/tools/#features_1","title":"Features:","text":"<ul> <li>Smart message splitting for handling Telegram's character limit.</li> <li>HTML formatting support, including links.</li> </ul>"},{"location":"agent/tools/#how-it-works_2","title":"How It Works:","text":"<ol> <li>Takes an HTML-formatted message.</li> <li>Splits the message into smaller chunks if it exceeds Telegram's length limit.</li> <li>Posts each chunk sequentially.</li> <li>Returns the IDs of all successfully posted messages.</li> </ol>"},{"location":"agent/tools/#3-perplexity-search-tool-search_with_perplexitypy","title":"3. Perplexity Search Tool (<code>search_with_perplexity.py</code>)","text":"<p>The Perplexity tool leverages AI to search for cryptocurrency-related news using Perplexity's advanced API.</p>"},{"location":"agent/tools/#features_2","title":"Features:","text":"<ul> <li>Powered by the \"llama-3.1-sonar-small-128k-online\" model.</li> <li>Tracks token usage and estimates costs ($0.2 per million tokens).</li> </ul>"},{"location":"agent/tools/#how-it-works_3","title":"How It Works:","text":"<ol> <li>Takes a search query as input.</li> <li>Sends the query to Perplexity's API with preconfigured settings (temperature 0.3, top_p 0.8).</li> <li>Receives AI-processed search results.</li> <li>Formats the results for easy consumption and returns them.</li> </ol>"},{"location":"agent/tools/#how-to-add-a-new-tool","title":"How to Add a New Tool?","text":"<p>Adding a new tool to Nevron is a straightforward process. Follow these steps:</p> <ol> <li> <p>Create a New File:    Add a new Python file in the <code>src/tools/</code> directory (e.g., <code>new_tool.py</code>).</p> </li> <li> <p>Implement the Tool's Functionality:    Define the tool's purpose and logic, adhering to Nevron's modular architecture.</p> </li> <li> <p>Test Thoroughly:    Ensure the tool works as expected by writing and running tests.</p> </li> <li> <p>Integrate into Workflows:    Once the tool is ready, it can be imported and used in workflows to enhance functionality.</p> </li> </ol>"},{"location":"agent/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Ensure all tools implement robust error handling for smooth operation.</li> <li>Logging: Use <code>loguru</code> to maintain detailed and consistent logs across all tools.</li> <li>Asynchronous Execution: Leverage <code>asyncio</code> where applicable for non-blocking performance.</li> <li>Reusability: Design tools as modular and reusable components for seamless integration into multiple workflows.</li> </ol> <p>Nevron's tools form the backbone of its ability to automate tasks, interact with external systems, and deliver actionable insights.</p> <p>From publishing on social media to conducting advanced AI-driven searches, tools are modular, reusable, and integral to the agent's success.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/workflows/","title":"Workflows","text":""},{"location":"agent/workflows/#overview","title":"Overview","text":"<p>Workflows are the backbone of Nevron\u2019s logic, defining tasks and coordinating tools and modules to deliver actionable outcomes. Each workflow is designed for autonomy and efficiency, allowing the agent to function in diverse scenarios seamlessly.</p>"},{"location":"agent/workflows/#available-workflows","title":"Available Workflows","text":""},{"location":"agent/workflows/#1-analyze-signal","title":"1. Analyze Signal","text":"<p>Focuses on processing actionable signals, such as market trends or data feeds, and publishing insights through communication channels like Twitter.</p> <p>Features: - Fetches and validates signals. - Analyzes data using a Large Language Model (LLM). - Publishes concise updates (e.g., tweets).</p> <p>Location: <code>src/workflows/analyze_signal.py</code></p>"},{"location":"agent/workflows/#2-research-news","title":"2. Research News","text":"<p>Specializes in gathering, analyzing, and summarizing news content for dissemination.</p> <p>Features: - Collects and validates news articles. - Uses LLMs for analysis and contextualization. - Publishes summaries through predefined channels.</p> <p>Location: <code>src/workflows/research_news.py</code></p>"},{"location":"agent/workflows/#workflow-architecture","title":"Workflow Architecture","text":"<p>Workflows in Nevron follow a modular design to ensure scalability and consistency:</p> <ol> <li>Input Validation: Ensures data completeness.</li> <li>Core Execution: Processes the workflow logic.</li> <li>Result Aggregation: Collects outputs for further use.</li> <li>Memory Storage: Saves results for future reference.</li> <li>Feedback Integration: Logs outcomes for iterative learning.</li> </ol>"},{"location":"agent/workflows/#integration-points","title":"Integration Points","text":"<p>Workflows rely on Nevron\u2019s modular components for execution:</p> <ul> <li>Tools:</li> <li>Signal fetching for data processing.</li> <li>Automated publishing to Twitter.</li> <li> <p>News gathering for insights.</p> </li> <li> <p>Modules:</p> </li> <li>Planning Module: Guides decision-making.</li> <li>Memory Module: Provides context from past events.</li> <li>LLM Integration: Drives analysis and content generation.</li> <li>Feedback Module: Tracks performance for improvement.</li> </ul>"},{"location":"agent/workflows/#how-to-add-a-new-workflow","title":"How to Add a New Workflow?","text":"<ol> <li>Create a New File:</li> <li> <p>Add a Python file in <code>src/workflows/</code> (e.g., <code>new_workflow.py</code>).</p> </li> <li> <p>Define the Workflow Class:    <pre><code>from src.workflows.base import BaseWorkflow\n\nclass NewWorkflow(BaseWorkflow):\n    name = \"new_workflow\"\n</code></pre></p> </li> <li> <p>Implement Logic:    <pre><code>def execute(self):\n    # Core workflow functionality\n    logger.info(\"Executing workflow\")\n\ndef validate(self):\n    # Input validation logic\n    logger.info(\"Validating workflow input\")\n</code></pre></p> </li> <li> <p>Register Workflow:    <pre><code>from src.workflows.new_workflow import NewWorkflow\nagent.register_workflow(NewWorkflow())\n</code></pre></p> </li> <li> <p>Test:</p> <ul> <li>Write unit tests for components.</li> <li>Validate integration with the agent.</li> <li>Test edge cases and error handling.</li> </ul> </li> </ol>"},{"location":"agent/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Error Handling:</p> <ul> <li>Anticipate edge cases.</li> <li>Provide clear error messages for debugging.</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Log critical workflow steps for monitoring.</li> <li>Track execution times and performance metrics.</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Unit tests for components.</li> <li>Integration tests to verify module and tool interactions.</li> <li>Simulate failure scenarios for robustness.</li> </ul> </li> </ol> <p>Nevron\u2019s workflows enable seamless automation and intelligent decision-making, forming the core of its adaptability and efficiency. For further details, refer to the GitHub Discussions.</p>"}]}