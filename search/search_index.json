{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Developer Documentation","text":"<p>Learn how to build your first Nevron, an autonomous AI agent in Python.</p> <p>This framework is designed to be a modular and extensible framework for building autonomous AI agents, which can perform tasks idependently on their own.</p> <p>Follow this documentation to learn how to create your first Nevron, build your own tools &amp; workflows, and integrate with external services.</p> <p>Note: This documentation version is for the latest stable version of Nevron. If you are looking for the documentation for the development version, please refer to the github repository. and build the docs locally. More on that in the Development Setup section.</p>"},{"location":"#overview","title":"Overview","text":"<p>Nevron is an open-source framework that support the development, deployment and management of autonomous AI agents.</p> <p>This framework is built on top of:</p> <ul> <li>Python programming language</li> <li>Q-learning algorithm for decision making</li> <li>State of the art LLM-powered intelligence</li> <li>Modular architecture with planning, feedback, and memory components</li> <li>Integration with external services (Telegram, Twitter, Discord, etc.)</li> <li>Vector-based memory storage using Chroma or Qdrant</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Autonomous Decision Making: Nevron uses Q-learning algorithm for intelligent decision making</li> <li>LLM Integration: Powered by a wide range of Large Language Models (e.g., OpenAI, Anthropic, xAI, etc.)</li> <li>Modular Workflows: Predefined autonomous agent task execution patterns<ul> <li>Analyze signal workflow</li> <li>Research news workflow</li> </ul> </li> <li>Memory Management: Qdrant-based vector storage for context retention</li> <li>External Integrations:<ul> <li>Telegram messaging</li> <li>Twitter interaction</li> <li>News API integration (in progress)</li> <li>Perplexity research integration (in progress)</li> </ul> </li> </ul>"},{"location":"#nevron-vs-other-frameworks","title":"Nevron vs. Other Frameworks","text":""},{"location":"#core-components","title":"Core Components","text":""},{"location":"#1-planning-module","title":"1. Planning Module","text":"<p>Handles decision-making using Q-learning algorithm to determine optimal actions for the agent.</p> <ul> <li>Q-Learning<ul> <li>Uses state-action value mapping for decision making</li> <li>Configurable parameters:<ul> <li>Learning rate (PLANNING_ALPHA)</li> <li>Discount factor (PLANNING_GAMMA) </li> <li>Exploration rate (PLANNING_EPSILON)</li> </ul> </li> </ul> </li> </ul>"},{"location":"#2-memory-module","title":"2. Memory Module","text":"<p>Manages agent's memory using vector storage for efficient context retrieval, which enables the agent to remember and recall previous interactions and events.</p> <ul> <li> <p>Multiple Vector Databases Support</p> <ul> <li>Chroma (default)</li> <li>Qdrant (alternative vector database)</li> </ul> </li> <li> <p>Features</p> <ul> <li>Vector embeddings via OpenAI's text-embedding-3-small model</li> <li>Semantic similarity search</li> <li>Metadata storage for context</li> <li>Configurable collection management</li> </ul> </li> <li> <p>Backend</p> <ul> <li>Abstract memory backend interface</li> <li>Modular backend architecture for optimal performance &amp; customization</li> <li>Async storage and retrieval operations</li> </ul> </li> </ul> <p>More about memory module can be found in the Memory section.</p>"},{"location":"#3-feedback-module","title":"3. Feedback Module","text":"<p>Feedback module is responsible for processing action results and updating the Q-learning model for improved decision making.</p> <ul> <li> <p>Functions</p> <ul> <li>Collects feedback from action execution</li> <li>Evaluates action outcomes</li> <li>Updates Q-learning parameters</li> <li>Maintains feedback history</li> </ul> </li> <li> <p>Integration</p> <ul> <li>Direct integration with Planning Module</li> <li>Performance metrics tracking</li> </ul> </li> </ul> <p>More about feedback module can be found in the Planning section.</p>"},{"location":"#4-tools","title":"4. Tools","text":"<p>Nevron supports integrations with external services and APIs for extended functionality &amp; integrations in diferent platforms.</p> <p>For development purposes, Nevron comes with a set of tools that can be used as a starting point for building your own tools and integrating more complex functionality to your AI agent.</p> <ul> <li> <p>Telegram</p> <ul> <li>Telegram Bot integration</li> <li>Channel/group support</li> <li>HTML message formatting</li> </ul> </li> <li> <p>Twitter</p> <ul> <li>Tweet posting</li> <li>Media handling</li> <li>Thread creation</li> </ul> </li> <li> <p>Research</p> <ul> <li>Perplexity API integration (in progress)</li> <li>News API integration (in progress)</li> </ul> </li> </ul> <p>More about tools can be found in the Tools section.</p>"},{"location":"#5-llm-integration","title":"5. LLM Integration","text":"<p>Powers the agent's intelligence and natural language capabilities.</p> <ul> <li>Supported Providers<ul> <li>OpenAI (primary)</li> <li>gpt-4o for decision making</li> <li>text-embedding-3-small for embeddings</li> <li>Anthropic (alternative)</li> </ul> </li> </ul> <p>More about LLM integration can be found in the LLM section.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For setup and development instructions, please refer to our Quickstart and setup your first AI agent in minutes.</p>"},{"location":"#creators","title":"Creators","text":"<p>Nevron was created by Neurobro team. If you want to learn more about Nevron, our core values &amp; team behind it, please visit About page.</p> <p>Support by upvoting Nevron on Product Hunt.</p> <p> </p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"about/","title":"About Nevron","text":"<p>Nevron is an open-source AI Agent framework developed by Neurobro, dedicated to creating intelligent, modular agents for automation and decision-making.</p> <p>Our mission is to make AI innovation accessible, decentralized, and collaborative to empower individuals and organizations globally. We envision a future where AI is democratized, transparent, and seamlessly integrated into everyday life to drive meaningful impact and sustainable growth.</p> <p>Nevron is part of Neurobro's commitment to open-source decentralized artificial intelligence, fostering innovation through community-driven development.</p> <p>To learn more about our vision, mission &amp; belief, feel free to read our Litepaper.</p> <p>More about our AI Agents can be found here</p>"},{"location":"about/#our-principles","title":"Our Principles","text":"<ul> <li> <p>Accessibility: AI tools designed for everyone, from students to enterprises, ensuring broad adoption and usability.</p> </li> <li> <p>Transparency: Open-source code, processes, and decision-making to build trust and foster collaboration.</p> </li> <li> <p>Sustainability: A balanced approach to fostering innovation while ensuring long-term project growth and community health.</p> </li> <li> <p>Decentralization: Building tools that prioritize autonomy and reduce reliance on centralized entities, in line with the ethos of blockchain and Web3 technologies.</p> </li> <li> <p>Collaboration: Actively engaging with a global community of developers and researchers to advance AI's potential.</p> </li> <li> <p>Ethical AI: Developing AI systems that respect privacy, promote fairness, and align with human values.</p> </li> </ul>"},{"location":"about/#get-in-touch","title":"Get In Touch","text":"<p>Follow us on social media to stay updated on the latest developments and get involved in the community.</p> <ul> <li>Neurobro Website</li> <li>Neurobro on Twitter</li> <li>Neurobro on Telegram</li> <li>Neurobro on YouTube</li> <li>Neurobro on TikTok</li> </ul> <p>Stay updated and contribute to the evolution of AI with Nevron. </p> <p>Together, we can push the boundaries of what's possible \u26a1\ufe0f</p>"},{"location":"deployment/","title":"Deployment","text":"<p>This guide covers deployment option for Nevron using Docker.</p>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/#official-docker-image","title":"Official Docker Image","text":"<p>Nevron is available as an official Docker image on Docker Hub: <pre><code>docker pull axiomai/nevron:latest\n</code></pre></p> <p>Also you can build the image locally:</p> <pre><code>docker build -t axiomai/nevron:latest .\n</code></pre>"},{"location":"deployment/#running-with-docker","title":"Running with Docker","text":"<p>Basic run command: <pre><code># create directories for volumes\nmkdir -p volumes/.chromadb\n\n# run the agent\ndocker run -d \\\n  --name nevron \\\n  -e .env \\\n  -v $(pwd)/volumes/.chromadb:/app/.chromadb \\\n  axiomai/nevron:latest\n</code></pre></p>"},{"location":"deployment/#configuration","title":"Configuration","text":""},{"location":"deployment/#volume-mounts","title":"Volume Mounts","text":"<ul> <li><code>.chromadb</code>: Persistent storage for ChromaDB (when using ChromaDB backend)</li> <li><code>qdrant_storage</code>: Persistent storage for Qdrant (when using Qdrant backend)</li> </ul>"},{"location":"deployment/#environment-variables","title":"Environment Variables","text":"<p>You'll need to set the <code>OPENAI_API_KEY</code> environment variable to be able to use the agent.</p> <p>For a complete list of available environment variables, see the Environment Variables documentation.</p>"},{"location":"deployment/#docker-compose","title":"Docker Compose","text":"<p>For production deployments, we provide a <code>docker-compose.yml</code>:</p> <pre><code># See the full file in the repository\nservices:\n  nevron:\n    image: axiomai/nevron:latest\n    # ... configuration ...\n\n  qdrant:\n    image: qdrant/qdrant:latest\n    # ... configuration ...\n</code></pre> <p>Key features of our Docker Compose setup:</p> <ol> <li>Service Definitions</li> <li>Reusable service defaults</li> <li>Automatic restart policies</li> <li>Proper logging configuration</li> <li> <p>Network isolation</p> </li> <li> <p>Volume Management</p> </li> <li>Persistent storage for logs</li> <li>Configurable volume base directory</li> <li> <p>Separate volumes for different components</p> </li> <li> <p>Networking</p> </li> <li>Internal network for service communication</li> <li>External network for API access</li> <li> <p>Bridge network driver for security</p> </li> <li> <p>Environment Configuration</p> </li> <li>Environment file support</li> <li>Override capability for all settings</li> <li>Service-specific environment variables</li> </ol> <p>To use Docker Compose:</p> <pre><code># Create required directories\nmkdir -p volumes/{nevron,qdrant}/{logs,data,snapshots}\n\n# Add your .env file\ncp .env.example .env\n\n# Start services\ndocker compose up -d\n\n# View logs\ndocker compose logs -f\n\n# Stop services\ndocker compose down\n</code></pre> <p>Important: - Make sure to set the correct environment variables in the <code>.env</code> file. - Make sure to set the correct volume mounts in the <code>docker-compose.yml</code> file.</p>"},{"location":"deployment/#production-considerations","title":"Production Considerations","text":"<p>When deploying to production, consider the following:</p> <ol> <li>Use a production-grade process manager (e.g., supervisord, systemd)</li> <li>Set up proper logging and monitoring</li> <li>Use secure storage for API keys and sensitive data</li> <li>Configure appropriate resource limits</li> <li>Set up health checks and automatic restarts</li> <li>Use a reverse proxy for any exposed endpoints</li> <li>Implement proper backup strategies for memory backends</li> </ol> <p>For production deployments, we recommend: - Using the Docker Compose deployment method - Setting <code>ENVIRONMENT=production</code> in your configuration - Using a dedicated memory backend instance - Implementing proper monitoring and alerting - Regular backups of memory storage </p>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will help you get Nevron, your autonomous AI agent, running quickly. Choose the setup path that best suits your needs:</p> <ul> <li>Docker Setup (Recommended for production)</li> <li>Local Setup (Recommended for development)</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Common requirements for all installation methods: - OpenAI API key (or any other API key of an LLM provider of your choice)</p> <p>Additional requirements: - For Docker setup: Docker - For local setup: Python 3.13 and Poetry</p>"},{"location":"quickstart/#docker-setup","title":"Docker Setup","text":"<p>Get Nevron running with Docker in 3 steps:</p>"},{"location":"quickstart/#1-pull-setup","title":"1. Pull &amp; Setup","text":"<pre><code># pull the latest image\ndocker pull axiomai/nevron:latest\n\n# create directories for volumes\nmkdir -p volumes/.chromadb\n\n# copy example environment file\ncp .env.example .env\n</code></pre>"},{"location":"quickstart/#2-configure","title":"2. Configure","text":"<p>Edit <code>.env</code> file with your settings: <pre><code># Required: Choose one of the LLM providers and set its API key\nOPENAI_API_KEY=your_key_here    # For OpenAI\n# ANTHROPIC_API_KEY=your_key_here # For Anthropic\n# XAI_API_KEY=your_key_here      # For xAI\n# LLAMA_API_KEY=your_key_here    # For Llama with Fireworks\n# Or use local Llama with Ollama:\n# LLAMA_PROVIDER=ollama\n# LLAMA_OLLAMA_MODEL=llama3.1:8b\n\n# Optional: Set the environment\nENVIRONMENT=production          # Recommended for Docker setup\n\n# Optional: Choose memory backend\nMEMORY_BACKEND_TYPE=chroma     # Default memory backend\n</code></pre></p> <p>Also, you can configure the personality, goals and rest time of your agent in <code>.env</code>.</p> <pre><code>AGENT_PERSONALITY=\"A helpful AI assistant focused on research and analysis\"\nAGENT_GOAL=\"To assist with information gathering and analysis\"\nAGENT_REST_TIME=300  # seconds between actions\n</code></pre> <p>Note: the full list of available configuration options is available in the Environment Variables documentation.</p>"},{"location":"quickstart/#3-run","title":"3. Run","text":"<pre><code>docker run -d \\\n  --name nevron \\\n  -e .env \\\n  -v $(pwd)/volumes/.chromadb:/app/.chromadb \\\n  axiomai/nevron:latest\n</code></pre> <p>For production deployments, we recommend using Docker Compose. See our Deployment Guide for details.</p>"},{"location":"quickstart/#local-setup","title":"Local Setup","text":"<p>Set up Nevron locally in 5 steps:</p>"},{"location":"quickstart/#1-clone-install","title":"1. Clone &amp; Install","text":"<pre><code># clone the repository\ngit clone https://github.com/axioma-ai-labs/nevron.git\ncd nevron\n\n# install Poetry if you haven't already\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# install dependencies\npoetry install\n</code></pre>"},{"location":"quickstart/#2-configure-environment","title":"2. Configure Environment","text":"<pre><code># copy example environment file\ncp .env.dev .env\n</code></pre> <p>Required environment variables: <pre><code>OPENAI_API_KEY=your_key_here    # Required for embeddings\n</code></pre></p> <p>For a complete list of available environment variables, see the Environment Variables documentation.</p>"},{"location":"quickstart/#3-configure-personality","title":"3. Configure Personality","text":"<p>Setup the personality, goals and rest time of your agent in <code>.env</code>: <pre><code>AGENT_PERSONALITY=\"A helpful AI assistant focused on research and analysis\"\nAGENT_GOAL=\"To assist with information gathering and analysis\"\nAGENT_REST_TIME=300  # seconds between actions\n</code></pre></p>"},{"location":"quickstart/#4-run","title":"4. Run","text":"<pre><code>make run\n</code></pre>"},{"location":"quickstart/#available-workflows","title":"Available Workflows","text":"<p>Nevron comes with two pre-configured workflows:</p> <ul> <li><code>Analyze signal</code>: Processes and analyzes incoming signal data</li> <li><code>Research news</code>: Gathers and analyzes news using Perplexity API</li> </ul> <p>For more information about workflows, see the Workflows documentation.</p>"},{"location":"quickstart/#customization","title":"Customization","text":"<p>You can customize Nevron by: - Adding custom workflows and tools - Adjusting planning parameters - Switching LLM providers</p> <p>See the Agent Overview for more details.</p>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues: - Ensure all required API keys are set in <code>.env</code> - Check logs in the console for detailed error messages - Verify Python version: <code>python --version</code> (should be 3.13) - Confirm dependencies: <code>poetry show</code></p> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/discord/","title":"Discord Integration","text":""},{"location":"agent/discord/#setup","title":"Setup","text":"<ol> <li>Create a Discord Application and Bot </li> <li>Go to Discord Developer Portal</li> <li>Create a \"New Application\" </li> <li>Go to the \"Bot\" section and create a bot</li> <li> <p>Choose appropriate bot rights (actions to perform) and copy the bot token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>DISCORD_BOT_TOKEN=your_bot_token_here\nDISCORD_CHANNEL_ID=your_channel_id\n</code></pre></p> </li> <li> <p>Invite Bot to Server</p> </li> <li>Go to OAuth2 &gt; URL Generator</li> <li>Select scopes: <code>bot</code></li> <li>Select permissions: <ul> <li>Read Messages/View Channels</li> <li>Send Messages</li> <li>Add Reactions</li> </ul> </li> <li>Copy and use the generated URL to invite the bot</li> </ol>"},{"location":"agent/discord/#basic-bot-setup","title":"Basic Bot Setup","text":"<pre><code>from src.tools.discord import DiscordTool\n\n# Initialize and start bot\ndiscord = DiscordTool()\nawait discord.initialize_bot()\n</code></pre>"},{"location":"agent/discord/#example-usage","title":"Example Usage","text":"<p>A bot that responds to greetings and adds reactions:</p> <pre><code>async def handle_message(message_data):\n    if message_data['content'].lower() in ['hi', 'hello']:\n        # Send response\n        msg_id = await discord.send_message(\n            channel_id=settings.DISCORD_CHANNEL_ID,\n            content=f\"Hello {message_data['username']}! \ud83d\udc4b\"\n        )\n        # Add reaction\n        await discord.add_reaction(\n            channel_id=settings.DISCORD_CHANNEL_ID,\n            message_id=msg_id,\n            emoji=\"\ud83d\udc4b\"\n        )\n\n# Start listening\nawait discord.listen_to_messages(\n    channel_id=settings.DISCORD_CHANNEL_ID,\n    callback=handle_message\n)\n</code></pre> <p>This example demonstrates: - Message sending - Message listening - Adding reactions - Basic interaction flow</p>"},{"location":"agent/discord/#features","title":"Features","text":"<ul> <li>Send messages to specific channels</li> <li>Listen for incoming messages</li> <li>Add reactions to messages</li> <li>Handle reaction events</li> <li>Error handling and logging</li> </ul> <p>## TODOs for Future Enhancements:</p> <p>Support advanced message commands. Add the ability to send/receive attachments. Add reaction support for messages. Implement permission-based access control.</p>"},{"location":"agent/discord/#reference","title":"Reference","text":"<p>For implementation details, see: tools/discord.py</p>"},{"location":"agent/feedback/","title":"Feedback Module","text":""},{"location":"agent/feedback/#overview","title":"Overview","text":"<p>The Feedback Module is an integral component designed to collect and process feedback for actions performed by the agent. It enables the system to evaluate outcomes, track performance, and adapt behavior based on historical data. This module is vital for improving the agent's decision-making and maintaining accountability.</p> <p>The Feedback Module is implemented in <code>src/feedback/feedback_module.py</code> and provides structured methods for collecting, storing, and analyzing feedback related to agent actions.</p>"},{"location":"agent/feedback/#how-it-works","title":"How It Works","text":"<p>The Feedback Module operates as follows:</p> <ol> <li> <p>Feedback Collection:    Captures feedback based on the action performed and its outcome, assigning a feedback score to evaluate success or failure.</p> </li> <li> <p>Feedback Storage:    Maintains an internal history of feedback entries, including details about the action, its outcome, and the assigned feedback score.</p> </li> <li> <p>Feedback Retrieval:    Allows querying of recent feedback entries for analysis and monitoring.</p> </li> <li> <p>Feedback Reset:    Provides functionality to clear feedback history, ensuring the module can be reset when needed.</p> </li> </ol>"},{"location":"agent/feedback/#technical-features","title":"Technical Features","text":""},{"location":"agent/feedback/#1-feedback-collection","title":"1. Feedback Collection","text":"<p>The <code>collect_feedback</code> method records feedback for a specific action and its outcome. It assigns a feedback score based on predefined criteria:</p> <ul> <li>Failure: Assigned a score of <code>-1.0</code> if the outcome is <code>None</code>.</li> <li>Success: Assigned a score of <code>1.0</code> for successful outcomes.</li> </ul>"},{"location":"agent/feedback/#implementation","title":"Implementation:","text":"<pre><code>feedback_score = -1.0 if outcome is None else 1.0\n</code></pre> <ul> <li> <p>Inputs:</p> </li> <li> <p><code>action</code> (str): The name of the action performed.</p> </li> <li> <p><code>outcome</code> (Any): The outcome of the action; <code>None</code> for failure or a value indicating success.</p> </li> <li> <p>Output:</p> </li> <li> <p>Returns a feedback score (<code>float</code>) for the action.</p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>feedback_score = feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre>"},{"location":"agent/feedback/#2-feedback-history-retrieval","title":"2. Feedback History Retrieval","text":"<p>The <code>get_feedback_history</code> method retrieves the most recent feedback entries, limited by the specified count.</p>"},{"location":"agent/feedback/#features","title":"Features:","text":"<ul> <li>Enables monitoring of agent performance.</li> <li>Default limit is set to 10 entries.</li> </ul>"},{"location":"agent/feedback/#example","title":"Example:","text":"<pre><code>recent_feedback = feedback_module.get_feedback_history(limit=5)\n</code></pre>"},{"location":"agent/feedback/#3-feedback-reset","title":"3. Feedback Reset","text":"<p>The <code>reset_feedback_history</code> method clears the internal feedback history, resetting the module's state. This is useful for testing or reinitializing feedback tracking.</p>"},{"location":"agent/feedback/#example_1","title":"Example:","text":"<pre><code>feedback_module.reset_feedback_history()\n</code></pre>"},{"location":"agent/feedback/#key-methods","title":"Key Methods","text":""},{"location":"agent/feedback/#collect_feedback","title":"<code>collect_feedback</code>","text":"<p>Captures feedback for a given action and outcome.</p> <p>Arguments:</p> <ul> <li><code>action</code> (str): The action name.</li> <li><code>outcome</code> (Optional[Any]): The outcome of the action.</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Feedback score.</li> </ul>"},{"location":"agent/feedback/#get_feedback_history","title":"<code>get_feedback_history</code>","text":"<p>Retrieves recent feedback entries.</p> <p>Arguments:</p> <ul> <li><code>limit</code> (int): Number of entries to retrieve (default: 10).</li> </ul> <p>Returns:</p> <ul> <li><code>List[Dict[str, Any]]</code>: Recent feedback entries.</li> </ul>"},{"location":"agent/feedback/#reset_feedback_history","title":"<code>reset_feedback_history</code>","text":"<p>Clears the feedback history.</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"agent/feedback/#example-workflow","title":"Example Workflow","text":"<ol> <li>An action is performed by the agent (e.g., <code>fetch_data</code>).</li> <li>The outcome of the action is evaluated, and feedback is collected:    <pre><code>feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre></li> <li>The collected feedback is stored in the feedback history for analysis.</li> <li>The feedback history can be retrieved for review:    <pre><code>recent_feedback = feedback_module.get_feedback_history(limit=5)\n</code></pre></li> <li>Feedback history is reset when needed:    <pre><code>feedback_module.reset_feedback_history()\n</code></pre></li> </ol>"},{"location":"agent/feedback/#benefits","title":"Benefits","text":"<ul> <li>Performance Monitoring: Tracks agent actions and their outcomes, enabling better performance evaluation.</li> <li>Adaptability: Facilitates improvements by learning from historical data.</li> <li>Simplicity: Provides clear and structured methods for feedback collection and retrieval.</li> </ul>"},{"location":"agent/feedback/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Feedback Collection: Ensure feedback is collected for all critical actions to build a comprehensive history.</li> <li>Periodic Reset: Use <code>reset_feedback_history</code> to clear stale feedback during testing or major updates.</li> <li>Logging: Leverage <code>loguru</code> for detailed insights into feedback processing.</li> </ol>"},{"location":"agent/feedback/#known-limitations","title":"Known Limitations","text":"<ul> <li>Scoring Granularity: Feedback scores are currently binary (<code>-1.0</code> or <code>1.0</code>). Future versions can introduce more nuanced scoring.</li> <li>Debugging Notes: Debugging comments in <code>collect_feedback</code> indicate areas for improvement.</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/github/","title":"GitHub Integration","text":""},{"location":"agent/github/#setup","title":"Setup","text":"<ol> <li>Generate GitHub Personal Access Token</li> <li>Go to GitHub Settings</li> <li>Click \"Generate new token\" &gt; \"Generate new token (classic)\"</li> <li>Select required scopes:<ul> <li><code>repo</code> (Full control of private repositories)</li> <li><code>workflow</code> (Optional: for workflow actions)</li> </ul> </li> <li> <p>Copy the generated token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>GITHUB_TOKEN=your_personal_access_token_here\n</code></pre></p> </li> </ol>"},{"location":"agent/github/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.github import GitHubIntegration, FileChange\n\n# Initialize GitHub client\ngithub = GitHubIntegration()\n\n# Initialize repository\nawait github.initialize_repo(\n    owner=\"username\",\n    repo_name=\"repository\",\n    branch=\"main\"\n)\n\n# Create a pull request\nfiles = [\n    FileChange(\n        path=\"path/to/file.py\",\n        content=\"Updated file content\"\n    )\n]\n\npr = await github.create_pull_request(\n    branch=\"feature-branch\",\n    title=\"Add new feature\",\n    description=\"Implemented new functionality\",\n    files=files\n)\n\n# Create a direct commit\nawait github.create_commit(\n    branch=\"main\",\n    message=\"Update documentation\",\n    files=files\n)\n</code></pre>"},{"location":"agent/github/#features","title":"Features","text":"<ul> <li>Repository initialization and management</li> <li>File content processing and memory storage</li> <li>Pull request creation with multiple file changes</li> <li>Direct commit creation</li> <li>Automatic branch management</li> <li>Local repository caching</li> <li>Repository synchronization</li> </ul>"},{"location":"agent/github/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for GitHub Actions workflow management</li> <li>Implement issue creation and management</li> <li>Implement code review automation</li> <li>Add support for GitHub Projects &amp; Packages</li> <li>Implement repository statistics and analytics</li> <li>Implement repository security scanning</li> <li>Handle GitHub webhook events (e.g., new commits, pull requests) for real-time agent updates</li> <li>Enable metadata-based memory filtering for selective file processing</li> <li>Implement branch management and conflict resolution</li> <li>Add support for attachments and binary files</li> <li>Optimize performance for large repositories</li> </ul>"},{"location":"agent/github/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/github.py</code></p> <p>The implementation uses the PyGithub library. For more information, refer to: - PyGithub Documentation - GitHub REST API Documentation</p>"},{"location":"agent/google_drive/","title":"Google Drive Integration","text":""},{"location":"agent/google_drive/#setup","title":"Setup","text":"<ol> <li>Enable Google Drive API</li> <li>Go to Google Cloud Console</li> <li>Navigate to APIs &amp; Services &gt; Library</li> <li> <p>Search for and enable \"Google Drive API\"</p> </li> <li> <p>Generate Credentials</p> </li> <li>Go to APIs &amp; Services &gt; Credentials</li> <li>Click \"Create Credentials\" &gt; \"OAuth 2.0 Client ID\"</li> <li>Select \"Desktop Application\"</li> <li>Download the credentials file as <code>credentials.json</code></li> <li> <p>Place it in your project root directory</p> </li> <li> <p>Configure Environment Variables    No environment variables needed, but ensure <code>credentials.json</code> is in your project root.</p> </li> </ol>"},{"location":"agent/google_drive/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.google_drive import authenticate_google_drive, search_files, upload_file, download_file\n\n# Initialize Google Drive service\nservice = authenticate_google_drive()\n\n# Search for files\nresults = search_files(service, \"name contains 'report'\")\n\n# Upload a file\nfile_id = upload_file(service, \n    file_path=\"path/to/file.pdf\",\n    mime_type=\"application/pdf\"\n)\n\n# Download a file\ndownload_file(service,\n    file_id=\"your_file_id\",\n    destination=\"path/to/save/file.pdf\"\n)\n</code></pre>"},{"location":"agent/google_drive/#features","title":"Features","text":"<ul> <li>OAuth2 authentication flow</li> <li>File search with custom queries</li> <li>File upload with MIME type support</li> <li>File download with progress tracking</li> <li>Token persistence for future sessions</li> <li>Automatic token refresh</li> <li>Error handling and logging</li> </ul>"},{"location":"agent/google_drive/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for folder operations</li> <li>Implement file sharing functionality</li> <li>Add batch upload/download support</li> <li>Add support for editing Google Sheets and Docs.</li> <li>Implement real-time notifications for file updates using Google Drive API webhooks.</li> <li>Add support for Google Sheets/Docs creation</li> <li>Add support for Team Drives</li> <li>Implement file change tracking</li> <li>Handle advanced file sharing and permission settings.</li> </ul>"},{"location":"agent/google_drive/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/google_drive.py</code></p> <p>The implementation uses the official Google Drive API v3. For more information, refer to: - Google Drive API Documentation - Google Auth Library for Python</p>"},{"location":"agent/lens_protocol/","title":"Lens Protocol Integration","text":""},{"location":"agent/lens_protocol/#setup","title":"Setup","text":"<ol> <li>Enable Lens Protocol API Access</li> <li>Go to Lens Protocol Developer Portal</li> <li>Create a developer account</li> <li> <p>Navigate to API Keys section</p> </li> <li> <p>Generate API Key</p> </li> <li>In the Developer Portal, create a new API key</li> <li>Copy the generated API key</li> <li> <p>(Optional) Set API key restrictions and rate limits</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>LENS_API_KEY=your_api_key_here\nLENS_PROFILE_ID=your_profile_id\n</code></pre></p> </li> </ol>"},{"location":"agent/lens_protocol/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.lens_protocol import LensProtocolTool\n\n# Initialize Lens Protocol client\nlens = LensProtocolTool()\n\n# Connect to Lens Protocol\nauth_credentials = {\n    'api_key': 'your_api_key_here'\n}\nlens.initialize_connection(auth_credentials)\n\n# Get profile information\nprofile = lens.get_profile(\"lens.dev\")\n\n# Fetch recent content\npublications = lens.fetch_content({\n    'limit': 5,\n    'sort': 'DESC'\n})\n\n# Publish content\nresult = lens.publish_content(\n    profile_id=\"your_profile_id\",\n    content=\"Hello Lens Protocol!\"\n)\n</code></pre>"},{"location":"agent/lens_protocol/#features","title":"Features","text":"<ul> <li>Profile information retrieval and management</li> <li>Content publication to the Lens network</li> <li>Content exploration and fetching with custom parameters</li> <li>Access to social metrics (followers, following, reactions)</li> <li>Publication statistics (comments, mirrors, reactions)</li> <li>GraphQL-based API integration</li> <li>Error handling and logging</li> </ul>"},{"location":"agent/lens_protocol/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for authentication and profile management</li> <li>Implement follow/unfollow functionality</li> <li>Add comment creation and management</li> <li>Support for mirroring content</li> <li>Implement content moderation features</li> <li>Add support for media attachments</li> <li>Implement notification handling</li> <li>Add support for collecting publications</li> <li>Implement profile search functionality</li> <li>Add support for encrypted direct messaging</li> </ul>"},{"location":"agent/lens_protocol/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/lens_protocol.py</code></p> <p>The implementation uses the Lens Protocol API v2. For more information, refer to: - Lens Protocol Documentation - Lens Protocol API Reference - GraphQL Schema Documentation</p>"},{"location":"agent/llama/","title":"Llama Integration","text":"<p>The agent supports local inference using Meta's Llama models (8B, 70B, and 405B parameters) for scenarios requiring on-premise or self-hosted language models.</p>"},{"location":"agent/llama/#setup","title":"Setup","text":"<ol> <li>Download Llama Model</li> <li>Obtain access to Meta's Llama models through Meta's AI website</li> <li> <p>Download your preferred model size:</p> <ul> <li>Llama-8B (16GB minimum RAM)</li> <li>Llama-70B (140GB minimum RAM)</li> <li>Llama-405B (780GB minimum RAM)</li> </ul> </li> <li> <p>Install Dependencies    <pre><code>pip install torch&gt;=2.2.0 transformers&gt;=4.38.0\n</code></pre></p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>LLAMA_MODEL_PATH=/path/to/your/llama/model\nLLM_PROVIDER=llama\n</code></pre></p> </li> </ol>"},{"location":"agent/llama/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.llm.llm import LLM\n\n# Initialize LLM with Llama backend\nllm = LLM()  # Will use Llama if LLM_PROVIDER=llama\n\n# Generate a response\nresponse = await llm.generate_response([\n    {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n])\n</code></pre>"},{"location":"agent/llama/#model-selection","title":"Model Selection","text":"<p>Choose the appropriate model size based on your requirements:</p>"},{"location":"agent/llama/#llama-8b","title":"Llama-8B","text":"<ul> <li>Minimum Requirements:</li> <li>16GB RAM</li> <li>20GB disk space</li> <li>Best for:</li> <li>Basic text generation</li> <li>Simple chat interactions</li> <li>Resource-constrained environments</li> </ul>"},{"location":"agent/llama/#llama-70b","title":"Llama-70B","text":"<ul> <li>Minimum Requirements:</li> <li>140GB RAM</li> <li>140GB disk space</li> <li>Best for:</li> <li>Complex reasoning</li> <li>Code generation</li> <li>Advanced chat applications</li> </ul>"},{"location":"agent/llama/#llama-405b","title":"Llama-405B","text":"<ul> <li>Minimum Requirements:</li> <li>780GB RAM</li> <li>800GB disk space</li> <li>Best for:</li> <li>Research applications</li> <li>Maximum model capability</li> <li>Enterprise-scale deployments</li> </ul>"},{"location":"agent/llama/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"agent/llama/#memory-management","title":"Memory Management","text":"<p>The agent automatically manages model loading based on available resources: <pre><code># Override default device mapping\nresponse = await llm.generate_response(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    device_map=\"cpu\"  # Force CPU inference\n)\n\n# Control memory usage\nresponse = await llm.generate_response(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    max_tokens=512,  # Limit response length\n    torch_dtype=\"auto\"  # Automatic precision selection\n)\n</code></pre></p>"},{"location":"agent/llama/#generation-parameters","title":"Generation Parameters","text":"<p>Customize response generation: <pre><code># Creative writing\nresponse = await llm.generate_response(\n    messages=[{\"role\": \"user\", \"content\": \"Write a story\"}],\n    temperature=0.9,  # More creative\n    top_p=0.95\n)\n\n# Factual responses\nresponse = await llm.generate_response(\n    messages=[{\"role\": \"user\", \"content\": \"Explain TCP/IP\"}],\n    temperature=0.2,  # More focused\n    top_p=0.1\n)\n</code></pre></p>"},{"location":"agent/llama/#example-use-cases","title":"Example Use Cases","text":""},{"location":"agent/llama/#chat-application","title":"Chat Application","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\nresponse = await llm.generate_response(messages)\n</code></pre>"},{"location":"agent/llama/#code-generation","title":"Code Generation","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a Python expert.\"},\n    {\"role\": \"user\", \"content\": \"Write a function to calculate Fibonacci numbers.\"}\n]\nresponse = await llm.generate_response(\n    messages,\n    temperature=0.2  # Lower temperature for code generation\n)\n</code></pre>"},{"location":"agent/llama/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Management</li> <li>Monitor system memory usage</li> <li>Use appropriate model size for your hardware</li> <li> <p>Consider CPU fallback for large models</p> </li> <li> <p>Performance Optimization</p> </li> <li>Use GPU acceleration when available</li> <li>Adjust batch sizes based on memory constraints</li> <li> <p>Cache frequently used responses</p> </li> <li> <p>Error Handling</p> </li> <li>Implement proper validation of model files</li> <li>Handle out-of-memory scenarios gracefully</li> <li>Monitor model performance and errors</li> </ol>"},{"location":"agent/llama/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and solutions:</p> <ol> <li>Out of Memory Errors</li> <li>Switch to a smaller model</li> <li>Use CPU fallback</li> <li> <p>Reduce batch size or context length</p> </li> <li> <p>Missing Model Files</p> </li> <li>Verify model path in environment variables</li> <li>Check file permissions</li> <li> <p>Ensure all required model files are present</p> </li> <li> <p>Poor Performance</p> </li> <li>Check GPU utilization</li> <li>Monitor system resources</li> <li>Adjust generation parameters</li> </ol>"},{"location":"agent/llama/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/llm/providers/llama.py</code></p> <p>For more information, refer to: - Llama Model Documentation - Hugging Face Transformers - PyTorch Documentation</p>"},{"location":"agent/llm/","title":"LLM Integration","text":"<p>Large Language Models are the backbone of the Autonomous Agent. They are the core component that allows the agent to understand and respond to natural language.</p>"},{"location":"agent/llm/#implementation","title":"Implementation","text":"<p>The LLM integration is primarily handled through the <code>src/llm</code> directory, which provides:</p> <ul> <li>API interaction with OpenAI/Anthropic/xAI models</li> <li>Embeddings generation for memory storage</li> <li>Context management</li> <li>Response processing &amp; generation</li> </ul>"},{"location":"agent/llm/#overview","title":"Overview","text":""},{"location":"agent/llm/#1-embeddings","title":"1. Embeddings","text":"<p>For memory storage, the agent uses OpenAI's embedding models to generate vector representations of the memories. These vectors are then stored in a vector database for efficient retrieval and semantic search. To generate embeddings, the agent uses the <code>src/llm/embeddings.py</code> module.</p> <p>For embeddings generation we recommend using OpenAI's <code>text-embedding-3-large</code> model.</p>"},{"location":"agent/llm/#2-response-processing-generation","title":"2. Response Processing &amp; Generation","text":"<p>The agent uses the LLM class to generate responses through different providers. The <code>src/llm/llm.py</code> module provides:</p> <ul> <li>Unified interface for multiple LLM providers through the <code>LLM</code> class</li> <li>Automatic system message injection with agent personality and goals</li> <li>Async response generation via <code>generate_response()</code> method</li> <li>Support for additional parameters like model and temperature</li> <li>OpenAI client initialization helper via <code>get_oai_client()</code></li> </ul>"},{"location":"agent/llm/#configuration","title":"Configuration","text":"<p>Currently, the agent is configured to use OpenAI's <code>gpt-4o</code> model, but it can be easily configured to use other models (e.g. <code>gpt-4o-mini</code>, <code>gpt-4</code>, Anthropic's models like <code>claude-3-5-sonnet</code> or xAI's <code>grok-2-latest</code> model).</p>"},{"location":"agent/llm/#environment-variables","title":"Environment Variables","text":"<p>To choose model of your choice, set the following environment variables: <pre><code>OPENAI_API_KEY=your-api-key\nOPENAI_MODEL=gpt-4  # or other supported models (e.g. gpt-4o-mini, gpt-4o, claude-3-5-sonnet, grok-2-latest)\nOPENAI_EMBEDDING_MODEL=text-embedding-3-large # or other supported embedding models\n</code></pre></p>"},{"location":"agent/llm/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Token Management    Monitor and track token consumption across API calls. Implement rate limiting mechanisms to prevent exceeding quotas. Establish proper API quota management systems to maintain service availability.</p> </li> <li> <p>Error Handling    Implement graceful fallback mechanisms when API calls fail. Set up automatic retry logic with exponential backoff. Maintain comprehensive error logging to track and debug issues.</p> </li> <li> <p>Cost Optimization    Select appropriate model tiers based on task requirements. Implement response caching for frequently requested prompts. Track and analyze API usage patterns to optimize costs.</p> </li> </ol>"},{"location":"agent/llm/#future-enhancements","title":"Future Enhancements","text":"<p>We're planning to add support for additional LLM providers, advanced prompt engineering, fine-tuning capabilities, and enhanced error handling in the nearest future.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/memory/","title":"Memory Module","text":""},{"location":"agent/memory/#overview","title":"Overview","text":"<p>The Memory Module is a critical component designed to store and retrieve structured memory entries for the agent. It leverages vector embedding and similarity search to maintain a contextual memory, allowing the agent to make decisions based on historical events and outcomes. This module supports flexible backends to meet different scalability and performance needs.</p> <p>The Memory Module is implemented in <code>src/memory/memory_module.py</code> and integrates with vector storage systems like Qdrant and ChromaDB.</p>"},{"location":"agent/memory/#how-it-works","title":"How It Works","text":"<p>The Memory Module operates as follows:</p> <ol> <li> <p>Embedding Generation: Converts textual data into vector embeddings using OpenAI's embedding models.</p> </li> <li> <p>Memory Storage: Stores embeddings and associated metadata in a vector store backend.</p> </li> <li> <p>Memory Retrieval: Performs similarity searches to retrieve related memories based on a query.</p> </li> <li> <p>Backend Flexibility: Supports multiple backends, such as Qdrant and ChromaDB, to accommodate various storage and performance requirements.</p> </li> </ol>"},{"location":"agent/memory/#technical-features","title":"Technical Features","text":""},{"location":"agent/memory/#1-embedding-generation","title":"1. Embedding Generation","text":"<p>The module uses an <code>EmbeddingGenerator</code> to convert textual descriptions (e.g., events, actions, outcomes) into vector embeddings. These embeddings form the basis for similarity searches.</p>"},{"location":"agent/memory/#key-features","title":"Key Features:","text":"<ul> <li>Supports asynchronous embedding generation using OpenAI models.</li> <li>Combines event, action, and outcome data to create meaningful embeddings.</li> </ul>"},{"location":"agent/memory/#example","title":"Example:","text":"<pre><code>embedding = await embedding_generator.get_embedding(\"event action outcome\")\n</code></pre>"},{"location":"agent/memory/#2-memory-storage","title":"2. Memory Storage","text":"<p>The <code>store</code> method saves memory entries in the vector store backend.</p>"},{"location":"agent/memory/#features","title":"Features:","text":"<ul> <li>Combines textual data and embeddings for efficient storage.</li> <li>Includes metadata for additional context.</li> </ul>"},{"location":"agent/memory/#implementation","title":"Implementation:","text":"<pre><code>await memory_module.store(\n    event=\"User Login\",\n    action=\"Verify Credentials\",\n    outcome=\"Success\",\n    metadata={\"user_id\": 1234}\n)\n</code></pre>"},{"location":"agent/memory/#3-memory-search","title":"3. Memory Search","text":"<p>The <code>search</code> method retrieves similar memory entries by performing a vector similarity search.</p>"},{"location":"agent/memory/#features_1","title":"Features:","text":"<ul> <li>Asynchronous search capability.</li> <li>Configurable <code>top_k</code> parameter to control the number of results.</li> </ul>"},{"location":"agent/memory/#example_1","title":"Example:","text":"<pre><code>results = await memory_module.search(query=\"User Login\", top_k=5)\n</code></pre>"},{"location":"agent/memory/#4-backend-flexibility","title":"4. Backend Flexibility","text":"<p>The module supports multiple backends for vector storage (by default Chroma):</p> <ul> <li> <p>Chroma:</p> <ul> <li>Local &amp; lightweight vector store.</li> <li>Uses persistent storage on disk.</li> </ul> </li> <li> <p>Qdrant:</p> <ul> <li>High-performance distributed vector store.</li> <li>Requires configuration for host, port, and vector size.</li> </ul> </li> </ul>"},{"location":"agent/memory/#backend-initialization","title":"Backend Initialization:","text":"<pre><code>backend = QdrantBackend(\n    collection_name=\"memory_collection\",\n    host=\"localhost\",\n    port=6333,\n    vector_size=512\n)\n</code></pre> <p>Please note: If you want to use Qdrant as a memory backend, you need to have Docker installed for running the Qdrant container. Install this software first (follow the official documentation).</p>"},{"location":"agent/memory/#example-workflow","title":"Example Workflow","text":"<ol> <li> <p>Store Memory:    <pre><code>await memory_module.store(\n    event=\"User Registration\",\n    action=\"Send Confirmation Email\",\n    outcome=\"Email Sent\",\n    metadata={\"user_email\": \"example@example.com\"}\n)\n</code></pre></p> </li> <li> <p>Search Memory:    <pre><code>similar_memories = await memory_module.search(\n    query=\"User Registration\",\n    top_k=3\n)\n</code></pre></p> </li> <li> <p>Backend Initialization:    <pre><code>memory_module = get_memory_module(\n    backend_type=\"qdrant\"\n)\n</code></pre></p> </li> </ol>"},{"location":"agent/memory/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Consistent Data Structure:    Ensure all memory entries follow a standardized structure for consistency.</p> </li> <li> <p>Choose the Right Backend:    Use Qdrant for distributed, scalable setups and ChromaDB for lightweight, local use cases.</p> </li> <li> <p>Asynchronous Operations:    Take advantage of asynchronous methods for efficient execution.</p> </li> </ol> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/overview/","title":"Nevron Overview","text":""},{"location":"agent/overview/#architecture","title":"Architecture","text":"<p>Nevron is an autonomous AI agent built with a modular architecture consisting of several key components that work together to enable intelligent decision-making and task execution.</p> <p></p>"},{"location":"agent/overview/#1-planning-module-q-learning","title":"1. Planning Module (Q-Learning)","text":"<p>The Planning Module serves as the decision-making engine, leveraging Q-Learning, a model-free reinforcement learning algorithm. It determines intelligent actions based on the current system state and optimal action planning through learned patterns. The module continuously adapts to new data via feedback loops while maintaining state-action value mappings for consistency.</p>"},{"location":"agent/overview/#2-feedback-module","title":"2. Feedback Module","text":"<p>The Feedback Module bridges planning and execution by processing the outcomes of system actions. It evaluates the outcomes of the actions and updates the Q-learning model to improve decision-making.</p>"},{"location":"agent/overview/#3-workflows","title":"3. Workflows","text":"<p>Workflows act as the execution layer, translating high-level plans into actionable steps. This module manages task sequences and dependencies. It provides standardized operation patterns, maintaining consistency and efficiency throughout the system.</p> <p>Using workflows, Nevron can perform tasks such as signal analysis or news research.</p>"},{"location":"agent/overview/#4-tools","title":"4. Tools","text":"<p>The Tools module represents the operational toolkit, executing workflow tasks via integrations. It interfaces seamlessly with external services and APIs to deliver concrete implementation capabilities. </p> <p>This enables effective real-world interactions such as sending messages to Telegram or Twitter.</p>"},{"location":"agent/overview/#5-memory-module-chroma","title":"5. Memory Module (Chroma)","text":"<p>The Memory Module, powered by Chroma, serves as a sophisticated vector storage system for the platform. </p> <p>It maintains a persistent history of states and actions, facilitating efficient context retrieval. By storing rewards and learning patterns, it optimizes performance through Qdrant\u2019s advanced vector database capabilities.</p> <p>Alternative to Qdrant, Chroma is a vector database that is easier to setup and use.</p>"},{"location":"agent/overview/#decision-making-process","title":"Decision Making Process","text":"<ol> <li> <p>State Assessment</p> <ul> <li>Nevron evaluates current context</li> <li>Retrieves relevant memories</li> <li>Analyzes available actions</li> </ul> </li> <li> <p>Action Selection</p> <ul> <li>Q-learning model selects optimal action</li> <li>Based on historical performance</li> <li>Considers current state</li> </ul> </li> <li> <p>Execution</p> <ul> <li>Selected workflow is triggered</li> <li>Tools are utilized as needed</li> <li>Results are captured</li> </ul> </li> <li> <p>Feedback Loop</p> <ul> <li>Action outcomes are evaluated</li> <li>Q-learning model is updated</li> <li>Memory is stored for future reference</li> </ul> </li> </ol>"},{"location":"agent/overview/#configuration","title":"Configuration","text":"<p>The agent's behavior can be configured via:</p> <ul> <li>Environment variables</li> <li>Configuration files</li> <li>Q-learning parameters</li> <li>Workflows</li> <li>Tools</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/planning/","title":"Planning Module","text":""},{"location":"agent/planning/#overview","title":"Overview","text":"<p>The Planning Module is a critical component of the Nevron autonomous AI agent. It leverages Q-learning, a model-free reinforcement learning algorithm, to make intelligent decisions based on the current system state.</p> <p>Key capabilities include:</p> <ul> <li>Learning from experience: Adapts behavior based on feedback from the environment.</li> <li>Balancing exploration and exploitation: Chooses between trying new actions (exploration) and relying on known effective actions (exploitation).</li> <li>Planning for the future: Prioritizes long-term success while considering immediate rewards.</li> </ul> <p>The module continuously evolves by adapting to new data through feedback loops, while maintaining consistent state-action value mappings.</p>"},{"location":"agent/planning/#the-planning-module-is-implemented-in-srcplanningplanning_modulepy","title":"The Planning Module is implemented in <code>src/planning/planning_module.py</code>.","text":""},{"location":"agent/planning/#how-it-works","title":"How It Works","text":"<p>The Planning Module empowers the AI agent with sophisticated decision-making capabilities. It utilizes a Q-table to record and update state-action value mappings, which guide the agent's decisions. This process enables the agent to refine its behavior through accumulated experience.</p> <p>Key functionalities include:</p> <ol> <li> <p>Dynamic Adaptability: When environmental conditions change, the module quickly updates the Q-table to learn new behavioral patterns.</p> </li> <li> <p>Autonomous Operations: The agent independently makes informed decisions, reducing the need for constant human intervention.</p> </li> <li> <p>Managing Complexity: Effectively navigates and prioritizes decisions in environments with numerous states and actions.</p> </li> </ol> <p>The Q-table acts as the module's memory, tracking:</p> <ul> <li>States encountered by the agent.</li> <li>Available actions for each state.</li> <li>Expected rewards for each action.</li> </ul> <p>Over time, this table becomes the primary reference for decision-making, ensuring the agent operates effectively in both familiar and new situations.</p>"},{"location":"agent/planning/#core-components","title":"Core Components","text":"<p>The Planning Module's behavior is controlled by several configuration parameters that define its learning dynamics and decision-making capabilities. These parameters are categorized into:</p>"},{"location":"agent/planning/#core-parameters","title":"Core Parameters","text":"<ul> <li><code>actions</code>: Defines the action space available to the agent, typically using the <code>AgentAction</code> enumeration to ensure a comprehensive list of permitted actions.</li> <li><code>q_table_path</code>: Specifies the file path for saving and loading the Q-table, enabling state preservation across sessions.</li> </ul>"},{"location":"agent/planning/#learning-parameters","title":"Learning Parameters","text":"<ul> <li> <p><code>planning_alpha</code> (Learning Rate):</p> <ul> <li>Controls how quickly the agent learns from new experiences.</li> <li>Higher values (e.g., 0.9): Faster learning but potentially less stability.</li> <li>Lower values (e.g., 0.1): Slower but more stable learning.</li> </ul> </li> <li> <p><code>planning_gamma</code> (Discount Factor):</p> <ul> <li>Balances the importance of future rewards versus immediate ones.</li> <li>Values closer to 1: Focus on long-term consequences.</li> <li>Lower values (e.g., 0.5): Emphasis on immediate rewards.</li> </ul> </li> <li> <p><code>planning_epsilon</code> (Exploration Rate):</p> <ul> <li>Determines the balance between exploration (trying new actions) and exploitation (sticking to known strategies).</li> <li>Higher values: Encourages trying new actions in uncertain environments.</li> <li>Lower values: Relies on proven strategies in well-understood scenarios.</li> </ul> </li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/shopify/","title":"Shopify Integration","text":""},{"location":"agent/shopify/#setup","title":"Setup","text":"<ol> <li>Create a Shopify Private App</li> <li>Go to your Shopify admin panel</li> <li>Navigate to Apps &gt; Develop apps</li> <li>Click \"Create an app\"</li> <li>Configure the app permissions (products, orders, inventory)</li> <li> <p>Generate API credentials</p> </li> <li> <p>Get API Credentials</p> </li> <li>Note down the API key</li> <li>Note down the API password (access token)</li> <li>Note your store name </li> <li> <p>Or: create a test store beforehead, Go To Apps &amp; sales channels &gt; install your app &gt; give neccessary permisions &gt; Go To API Credentials and note down API key/password/store name</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SHOPIFY_API_KEY=your_api_key_here\nSHOPIFY_PASSWORD=your_password_here\nSHOPIFY_STORE_NAME=your-store-name\n</code></pre></p> </li> </ol>"},{"location":"agent/shopify/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.shopify import initialize_shopify_client, get_products, get_orders, update_inventory\n\n# Initialize Shopify client\nclient = initialize_shopify_client()\n\n# Get all products\nproducts = get_products(client)\n\n# Get all orders\norders = get_orders(client)\n\n# Update inventory for a product\nupdate_inventory(\n    client=client,\n    product_id=\"product_id_here\",\n    inventory_level=100\n)\n</code></pre>"},{"location":"agent/shopify/#features","title":"Features","text":"<ul> <li>Secure authentication with Shopify API</li> <li>Retrieve product listings and details</li> <li>Access order information</li> <li>Manage inventory levels</li> <li>Error handling and logging</li> <li>SSL verification handling</li> <li>Session management</li> </ul>"},{"location":"agent/shopify/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for creating and updating products</li> <li>Implement customer management</li> <li>Add support for discount codes</li> <li>Add support for handling Shopify webhooks for real-time updates</li> <li>Enable advanced features like creating new products or orders</li> <li>Enhance error handling and retry mechanisms for API interactions</li> <li>Add support for fulfillment operations</li> <li>Implement multi-location inventory management</li> <li>Add support for collection management</li> <li>Implement order processing workflows</li> </ul>"},{"location":"agent/shopify/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/shopify.py</code></p> <p>The implementation uses the official Shopify Python API. For more information, refer to: - Shopify Admin API Documentation - Shopify Python API Library</p>"},{"location":"agent/slack/","title":"Slack Integration","text":""},{"location":"agent/slack/#setup","title":"Setup","text":"<ol> <li>Create a Slack App</li> <li>Go to Slack API Apps page</li> <li>Click \"Create New App\" &gt; \"From scratch\"</li> <li>Choose an app name and workspace</li> <li> <p>Save the app configuration</p> </li> <li> <p>Configure Bot Token and Permissions</p> </li> <li>Navigate to \"OAuth &amp; Permissions\" in your app settings</li> <li>Under \"Scopes\", add these Bot Token Scopes:<ul> <li><code>chat:write</code> (Send messages)</li> <li><code>channels:history</code> (View messages in channels)</li> <li><code>channels:read</code> (View basic channel info)</li> <li><code>im:history</code> (View direct messages)</li> <li><code>users:read</code> (View basic user info)</li> </ul> </li> <li>Click \"Install to Workspace\"</li> <li> <p>Copy the \"Bot User OAuth Token\"</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SLACK_BOT_TOKEN=xoxb-your-bot-token-here\nSLACK_APP_TOKEN=xapp-your-app-token-here\nSLACK_SIGNING_SECRET=your-signing-secret-here\n</code></pre></p> </li> </ol>"},{"location":"agent/slack/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.slack import SlackClient\n\n# Initialize Slack client\nslack = SlackClient()\n\n# Send a message to a channel\nresponse = slack.send_message(\n    channel=\"#general\",\n    text=\"Hello from your AI assistant!\"\n)\n\n# Listen for messages\n@slack.event(\"message\")\ndef handle_message(event):\n    channel = event[\"channel\"]\n    text = event[\"text\"]\n    slack.send_message(channel=channel, text=f\"Received: {text}\")\n</code></pre>"},{"location":"agent/slack/#features","title":"Features","text":"<ul> <li>Real-time message handling with event subscriptions</li> <li>Send and receive messages in channels and DMs</li> <li>Process message threads and replies</li> <li>Support for rich message formatting and blocks</li> <li>Message history and context management</li> <li>User and channel information retrieval</li> <li>Efficient caching of API client</li> </ul>"},{"location":"agent/slack/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for Slack modals and interactive components</li> <li>Implement slash commands</li> <li>Add support for message reactions</li> <li>Implement file management features</li> <li>Add support for user presence tracking</li> <li>Implement workspace analytics</li> <li>Add support for app home customization</li> <li>Implement message scheduling features</li> <li>Manage interactive components such as buttons and menus</li> <li>Enhance error handling and retry mechanisms for API interactions</li> </ul>"},{"location":"agent/slack/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/slack.py</code></p> <p>The implementation uses the official Slack Bolt Framework. For more information, refer to: - Slack API Documentation - Slack Bolt Python Framework</p>"},{"location":"agent/slack/#bot-memory","title":"Bot Memory","text":"<p>The Slack integration maintains conversation history through a message store that: - Tracks message threads and their context - Stores recent interactions per channel/user - Maintains conversation state for ongoing dialogues - Implements memory cleanup for older messages - Supports context retrieval for follow-up responses</p> <p>Example of history usage: <pre><code># Access conversation history\nhistory = slack.get_conversation_history(channel_id)\n\n# Get context for a specific thread\nthread_context = slack.get_thread_context(thread_ts)\n\n# Store custom context\nslack.store_context(\n    channel_id=channel,\n    thread_ts=thread,\n    context={\"key\": \"value\"}\n)\n</code></pre></p>"},{"location":"agent/spotify/","title":"Spotify Integration","text":""},{"location":"agent/spotify/#setup","title":"Setup","text":"<ol> <li>Create Spotify Developer Account</li> <li>Go to Spotify Developer Dashboard</li> <li>Log in with your Spotify account or create one</li> <li>Click \"Create an App\"</li> <li> <p>Fill in the app name and description</p> </li> <li> <p>Generate API Credentials</p> </li> <li>From your app's dashboard, click \"Settings\"</li> <li>Note your Client ID and Client Secret</li> <li>Add your redirect URI (if using remote server, can be chosen default)</li> <li> <p>Set app permissions under \"Scopes\"</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SPOTIFY_CLIENT_ID=your_client_id_here\nSPOTIFY_CLIENT_SECRET=your_client_secret_here\n</code></pre></p> </li> </ol>"},{"location":"agent/spotify/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.spotify import SpotifyTool\n\n# Initialize Spotify client\nspotify = SpotifyTool()\n\n# Authenticate and get access token\naccess_token = await spotify.authenticate()\n\n# Search for songs\nsongs = await spotify.search_song(\n    access_token=access_token,\n    query=\"Never Gonna Give You Up\",\n    limit=1\n)\n\n# Get user playlists\nplaylists = await spotify.get_user_playlists(\n    access_token=access_token\n)\n\n# Control playback\nawait spotify.control_playback(\n    access_token=access_token,\n    action=\"play\"  # or \"pause\", \"skip\"\n)\n</code></pre>"},{"location":"agent/spotify/#features","title":"Features","text":"<ul> <li>Client Credentials OAuth2 authentication</li> <li>Search for songs with customizable result limits</li> <li>Retrieve user playlists and details</li> <li>Control music playback (play, pause, skip)</li> <li>Error handling with custom SpotifyError class</li> <li>Async/await support for better performance</li> <li>Secure API communication with proper authentication</li> </ul>"},{"location":"agent/spotify/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for real-time listening session tracking</li> <li>Implement (collaborative) playlist creation and management</li> <li>Add track analysis and audio features</li> <li>Support for podcast playback</li> <li>Implement device management: handle multi-device playback scenarios</li> <li>Implement recommendation engine</li> </ul>"},{"location":"agent/spotify/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/spotify.py</code></p> <p>The implementation uses the official Spotify Web API. For more information, refer to: - Spotify Web API Documentation - Spotify Authorization Guide</p>"},{"location":"agent/tavily/","title":"Tavily Integration","text":""},{"location":"agent/tavily/#setup","title":"Setup","text":"<ol> <li>Get Tavily API Key</li> <li>Go to Tavily Dashboard</li> <li>Sign up or log in to your account</li> <li>Navigate to API section</li> <li> <p>Copy your API key</p> </li> <li> <p>Configure Environment Variables    Add this to your <code>.env</code> file:    <pre><code>TAVILY_API_KEY=your_api_key_here\n</code></pre></p> </li> </ol>"},{"location":"agent/tavily/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.tavily import initialize_tavily_client, execute_search, parse_search_results\n\n# Initialize Tavily client\ntavily_client = await initialize_tavily_client(api_key=\"your_api_key_here\")\n\n# Execute search\nresults = await execute_search(\n    client=tavily_client,\n    query=\"Python programming\"\n)\n\n# Parse results\nparsed_results = parse_search_results(results)\n</code></pre>"},{"location":"agent/tavily/#features","title":"Features","text":"<ul> <li>Execute web searches with customizable filters</li> <li>Support for both basic and advanced search depths</li> <li>Domain filtering (include/exclude specific domains)</li> <li>Configurable maximum results</li> <li>Relevance scoring for search results</li> <li>Published date information when available</li> <li>Asynchronous search execution</li> <li>SSL verification handling for development</li> </ul>"},{"location":"agent/tavily/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for image search</li> <li>Implement news-specific search</li> <li>Add support for time-based filtering</li> <li>Implement search analytics</li> <li>Add support for custom search engines</li> <li>Implement batch search capabilities</li> <li>Add support for semantic search</li> <li>Implement result caching</li> </ul>"},{"location":"agent/tavily/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/tavily.py</code></p> <p>The implementation uses the official Tavily API. For more information, refer to: - Tavily API Documentation - Tavily Python SDK</p>"},{"location":"agent/tools/","title":"Tools","text":""},{"location":"agent/tools/#overview","title":"Overview","text":"<p>Tools are integral components that empower Nevron to interact with external services and execute specific actions. Each tool is designed to handle distinct functionality, enhancing the agent's versatility and capability. Tools act as modular utilities that can be seamlessly integrated into workflows to perform specialized tasks. They enable connectivity with external services, third-party APIs, search engines, and custom functions, making Nevron highly adaptable and efficient.</p> <p>All tools are organized within the <code>src/tools/</code> directory.</p>"},{"location":"agent/tools/#how-it-works","title":"How It Works","text":"<p>Tools in Nevron are specialized modules that handle specific tasks within the system, integrating with workflows to provide seamless interactions with external platforms. Here's how the tools work at a high level:</p> <ol> <li> <p>Integration with Workflows:    Tools serve as reusable components that workflows rely on for executing key tasks such as publishing content, fetching data, or processing input.</p> </li> <li> <p>Purpose-Built Functionality:    Each tool is uniquely designed to address a specific need, such as interacting with Twitter, Telegram, or external APIs. This ensures workflows remain focused and efficient.</p> </li> <li> <p>Technical Features:    All tools share the following core capabilities:</p> </li> <li>Error Handling: Tools catch and log errors clearly, ensuring smooth operation.</li> <li>Logging: Use <code>loguru</code> for consistent, detailed logs.</li> <li>Configuration Management: Centralized settings allow easy updates and customization.</li> <li>Asynchronous Execution: Async/await ensures non-blocking performance.</li> <li>Custom Exceptions: Each tool defines specific error types for clarity (e.g., <code>TwitterError</code>, <code>TelegramError</code>, <code>APIError</code>).</li> </ol>"},{"location":"agent/tools/#available-tools","title":"Available Tools","text":""},{"location":"agent/tools/#1-twitter-integration-twitterpy","title":"1. Twitter Integration (<code>twitter.py</code>)","text":"<p>The Twitter tool automates content publishing to Twitter using both v1.1 and v2 of the Twitter API.</p>"},{"location":"agent/tools/#features","title":"Features:","text":"<ul> <li>Supports image uploads and tweet threads.</li> <li>Combines the strengths of v1.1 (better for media) and v2 (better for posting).</li> <li>Smart rate-limiting with 3-second delays between tweets.</li> </ul>"},{"location":"agent/tools/#how-it-works_1","title":"How It Works:","text":"<ol> <li>For Media:<ul> <li>Downloads the image.</li> <li>Converts it to grayscale.</li> <li>Uploads it to Twitter.</li> </ul> </li> <li>For Tweet Threads:<ul> <li>Posts tweets sequentially, linking them together as a thread.</li> <li>Delays are added between posts to avoid API rate limits.</li> </ul> </li> <li>Output:<ul> <li>Returns the status of each posted tweet.</li> </ul> </li> </ol>"},{"location":"agent/tools/#2-telegram-integration-tgpy","title":"2. Telegram Integration (<code>tg.py</code>)","text":"<p>The Telegram tool simplifies posting content to a specific Telegram channel while managing message length constraints.</p>"},{"location":"agent/tools/#features_1","title":"Features:","text":"<ul> <li>Smart message splitting for handling Telegram's character limit.</li> <li>HTML formatting support, including links.</li> </ul>"},{"location":"agent/tools/#how-it-works_2","title":"How It Works:","text":"<ol> <li>Takes an HTML-formatted message.</li> <li>Splits the message into smaller chunks if it exceeds Telegram's length limit.</li> <li>Posts each chunk sequentially.</li> <li>Returns the IDs of all successfully posted messages.</li> </ol>"},{"location":"agent/tools/#3-perplexity-search-tool-search_with_perplexitypy","title":"3. Perplexity Search Tool (<code>search_with_perplexity.py</code>)","text":"<p>The Perplexity tool leverages AI to search for cryptocurrency-related news using Perplexity's advanced API.</p>"},{"location":"agent/tools/#features_2","title":"Features:","text":"<ul> <li>Powered by the \"llama-3.1-sonar-small-128k-online\" model.</li> <li>Tracks token usage and estimates costs ($0.2 per million tokens).</li> </ul>"},{"location":"agent/tools/#how-it-works_3","title":"How It Works:","text":"<ol> <li>Takes a search query as input.</li> <li>Sends the query to Perplexity's API with preconfigured settings (temperature 0.3, top_p 0.8).</li> <li>Receives AI-processed search results.</li> <li>Formats the results for easy consumption and returns them.</li> </ol>"},{"location":"agent/tools/#how-to-add-a-new-tool","title":"How to Add a New Tool?","text":"<p>Adding a new tool to Nevron is a straightforward process. Follow these steps:</p> <ol> <li> <p>Create a New File:    Add a new Python file in the <code>src/tools/</code> directory (e.g., <code>new_tool.py</code>).</p> </li> <li> <p>Implement the Tool's Functionality:    Define the tool's purpose and logic, adhering to Nevron's modular architecture.</p> </li> <li> <p>Test Thoroughly:    Ensure the tool works as expected by writing and running tests.</p> </li> <li> <p>Integrate into Workflows:    Once the tool is ready, it can be imported and used in workflows to enhance functionality.</p> </li> </ol>"},{"location":"agent/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Ensure all tools implement robust error handling for smooth operation.</li> <li>Logging: Use <code>loguru</code> to maintain detailed and consistent logs across all tools.</li> <li>Asynchronous Execution: Leverage <code>asyncio</code> where applicable for non-blocking performance.</li> <li>Reusability: Design tools as modular and reusable components for seamless integration into multiple workflows.</li> </ol> <p>Nevron's tools form the backbone of its ability to automate tasks, interact with external systems, and deliver actionable insights.</p> <p>From publishing on social media to conducting advanced AI-driven searches, tools are modular, reusable, and integral to the agent's success.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/whatsapp/","title":"WhatsApp Integration","text":""},{"location":"agent/whatsapp/#setup","title":"Setup","text":"<ol> <li>Create WhatsApp API Account</li> <li>Go to Green API</li> <li>Register and create an account</li> <li>Create an instance</li> <li> <p>Get your Instance ID and API Token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>WHATSAPP_ID_INSTANCE=your_instance_id_here\nWHATSAPP_API_TOKEN=your_api_token_here\n</code></pre></p> </li> <li> <p>Phone Number Setup</p> </li> <li>Install WhatsApp on your phone</li> <li>Scan QR code from Green API dashboard</li> <li>Verify your phone number is connected</li> </ol>"},{"location":"agent/whatsapp/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.whatsapp import WhatsAppClient\n\n# Initialize WhatsApp client\nwhatsapp = WhatsAppClient()\nawait whatsapp.initialize()\n\n# Define message handler\nasync def handle_message(message_data):\n    if message_data['content'].lower() == 'hello':\n        await whatsapp.send_message(\n            recipient_id=message_data['phone'],\n            content=\"Hello! How can I help you?\"\n        )\n\n# Start listening for messages\nawait whatsapp.listen_to_messages(callback=handle_message)\n</code></pre>"},{"location":"agent/whatsapp/#example-usage","title":"Example Usage","text":"<p>A WhatsApp bot that handles incoming messages:</p> <pre><code># Send a message\nmessage_id = await whatsapp.send_message(\n    recipient_id=\"1234567890\",\n    content=\"Hello from Nevron!\"\n)\n\n# Format phone numbers\nformatted_number = format_phone_number(\"+1-234-567-890\")  # Returns \"1234567890@c.us\"\n</code></pre> <p>This example demonstrates: - Message sending - Message listening - Phone number formatting - Basic interaction flow</p>"},{"location":"agent/whatsapp/#features","title":"Features","text":"<ul> <li>Send messages to WhatsApp users</li> <li>Listen for incoming messages</li> <li>Format incorrect phone numbers for API automatically</li> <li>Handle message notifications</li> <li>Automatic notification cleanup from the queue</li> <li>Error handling and logging</li> <li>SSL verification management (currently disabled)</li> </ul>"},{"location":"agent/whatsapp/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for media messages</li> <li>Implement group chat functionality</li> <li>Enable message reactions and interactive message components.</li> <li>Add message status tracking</li> <li>Support for business features</li> <li>Add contact management</li> <li>Implement message templates</li> <li>Add support for voice messages</li> <li>Enable message scheduling</li> </ul>"},{"location":"agent/whatsapp/#reference","title":"Reference","text":"<p>For implementation details, see: tools/whatsapp.py</p>"},{"location":"agent/workflows/","title":"Workflows","text":""},{"location":"agent/workflows/#overview","title":"Overview","text":"<p>Workflows are the backbone of Nevron\u2019s logic, defining tasks and coordinating tools and modules to deliver actionable outcomes. Each workflow is designed for autonomy and efficiency, allowing the agent to function in diverse scenarios seamlessly.</p>"},{"location":"agent/workflows/#available-workflows","title":"Available Workflows","text":""},{"location":"agent/workflows/#1-analyze-signal","title":"1. Analyze Signal","text":"<p>Focuses on processing actionable signals, such as market trends or data feeds, and publishing insights through communication channels like Twitter.</p> <p>Features: - Fetches and validates signals. - Analyzes data using a Large Language Model (LLM). - Publishes concise updates (e.g., tweets).</p> <p>Location: <code>src/workflows/analyze_signal.py</code></p>"},{"location":"agent/workflows/#2-research-news","title":"2. Research News","text":"<p>Specializes in gathering, analyzing, and summarizing news content for dissemination.</p> <p>Features: - Collects and validates news articles. - Uses LLMs for analysis and contextualization. - Publishes summaries through predefined channels.</p> <p>Location: <code>src/workflows/research_news.py</code></p>"},{"location":"agent/workflows/#workflow-architecture","title":"Workflow Architecture","text":"<p>Workflows in Nevron follow a modular design to ensure scalability and consistency:</p> <ol> <li>Input Validation: Ensures data completeness.</li> <li>Core Execution: Processes the workflow logic.</li> <li>Result Aggregation: Collects outputs for further use.</li> <li>Memory Storage: Saves results for future reference.</li> <li>Feedback Integration: Logs outcomes for iterative learning.</li> </ol>"},{"location":"agent/workflows/#integration-points","title":"Integration Points","text":"<p>Workflows rely on Nevron\u2019s modular components for execution:</p> <ul> <li>Tools:</li> <li>Signal fetching for data processing.</li> <li>Automated publishing to Twitter.</li> <li> <p>News gathering for insights.</p> </li> <li> <p>Modules:</p> </li> <li>Planning Module: Guides decision-making.</li> <li>Memory Module: Provides context from past events.</li> <li>LLM Integration: Drives analysis and content generation.</li> <li>Feedback Module: Tracks performance for improvement.</li> </ul>"},{"location":"agent/workflows/#how-to-add-a-new-workflow","title":"How to Add a New Workflow?","text":"<ol> <li>Create a New File:</li> <li> <p>Add a Python file in <code>src/workflows/</code> (e.g., <code>new_workflow.py</code>).</p> </li> <li> <p>Define the Workflow Class:    <pre><code>from src.workflows.base import BaseWorkflow\n\nclass NewWorkflow(BaseWorkflow):\n    name = \"new_workflow\"\n</code></pre></p> </li> <li> <p>Implement Logic:    <pre><code>def execute(self):\n    # Core workflow functionality\n    logger.info(\"Executing workflow\")\n\ndef validate(self):\n    # Input validation logic\n    logger.info(\"Validating workflow input\")\n</code></pre></p> </li> <li> <p>Register Workflow:    <pre><code>from src.workflows.new_workflow import NewWorkflow\nagent.register_workflow(NewWorkflow())\n</code></pre></p> </li> <li> <p>Test:</p> <ul> <li>Write unit tests for components.</li> <li>Validate integration with the agent.</li> <li>Test edge cases and error handling.</li> </ul> </li> </ol>"},{"location":"agent/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Error Handling:</p> <ul> <li>Anticipate edge cases.</li> <li>Provide clear error messages for debugging.</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Log critical workflow steps for monitoring.</li> <li>Track execution times and performance metrics.</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Unit tests for components.</li> <li>Integration tests to verify module and tool interactions.</li> <li>Simulate failure scenarios for robustness.</li> </ul> </li> </ol> <p>Nevron\u2019s workflows enable seamless automation and intelligent decision-making, forming the core of its adaptability and efficiency. For further details, refer to the GitHub Discussions.</p>"},{"location":"agent/youtube/","title":"YouTube Integration","text":""},{"location":"agent/youtube/#setup","title":"Setup","text":"<ol> <li>Enable YouTube Data API</li> <li>Go to Google Cloud Console</li> <li>Navigate to APIs &amp; Services &gt; Library</li> <li> <p>Search for and enable \"YouTube Data API v3\"</p> </li> <li> <p>Generate API Key</p> </li> <li>Go to APIs &amp; Services &gt; Credentials</li> <li>Click \"Create Credentials\" &gt; \"API key\"</li> <li>Copy the generated API key</li> <li> <p>(Optional) Restrict the API key to YouTube Data API v3 only</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>YOUTUBE_API_KEY=your_api_key_here\nYOUTUBE_CHANNEL_ID=your_channel_id\n</code></pre></p> </li> </ol>"},{"location":"agent/youtube/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.youtube import YouTubeClient\n\n# Initialize YouTube client\nyoutube = YouTubeClient()\n\n# Search for videos\nresults = youtube.search_videos(\n    query=\"Python programming\",\n    max_results=5\n)\n\n# Get video details\nvideo_details = youtube.get_video_details(\n    video_id=\"video_id_here\"\n)\n</code></pre>"},{"location":"agent/youtube/#features","title":"Features","text":"<ul> <li>Search for videos with custom queries and filters</li> <li>Retrieve detailed video information (title, description, statistics)</li> <li>Access video comments and engagement metrics</li> <li>Get channel statistics and details</li> <li>Fetch playlist information</li> <li>Efficient caching of API client</li> </ul>"},{"location":"agent/youtube/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for video uploads and management</li> <li>Implement live streaming capabilities</li> <li>Add caption/subtitle handling</li> <li>Support for video analytics and reporting</li> <li>Implement playlist creation and management</li> <li>Add support for channel management</li> <li>Implement comment moderation features</li> <li>Add support for video categories and tags</li> </ul>"},{"location":"agent/youtube/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/youtube.py</code></p> <p>The implementation uses the official YouTube Data API v3. For more information, refer to: - YouTube Data API Documentation - Google API Client Library for Python</p>"},{"location":"development/contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to Nevron! This guide will help you get started with contributing to our project.</p>"},{"location":"development/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Ways to Contribute</li> <li>Development Process</li> <li>Code Style and Standards</li> <li>Pull Request Process</li> <li>Getting Help</li> </ul>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ol> <li>Code Contributions</li> <li>Bug fixes</li> <li>New features</li> <li>Performance improvements</li> <li> <p>Documentation improvements</p> </li> <li> <p>Non-Code Contributions</p> </li> <li>Reporting bugs</li> <li>Suggesting enhancements</li> <li>Improving documentation</li> <li>Answering questions in discussions</li> </ol>"},{"location":"development/contributing/#development-process","title":"Development Process","text":"<ol> <li>Find or Create an Issue</li> <li>Check existing issues</li> <li>Look for <code>good first issue</code> or <code>help wanted</code> labels</li> <li> <p>If you want to work on something new, create an issue first to discuss it</p> </li> <li> <p>Fork and Clone <pre><code>git clone https://github.com/YOUR-USERNAME/nevron.git\ncd nevron\n</code></pre></p> </li> <li> <p>Create a Branch</p> </li> <li>Create a new branch for your work:     <pre><code>git checkout -b feature/issue-number-description\n# Example: feature/42-add-redis-cache\n</code></pre></li> <li> <p>Use meaningful branch names, preferably referencing the issue number</p> </li> <li> <p>Make Changes</p> </li> <li>Write your code</li> <li>Follow the project's coding standards</li> <li>Keep commits atomic and write meaningful commit messages</li> <li> <p>Test your changes thoroughly</p> </li> <li> <p>Test Your Changes</p> </li> <li>Run formatting checks:      <pre><code>make format\n</code></pre></li> <li>Run linting checks:      <pre><code>make lint\n</code></pre></li> <li> <p>Run tests:      <pre><code>make test\n</code></pre></p> </li> <li> <p>Push and Create PR</p> </li> <li>Push your changes to your fork:      <pre><code>git push origin feature/issue-number-description\n</code></pre></li> <li>Then create a Pull Request on GitHub.</li> </ol>"},{"location":"development/contributing/#code-style-and-standards","title":"Code Style and Standards","text":""},{"location":"development/contributing/#python-standards","title":"Python Standards","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Follow PEP 484 for type hints</li> <li>Follow PEP 257 for docstrings</li> <li>Use Python 3.12+ features and patterns</li> </ul>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive names that reflect purpose</li> <li>Variables and functions: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Constants: <code>UPPER_CASE_WITH_UNDERSCORES</code></li> <li>Private attributes/methods: prefix with single underscore <code>_private_method</code></li> <li>\"Magic\" methods: surrounded by double underscores <code>__str__</code></li> <li>Type variable names: <code>PascalCase</code> preferably single letters (T, K, V)</li> </ul>"},{"location":"development/contributing/#code-organization","title":"Code Organization","text":"<ul> <li>One class per file unless classes are closely related</li> <li>Group related functionality into modules</li> <li>Use absolute imports</li> <li>Order imports as: standard library, third-party, local</li> <li>Use <code>isort</code> for import sorting</li> <li>Maximum line length: 100 characters</li> <li>Use 4 spaces for indentation (no tabs)</li> </ul>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>All public APIs must have docstrings</li> <li>Use Google-style docstring format:   <pre><code>def function_name(param1: str, param2: int) -&gt; bool:\n    \"\"\"Short description of function.\n\n    Longer description if needed.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: Description of when this error occurs\n    \"\"\"\n</code></pre></li> <li>Include type hints for all function arguments and return values</li> <li>Document exceptions that may be raised</li> <li>Keep comments focused on why, not what</li> <li>Update documentation when changing code</li> </ul>"},{"location":"development/contributing/#code-quality","title":"Code Quality","text":"<ul> <li>Keep functions small and focused (preferably under 50 lines)</li> <li>Maximum function arguments: 5</li> <li>Use early returns to reduce nesting</li> <li>Avoid global variables</li> <li>Use constants instead of magic numbers</li> <li>Handle all possible exceptions appropriately</li> <li>Use context managers (<code>with</code> statements) for resource management</li> <li>Use f-strings for string formatting</li> <li>Use list/dict/set comprehensions when they improve readability</li> </ul>"},{"location":"development/contributing/#testing-standards","title":"Testing Standards","text":"<ul> <li>Write tests for all new code</li> <li>Maintain minimum 90% test coverage</li> <li>Follow Arrange-Act-Assert pattern</li> <li>Use meaningful test names that describe the scenario</li> <li>One assertion per test when possible</li> <li>Use pytest fixtures for common setup</li> <li>Mock external dependencies</li> <li>Test edge cases and error conditions</li> </ul>"},{"location":"development/contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use appropriate data structures</li> <li>Avoid unnecessary object creation</li> <li>Use generators for large datasets</li> <li>Profile code when performance is critical</li> <li>Consider memory usage</li> <li>Use <code>collections</code> module specialized containers when appropriate</li> </ul>"},{"location":"development/contributing/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Never commit sensitive data (API keys, passwords)</li> <li>Use environment variables for configuration</li> <li>Validate all input data</li> <li>Use secure defaults</li> <li>Follow OWASP security guidelines</li> <li>Use <code>secrets</code> module for cryptographic operations</li> </ul>"},{"location":"development/contributing/#version-control","title":"Version Control","text":"<ul> <li>Write meaningful commit messages</li> <li>One logical change per commit</li> <li>Reference issue numbers in commits</li> <li>Keep commits small and focused</li> <li>Rebase feature branches on main before PR</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#pr-title-format","title":"PR Title Format","text":"<p>Title should be concise and descriptive.</p>"},{"location":"development/contributing/#pr-description-should-include","title":"PR Description Should Include","text":"<ul> <li>Reference to related issue(s)</li> <li>Clear description of changes</li> <li>Breaking changes (if any)</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>At least one maintainer approval required</li> <li>All review comments must be resolved</li> <li>Documentation must be updated</li> </ol>"},{"location":"development/contributing/#merging","title":"Merging","text":"<ul> <li>Always squash commits before merging</li> <li>Merge into <code>dev</code> branch first</li> <li>Once <code>dev</code> is stable, merge into <code>main</code> (done by maintainers)</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Join our Discussions</li> <li>Ask questions in issue comments</li> <li>Tag maintainers if stuck</li> </ul> <p>Remember: No contribution is too small, and all contributions are valued! </p>"},{"location":"development/environment/","title":"Environment Variables","text":"<p>This page lists all available environment variables that can be configured in Nevron.</p>"},{"location":"development/environment/#general-settings","title":"General Settings","text":""},{"location":"development/environment/#project-settings","title":"Project Settings","text":"<ul> <li><code>ENVIRONMENT</code>: Environment type (Production, Development, CI). Default: <code>PRODUCTION</code></li> <li><code>PROJECT_NAME</code>: Project name. Default: <code>autonomous-agent</code></li> </ul>"},{"location":"development/environment/#planning-settings","title":"Planning Settings","text":"<ul> <li><code>PERSISTENT_Q_TABLE_PATH</code>: Path to the persistent Q-table file. Default: <code>persistent_q_table.json</code></li> <li><code>PLANNING_ALPHA</code>: Learning rate. Default: <code>0.1</code></li> <li><code>PLANNING_GAMMA</code>: Discount factor. Default: <code>0.95</code></li> <li><code>PLANNING_EPSILON</code>: Exploration rate. Default: <code>0.1</code></li> </ul>"},{"location":"development/environment/#memory-settings","title":"Memory Settings","text":"<ul> <li><code>MEMORY_BACKEND_TYPE</code>: Memory backend type (<code>chroma</code> or <code>qdrant</code>). Default: <code>chroma</code></li> <li><code>MEMORY_COLLECTION_NAME</code>: Memory collection name. Default: <code>agent_memory</code></li> <li><code>MEMORY_HOST</code>: Memory host (Qdrant only). Default: <code>localhost</code></li> <li><code>MEMORY_PORT</code>: Memory port (Qdrant only). Default: <code>6333</code></li> <li><code>MEMORY_VECTOR_SIZE</code>: Memory vector size (Qdrant only). Default: <code>1536</code></li> <li><code>MEMORY_PERSIST_DIRECTORY</code>: Memory persist directory (ChromaDB only). Default: <code>.chromadb</code></li> </ul>"},{"location":"development/environment/#llm-settings","title":"LLM Settings","text":"<ul> <li><code>LLM_PROVIDER</code>: LLM provider type (<code>openai</code>, <code>anthropic</code>, <code>xai</code>, <code>llama</code>). Default: <code>openai</code></li> </ul>"},{"location":"development/environment/#anthropic","title":"Anthropic","text":"<ul> <li><code>ANTHROPIC_API_KEY</code>: Anthropic API key</li> <li><code>ANTHROPIC_MODEL</code>: Anthropic model name. Default: <code>claude-2</code></li> </ul>"},{"location":"development/environment/#openai","title":"OpenAI","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key</li> <li><code>OPENAI_MODEL</code>: OpenAI model name. Default: <code>gpt-4o-mini</code></li> <li><code>OPENAI_EMBEDDING_MODEL</code>: OpenAI embedding model. Default: <code>text-embedding-3-small</code></li> </ul>"},{"location":"development/environment/#xai","title":"xAI","text":"<ul> <li><code>XAI_API_KEY</code>: xAI API key</li> <li><code>XAI_MODEL</code>: xAI model name. Default: <code>grok-2-latest</code></li> </ul>"},{"location":"development/environment/#llama","title":"Llama","text":"<ul> <li><code>LLAMA_API_KEY</code>: Llama API key (required for Fireworks, Llama API, and OpenRouter providers)</li> <li><code>LLAMA_MODEL_NAME</code>: Llama model name for Fireworks. Default: <code>llama3-8b-8192</code></li> <li><code>LLAMA_PROVIDER</code>: Llama provider type (<code>fireworks</code>, <code>ollama</code>, <code>llama-api</code>, or <code>openrouter</code>). Default: <code>ollama</code></li> </ul>"},{"location":"development/environment/#ollama-settings","title":"Ollama Settings","text":"<ul> <li><code>LLAMA_OLLAMA_URL</code>: Ollama API URL. Default: <code>http://localhost:11434</code></li> <li><code>LLAMA_OLLAMA_MODEL</code>: Ollama model name. Default: <code>llama3.2:latest</code></li> </ul>"},{"location":"development/environment/#llama-api-settings","title":"Llama API Settings","text":"<ul> <li><code>LLAMA_API_BASE_URL</code>: Llama API base URL. Default: <code>https://api.llama-api.com</code></li> <li><code>LLAMA_API_MODEL</code>: Llama API model name. Default: <code>llama3.1-70b</code></li> </ul>"},{"location":"development/environment/#openrouter-settings","title":"OpenRouter Settings","text":"<ul> <li><code>LLAMA_OPENROUTER_URL</code>: OpenRouter API URL. Default: <code>https://openrouter.ai/api/v1</code></li> <li><code>LLAMA_OPENROUTER_MODEL</code>: OpenRouter model name. Default: <code>meta-llama/llama-3.2-1b-instruct</code></li> <li><code>LLAMA_OPENROUTER_SITE_URL</code>: Optional. Site URL for rankings on openrouter.ai</li> <li><code>LLAMA_OPENROUTER_SITE_NAME</code>: Optional. Site name for rankings on openrouter.ai</li> </ul>"},{"location":"development/environment/#agent-settings","title":"Agent Settings","text":""},{"location":"development/environment/#core-settings","title":"Core Settings","text":"<ul> <li><code>AGENT_PERSONALITY</code>: Description of agent's personality</li> <li><code>AGENT_GOAL</code>: Agent's primary goal</li> <li><code>AGENT_REST_TIME</code>: Rest time between actions in seconds. Default: <code>300</code></li> </ul>"},{"location":"development/environment/#integration-settings","title":"Integration Settings","text":""},{"location":"development/environment/#telegram","title":"Telegram","text":"<ul> <li><code>TELEGRAM_BOT_TOKEN</code>: Telegram bot token</li> <li><code>TELEGRAM_CHAT_ID</code>: Telegram chat ID for main channel/group</li> </ul>"},{"location":"development/environment/#twitter","title":"Twitter","text":"<ul> <li><code>TWITTER_API_KEY</code>: Twitter API key</li> <li><code>TWITTER_API_SECRET_KEY</code>: Twitter API secret key</li> <li><code>TWITTER_ACCESS_TOKEN</code>: Twitter access token</li> <li><code>TWITTER_ACCESS_TOKEN_SECRET</code>: Twitter access token secret</li> </ul>"},{"location":"development/environment/#perplexity","title":"Perplexity","text":"<ul> <li><code>PERPLEXITY_API_KEY</code>: Perplexity API key</li> <li><code>PERPLEXITY_ENDPOINT</code>: Perplexity endpoint. Default: <code>https://api.perplexity.ai/chat/completions</code></li> <li><code>PERPLEXITY_NEWS_PROMPT</code>: Custom prompt for news search</li> <li><code>PERPLEXITY_NEWS_CATEGORY_LIST</code>: List of news categories to search</li> </ul>"},{"location":"development/environment/#coinstats","title":"Coinstats","text":"<ul> <li><code>COINSTATS_API_KEY</code>: Coinstats API key</li> </ul>"},{"location":"development/environment/#discord","title":"Discord","text":"<ul> <li><code>DISCORD_BOT_TOKEN</code>: Discord bot token</li> <li><code>DISCORD_CHANNEL_ID</code>: Discord channel ID</li> </ul>"},{"location":"development/environment/#youtube","title":"YouTube","text":"<ul> <li><code>YOUTUBE_API_KEY</code>: YouTube API key</li> <li><code>YOUTUBE_PLAYLIST_ID</code>: YouTube playlist ID</li> </ul>"},{"location":"development/environment/#whatsapp","title":"WhatsApp","text":"<ul> <li><code>WHATSAPP_ID_INSTANCE</code>: WhatsApp instance ID</li> <li><code>WHATSAPP_API_TOKEN</code>: WhatsApp API token </li> </ul>"},{"location":"development/setup/","title":"Development Setup","text":""},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>pipenv</li> <li>make (for using Makefile commands)</li> <li>git</li> </ul>"},{"location":"development/setup/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/axioma-ai-labs/nevron.git\ncd nevron\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>make deps\n</code></pre></p> </li> </ol> <p>This will: - Install pipenv if not present - Install all project dependencies - Set up pre-commit hooks</p>"},{"location":"development/setup/#environment-setup","title":"Environment Setup","text":"<p>The project uses pipenv for dependency management. Key commands:</p> <pre><code># Activate virtual environment\npipenv shell\n\n# Install a new package\npipenv install package_name\n\n# Install a development package\npipenv install --dev package_name\n</code></pre>"},{"location":"development/setup/#ide-setup","title":"IDE Setup","text":""},{"location":"development/setup/#vscode","title":"VSCode","text":"<p>Recommended extensions: - Python - ruff - isort - GitLens</p> <p>Recommended settings (<code>settings.json</code>): <pre><code>{\n    \"python.formatting.provider\": \"ruff\",\n    \"editor.formatOnSave\": true,\n    \"python.linting.enabled\": true,\n    \"python.linting.mypyEnabled\": true\n}\n</code></pre></p>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ul> <li>Enable ruff formatter</li> <li>Set Python interpreter to the pipenv environment</li> <li>Enable mypy for type checking</li> </ul>"},{"location":"development/workflow/","title":"Development Workflow","text":""},{"location":"development/workflow/#code-style","title":"Code Style","text":"<p>Follow the Code Style and Standards section.</p>"},{"location":"development/workflow/#testing","title":"Testing","text":""},{"location":"development/workflow/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npipenv run pytest tests/path/to/test_file.py\n\n# Run with coverage\nmake coverage\n</code></pre>"},{"location":"development/workflow/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Follow the same directory structure as the source code</li> <li>Name test files with <code>test_</code> prefix</li> <li>Use pytest fixtures for common setup</li> <li>Aim for 100% test coverage for new code</li> </ul>"},{"location":"development/workflow/#debugging","title":"Debugging","text":""},{"location":"development/workflow/#local-debugging","title":"Local Debugging","text":"<ol> <li> <p>Use Python debugger (pdb): <pre><code>import pdb; pdb.set_trace()\n</code></pre></p> </li> <li> <p>VSCode debugging:</p> </li> <li>Set breakpoints in the editor</li> <li>Use the Run and Debug panel</li> <li>Configure <code>launch.json</code> for your specific needs</li> </ol>"},{"location":"development/workflow/#logging","title":"Logging","text":"<ul> <li>Use the loguru module</li> <li>Configure log levels appropriately (debug, info, warning, error, critical)</li> </ul>"},{"location":"development/workflow/#github-workflows","title":"GitHub Workflows","text":"<p>The project uses several GitHub Actions workflows:</p>"},{"location":"development/workflow/#main-workflow-mainyml","title":"Main Workflow (<code>main.yml</code>)","text":"<ul> <li>Runs on every push and pull request</li> <li>Performs linting and testing</li> <li>Checks code formatting</li> <li>Generates coverage report</li> </ul>"},{"location":"development/workflow/#documentation-workflow-deploy-docsyml","title":"Documentation Workflow (<code>deploy-docs.yml</code>)","text":"<ul> <li>Builds and deploys documentation to GitHub Pages</li> <li>Triggers on pushes to main branch</li> <li>Uses MkDocs Material theme</li> </ul>"},{"location":"development/workflow/#docker-workflow-dockeryml","title":"Docker Workflow (<code>docker.yml</code>)","text":"<ul> <li>Builds and pushes Docker images</li> <li>Runs on releases and main branch pushes</li> <li>Tags images appropriately</li> </ul>"},{"location":"development/workflow/#release-process","title":"Release Process","text":"<ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update CHANGELOG.md</li> <li>Create a new release on GitHub</li> <li>Tag the release following semantic versioning</li> <li>CI/CD will automatically:</li> <li>Build and test the package</li> <li>Deploy documentation</li> <li>Push Docker images</li> <li>Publish to PyPI </li> </ol>"}]}