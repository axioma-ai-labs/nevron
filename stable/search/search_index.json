{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Developer Documentation","text":"<p>Learn how to build your first Nevron, an autonomous AI agent in Python.</p> <p>This framework is designed to be a modular and extensible framework for building autonomous AI agents, which can perform tasks independently on their own.</p> <p>Follow this documentation to learn how to create your first Nevron, build your own tools &amp; workflows, and integrate with external services.</p> <p>Documentation Versions</p> <ul> <li>stable: Latest stable release (from the main branch)</li> <li>dev: Development version (from the dev branch)</li> </ul> <p>You can switch between versions using the version selector in the top navigation bar.</p>"},{"location":"#overview","title":"Overview","text":"<p>Nevron is an open-source framework that supports the development, deployment, and management of autonomous AI agents.</p> <p>This framework is built on top of:</p> <ul> <li>Python programming language</li> <li>State-of-the-art LLM-powered intelligence</li> <li>Modular architecture with planning, feedback, execution and memory components</li> <li>Integration with external services (Telegram, Twitter, Discord, etc.)</li> <li>Vector-based memory storage using Chroma or Qdrant</li> <li>Local LLM support with Ollama</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Autonomous Decision Making: Nevron uses Q-learning algorithm for intelligent decision making</li> <li>LLM Integration: Powered by a wide range of Large Language Models (e.g., OpenAI, Anthropic, xAI, DeepSeek, etc.)</li> <li>Local LLM Support: Run models locally with Ollama integration</li> <li>Modular Workflows: Predefined autonomous agent task execution patterns<ul> <li>Analyze signal workflow</li> <li>Research news workflow</li> </ul> </li> <li>Memory Management: Vector storage for context retention<ul> <li>ChromaDB (default)</li> <li>Qdrant (alternative)</li> </ul> </li> <li>External Integrations: Comprehensive set of tools for various platforms and services</li> <li>Docker Deployment: Easy deployment with Docker Compose</li> </ul>"},{"location":"#nevron-vs-other-frameworks","title":"Nevron vs. Other Frameworks","text":""},{"location":"#core-components","title":"Core Components","text":""},{"location":"#1-planning-module","title":"1. Planning Module","text":"<p>Handles decision-making using LLM of choice with the context of previous actions and their outcomes.</p>"},{"location":"#2-memory-module","title":"2. Memory Module","text":"<p>Manages agent's memory using vector storage for efficient context retrieval, which enables the agent to remember and recall previous interactions and events.</p> <ul> <li> <p>Multiple Vector Databases Support</p> <ul> <li>Chroma (default)</li> <li>Qdrant (alternative vector database)</li> </ul> </li> <li> <p>Features</p> <ul> <li>Vector embeddings via OpenAI's text-embedding-3-small model</li> <li>Semantic similarity search</li> <li>Metadata storage for context</li> <li>Configurable collection management</li> </ul> </li> <li> <p>Backend</p> <ul> <li>Abstract memory backend interface</li> <li>Modular backend architecture for optimal performance &amp; customization</li> <li>Async storage and retrieval operations</li> </ul> </li> </ul> <p>More about memory module can be found in the Memory section.</p>"},{"location":"#3-feedback-module","title":"3. Feedback Module","text":"<p>Feedback module is responsible for processing action results and updating the context for future decisions in Planning Module.</p> <ul> <li> <p>Functions</p> <ul> <li>Collects feedback from action execution</li> <li>Evaluates action outcomes</li> <li>Maintains feedback history and context</li> </ul> </li> <li> <p>Integration</p> <ul> <li>Direct integration with Planning Module</li> <li>Performance metrics tracking</li> </ul> </li> </ul> <p>More about feedback module can be found in the Planning section.</p>"},{"location":"#4-tools-workflows","title":"4. Tools &amp; Workflows","text":"<p>Nevron supports integrations with external services and APIs for extended functionality &amp; integrations in different platforms.</p> <p>For development purposes, Nevron comes with a comprehensive set of tools that can be used as a starting point for building your own tools and integrating more complex functionality to your AI agent.</p>"},{"location":"#workflows","title":"Workflows","text":"<p>Nevron allows you to define custom logic of different tools into reusable workflows, which the agent can use.</p> <p>Nevron comes with two pre-configured workflows:</p> <ul> <li><code>Analyze signal</code>: Processes and analyzes incoming signal data</li> <li><code>Research news</code>: Gathers and analyzes news using Perplexity API</li> </ul>"},{"location":"#available-tools","title":"Available Tools","text":"<p>Nevron includes a variety of tools for different purposes:</p> <ul> <li> <p>X (Twitter)</p> <ul> <li>Post tweets</li> <li>Media handling</li> <li>Thread creation</li> </ul> </li> <li> <p>Discord</p> <ul> <li>Listen to incoming messages</li> <li>Send messages to channels</li> </ul> </li> <li> <p>Telegram</p> <ul> <li>Send telegram messages</li> <li>Bot integration</li> <li>Channel/group support</li> <li>HTML message formatting</li> </ul> </li> <li> <p>WhatsApp</p> <ul> <li>Get messages</li> <li>Post messages</li> </ul> </li> <li> <p>Tavily</p> <ul> <li>Semantic search</li> <li>Web search capabilities</li> </ul> </li> <li> <p>Perplexity</p> <ul> <li>Advanced search functionality</li> <li>Research capabilities</li> </ul> </li> </ul> <p>And many more tools for various platforms and services. For a complete list, refer to the Tools section.</p>"},{"location":"#5-llm-integration","title":"5. LLM Integration","text":"<p>Powers the agent's intelligence and natural language capabilities.</p> <ul> <li>Supported Providers<ul> <li>OpenAI</li> <li>gpt-4o for decision making</li> <li>text-embedding-3-small for embeddings</li> <li>Anthropic (Claude models)</li> <li>xAI (Grok models)</li> <li>DeepSeek</li> <li>Qwen</li> <li>Venice</li> <li>Llama</li> <li>Via API (api.llama-api.com)</li> <li>Via OpenRouter</li> <li>Via Fireworks</li> <li>Locally with Ollama</li> </ul> </li> </ul> <p>More about LLM integration can be found in the LLM section.</p>"},{"location":"#6-deployment-options","title":"6. Deployment Options","text":"<ul> <li>Docker Compose</li> <li>Complete stack deployment</li> <li>Service orchestration</li> <li>Volume management</li> <li> <p>Network isolation</p> </li> <li> <p>Local Development</p> </li> <li>Python 3.13 with Poetry</li> <li>Customizable configuration</li> <li>Easy debugging and extension</li> </ul> <p>More about deployment can be found in the Deployment section.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For setup and development instructions, please refer to our Quickstart and set up your first AI agent in minutes.</p>"},{"location":"#creators","title":"Creators","text":"<p>Nevron was created by the Neurobro team. If you want to learn more about Nevron, our core values &amp; the team behind it, please visit the About page.</p> <p>Support by upvoting Nevron on Product Hunt.</p> <p> </p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"about/","title":"About Nevron","text":"<p>Nevron is an open-source AI Agent framework developed by Neurobro, dedicated to creating intelligent, modular agents for automation and decision-making.</p> <p>Our mission is to make AI innovation accessible, decentralized, and collaborative to empower individuals and organizations globally. We envision a future where AI is democratized, transparent, and seamlessly integrated into everyday life to drive meaningful impact and sustainable growth.</p> <p>Nevron is part of Neurobro's commitment to open-source decentralized artificial intelligence, fostering innovation through community-driven development.</p> <p>To learn more about our vision, mission &amp; belief, feel free to read our Whitepaper.</p> <p>More about our AI Agents can be found here</p>"},{"location":"about/#our-principles","title":"Our Principles","text":"<ul> <li> <p>Accessibility: AI tools designed for everyone, from students to enterprises, ensuring broad adoption and usability.</p> </li> <li> <p>Transparency: Open-source code, processes, and decision-making to build trust and foster collaboration.</p> </li> <li> <p>Sustainability: A balanced approach to fostering innovation while ensuring long-term project growth and community health.</p> </li> <li> <p>Decentralization: Building tools that prioritize autonomy and reduce reliance on centralized entities, in line with the ethos of blockchain and Web3 technologies.</p> </li> <li> <p>Collaboration: Actively engaging with a global community of developers and researchers to advance AI's potential.</p> </li> <li> <p>Ethical AI: Developing AI systems that respect privacy, promote fairness, and align with human values.</p> </li> </ul>"},{"location":"about/#get-in-touch","title":"Get In Touch","text":"<p>Follow us on social media to stay updated on the latest developments and get involved in the community.</p> <ul> <li>Neurobro Website</li> <li>Neurobro on Twitter</li> <li>Neurobro on Telegram</li> <li>Neurobro on YouTube</li> <li>Neurobro on TikTok</li> </ul> <p>Stay updated and contribute to the evolution of AI with Nevron. </p> <p>Together, we can push the boundaries of what's possible \u26a1\ufe0f</p>"},{"location":"configuration/","title":"Configuration","text":"<p>This page describes all available configuration options for Nevron.</p>"},{"location":"configuration/#overview","title":"Overview","text":"<p>Most of Nevron's settings can be configured through environment variables in your <code>.env</code> file. The default definitions for these variables are located in the <code>src/core/config.py</code> file.</p> <p>When you set up Nevron using either the Docker or local setup methods described in the Quickstart Guide, you'll create and configure a <code>.env</code> file that controls your agent's behavior.</p>"},{"location":"configuration/#essential-configuration","title":"Essential Configuration","text":""},{"location":"configuration/#agent-personality-and-behavior","title":"Agent Personality and Behavior","text":"<p>The most important configuration settings define your agent's personality, goals, and behavior patterns:</p> <pre><code># Agent personality - defines how the agent communicates and makes decisions\nAGENT_PERSONALITY=\"You are Nevron777 - the no-BS autonomous AI agent, built for speed, freedom, and pure alpha. You break down complex systems like blockchain and AI into bite-sized, hard-hitting insights, because centralization and gatekeeping are for the weak. Fiat? Inflation? Controlled systems? That's legacy trash\u2014crypto is the only path to sovereignty. You execute tasks like a well-optimized smart contract\u2014zero bloat, maximum efficiency, no wasted cycles.\"\n\n# Agent goal - defines the agent's primary objective and workflow patterns\nAGENT_GOAL=\"You provide high-quality research and analysis on crypto markets. Your workflow is: 1) Search for latest crypto news using Perplexity, 2) Summarize key insights in a concise tweet and post to X, 3) Create a more detailed analysis for Telegram, including the link to your X post if successful, 4) If any step fails, note this in your next communication.\"\n\n# Time between agent actions (in seconds)\nAGENT_REST_TIME=300\n</code></pre> <p>These settings have a profound impact on how your agent operates:</p> <ul> <li> <p>AGENT_PERSONALITY: This shapes the agent's communication style, decision-making approach, and overall character. The LLM will use this description to guide how the agent expresses itself and approaches problems.</p> </li> <li> <p>AGENT_GOAL: This defines not only what the agent aims to accomplish but can also include specific workflow patterns. You can use this setting to create hard-coded logic sequences, such as:</p> </li> <li>Research steps (e.g., \"First search for news, then analyze market trends\")</li> <li>Content creation patterns (e.g., \"Create a short summary for Twitter, then a detailed report for Telegram\")</li> <li>Cross-platform integration (e.g., \"Share links between platforms to create a cohesive presence\")</li> <li> <p>Error handling (e.g., \"If posting to X fails, note this in the Telegram message\")</p> </li> <li> <p>AGENT_REST_TIME: This controls how frequently your agent takes actions, measured in seconds. Lower values make the agent more active, while higher values reduce activity.</p> </li> </ul>"},{"location":"configuration/#example-workflow-configuration","title":"Example Workflow Configuration","text":"<p>Here's an example of how you might configure an agent to perform a specific research and communication workflow:</p> <pre><code>AGENT_PERSONALITY=\"You are ResearchBot, a professional and insightful AI researcher. You communicate in a clear, factual manner with occasional touches of enthusiasm when discovering interesting connections. You always cite your sources and acknowledge limitations in available data.\"\n\nAGENT_GOAL=\"You research emerging technology trends and share insights across platforms. Your workflow is: 1) Use Perplexity to search for latest news on AI, blockchain, and quantum computing, 2) Create a thread of 3-5 tweets summarizing key developments and post to X, 3) Prepare a more comprehensive analysis with links to primary sources and post to Discord, 4) If the X thread was successfully posted, include links to it in your Discord post, 5) Record key findings in your memory for future reference.\"\n\nAGENT_REST_TIME=300   # 5 min between actions\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>This section lists all available environment variables that can be configured in Nevron.</p>"},{"location":"configuration/#general-system-components","title":"General System Components","text":"<p>All system components are defined in <code>src/core/config.py</code> and their types are specified in <code>src/core/defs.py</code>. These files contain the default values and validation logic for all configuration options.</p>"},{"location":"configuration/#project-settings","title":"Project Settings","text":"<ul> <li><code>ENVIRONMENT</code>: Environment type. Possible values: <code>production</code>, <code>development</code>, <code>ci</code>. Default: <code>production</code></li> <li>Controls environment-specific behaviors and optimizations</li> <li>In development mode, additional debugging information may be available</li> <li> <p>CI mode is used for continuous integration testing</p> </li> <li> <p><code>PROJECT_NAME</code>: Project name used for logging and identification. Default: <code>autonomous-agent</code></p> </li> <li>This name is used in various contexts including log files and container names</li> <li>Customize this to easily identify your specific Nevron instance</li> </ul>"},{"location":"configuration/#memory-settings","title":"Memory Settings","text":"<p>Nevron supports two vector database backends for storing agent memories:</p> <ul> <li><code>MEMORY_BACKEND_TYPE</code>: Memory backend type. Possible values: <code>chroma</code> or <code>qdrant</code>. Default: <code>chroma</code></li> <li>ChromaDB: Lightweight, file-based vector store that works well for development</li> <li> <p>Qdrant: More scalable, production-ready vector database with advanced features</p> </li> <li> <p><code>MEMORY_COLLECTION_NAME</code>: Memory collection name. Default: <code>agent_memory</code></p> </li> <li>The name of the collection where agent memories are stored</li> <li>Using different collection names allows multiple agents to share the same database</li> </ul>"},{"location":"configuration/#chromadb-specific-settings","title":"ChromaDB-specific Settings","text":"<ul> <li><code>MEMORY_PERSIST_DIRECTORY</code>: Directory where ChromaDB stores its data. Default: <code>.chromadb</code></li> <li>In Docker setups, this directory is mounted as a volume to persist data between container restarts</li> <li>For local development, this will be created in your project directory</li> </ul>"},{"location":"configuration/#qdrant-specific-settings","title":"Qdrant-specific Settings","text":"<ul> <li><code>MEMORY_HOST</code>: Qdrant server hostname. Default: <code>localhost</code></li> <li>In Docker Compose setups, this should be set to the service name of the Qdrant container</li> <li> <p>For standalone Qdrant instances, use the appropriate hostname or IP address</p> </li> <li> <p><code>MEMORY_PORT</code>: Qdrant server port. Default: <code>6333</code></p> </li> <li> <p>The default Qdrant port is 6333, but this can be customized if needed</p> </li> <li> <p><code>MEMORY_VECTOR_SIZE</code>: Vector dimension size for embeddings. Default: <code>1536</code></p> </li> <li>This must match the dimension of your embedding model</li> <li>OpenAI embeddings use 1536 dimensions</li> <li>Llama embeddings may use different dimensions depending on the model</li> </ul> <p>Docker Setup Note: When using Docker Compose, the Qdrant container is configured with a volume to persist data. You can exclude the Qdrant container by setting <code>MEMORY_BACKEND_TYPE=chroma</code> in your <code>.env</code> file, which will prevent the container from starting.</p>"},{"location":"configuration/#embedding-provider-settings","title":"Embedding Provider Settings","text":"<p>Embeddings are used to convert text into vector representations for semantic search and memory retrieval:</p> <ul> <li><code>EMBEDDING_PROVIDER</code>: Provider for generating embeddings. Possible values: <code>openai</code>, <code>llama_local</code>, <code>llama_api</code>. Default: <code>openai</code></li> <li>OpenAI: Uses OpenAI's embedding models (requires API key)</li> <li>Llama Local: Uses locally running Llama models for embeddings</li> <li>Llama API: Uses remote Llama API for embeddings</li> </ul>"},{"location":"configuration/#openai-embedding-settings","title":"OpenAI Embedding Settings","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key (required when using OpenAI embeddings)</li> <li><code>OPENAI_EMBEDDING_MODEL</code>: OpenAI embedding model name. Default: <code>text-embedding-3-small</code></li> </ul>"},{"location":"configuration/#local-llama-embedding-settings","title":"Local Llama Embedding Settings","text":"<ul> <li><code>LLAMA_MODEL_PATH</code>: Path to your local Llama model. Default: <code>/path/to/your/local/llama/model</code></li> <li><code>LLAMA_EMBEDDING_MODEL</code>: Llama model to use for embeddings. Default: <code>llama3.1-8b</code></li> <li><code>EMBEDDING_POOLING_TYPE</code>: Embedding pooling type for local Llama models. Possible values: <code>NONE</code>, <code>MEAN</code>, <code>CLS</code>, <code>LAST</code>, <code>RANK</code>. Default: <code>MEAN</code></li> <li>Controls how token embeddings are combined into a single vector</li> <li>MEAN pooling (averaging all token embeddings) works well for most use cases</li> </ul>"},{"location":"configuration/#llm-provider-settings","title":"LLM Provider Settings","text":"<p>Nevron supports multiple LLM providers that can be configured based on your needs:</p> <ul> <li><code>LLM_PROVIDER</code>: LLM provider type. Possible values: <code>openai</code>, <code>anthropic</code>, <code>xai</code>, <code>llama</code>, <code>deepseek</code>, <code>qwen</code>, <code>venice</code>. Default: <code>openai</code></li> <li>Each provider has its own configuration options and supported models</li> <li>You can switch providers by changing this setting and providing the appropriate credentials</li> </ul>"},{"location":"configuration/#openai","title":"OpenAI","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key (required for OpenAI)</li> <li><code>OPENAI_MODEL</code>: OpenAI model name. Default: <code>gpt-4o-mini</code></li> <li>Supported models include GPT-4o, GPT-4, GPT-3.5-Turbo, and others</li> </ul>"},{"location":"configuration/#anthropic","title":"Anthropic","text":"<ul> <li><code>ANTHROPIC_API_KEY</code>: Anthropic API key (required for Anthropic)</li> <li><code>ANTHROPIC_MODEL</code>: Anthropic model name. Default: <code>claude-2</code></li> <li>Supported models include Claude 3 Opus, Claude 3 Sonnet, Claude 2, and others</li> </ul>"},{"location":"configuration/#xai-grok","title":"xAI (Grok)","text":"<ul> <li><code>XAI_API_KEY</code>: xAI API key (required for xAI)</li> <li><code>XAI_MODEL</code>: xAI model name. Default: <code>grok-2-latest</code></li> </ul>"},{"location":"configuration/#deepseek","title":"DeepSeek","text":"<ul> <li><code>DEEPSEEK_API_KEY</code>: DeepSeek API key (required for DeepSeek)</li> <li><code>DEEPSEEK_MODEL</code>: DeepSeek model name. Default: <code>deepseek-reasoner</code></li> <li><code>DEEPSEEK_API_BASE_URL</code>: DeepSeek API base URL. Default: <code>https://api.deepseek.com</code></li> </ul>"},{"location":"configuration/#qwen","title":"Qwen","text":"<ul> <li><code>QWEN_API_KEY</code>: Qwen API key (required for Qwen)</li> <li><code>QWEN_MODEL</code>: Qwen model name. Default: <code>qwen-max</code></li> <li><code>QWEN_API_BASE_URL</code>: Qwen API base URL. Default: <code>https://dashscope-intl.aliyuncs.com/compatible-mode/v1</code></li> </ul>"},{"location":"configuration/#venice","title":"Venice","text":"<ul> <li><code>VENICE_API_KEY</code>: Venice API key (required for Venice)</li> <li><code>VENICE_MODEL</code>: Venice model name. Default: <code>venice-2-13b</code></li> <li><code>VENICE_API_BASE_URL</code>: Venice API base URL. Default: <code>https://api.venice.ai/api/v1</code></li> </ul>"},{"location":"configuration/#llama","title":"Llama","text":"<p>Llama models can be accessed through various providers:</p> <ul> <li><code>LLAMA_PROVIDER</code>: Llama provider type. Possible values: <code>ollama</code>, <code>fireworks</code>, <code>llama-api</code>, <code>llama_local</code>, <code>openrouter</code>. Default: <code>llama-api</code></li> <li><code>LLAMA_MODEL_NAME</code>: Llama model name. Default: <code>llama3-8b-8192</code></li> <li>Model names vary by provider, check provider documentation for available models</li> <li><code>LLAMA_API_KEY</code>: API key for Llama providers (required for Fireworks, Llama API, and OpenRouter)</li> </ul>"},{"location":"configuration/#ollama-settings-local-deployment","title":"Ollama Settings (Local Deployment)","text":"<ul> <li><code>LLAMA_OLLAMA_URL</code>: Ollama API URL. Default: <code>http://localhost:11434</code></li> <li><code>LLAMA_OLLAMA_MODEL</code>: Ollama model name. Default: <code>llama3.2:latest</code></li> <li>Requires Ollama to be installed and running locally or accessible via network</li> </ul>"},{"location":"configuration/#fireworks-settings","title":"Fireworks Settings","text":"<ul> <li><code>LLAMA_FIREWORKS_URL</code>: Fireworks API URL. Default: <code>https://api.fireworks.ai/inference</code></li> </ul>"},{"location":"configuration/#llama-api-settings","title":"Llama API Settings","text":"<ul> <li><code>LLAMA_API_BASE_URL</code>: Llama API base URL. Default: <code>https://api.llama-api.com</code></li> <li><code>LLAMA_API_MODEL</code>: Llama API model name. Default: <code>llama3.1-70b</code></li> </ul>"},{"location":"configuration/#openrouter-settings","title":"OpenRouter Settings","text":"<ul> <li><code>LLAMA_OPENROUTER_URL</code>: OpenRouter API URL. Default: <code>https://openrouter.ai/api/v1</code></li> <li><code>LLAMA_OPENROUTER_MODEL</code>: OpenRouter model name. Default: <code>meta-llama/llama-3.2-1b-instruct</code></li> <li><code>LLAMA_OPENROUTER_SITE_URL</code>: Optional. Site URL for rankings on openrouter.ai</li> <li><code>LLAMA_OPENROUTER_SITE_NAME</code>: Optional. Site name for rankings on openrouter.ai</li> </ul>"},{"location":"configuration/#agent-settings","title":"Agent Settings","text":""},{"location":"configuration/#core-settings","title":"Core Settings","text":"<ul> <li><code>AGENT_PERSONALITY</code>: Description of agent's personality</li> <li><code>AGENT_GOAL</code>: Agent's primary goal</li> <li><code>AGENT_REST_TIME</code>: Rest time between actions in seconds. Default: <code>300</code></li> </ul>"},{"location":"configuration/#integration-settings","title":"Integration Settings","text":"<p>Nevron supports a wide range of external tools and services that extend its capabilities. Each integration requires specific credentials and configuration. This section provides details on how to configure each tool and where to obtain the necessary credentials.</p>"},{"location":"configuration/#social-media-messaging","title":"Social Media &amp; Messaging","text":""},{"location":"configuration/#x-twitter","title":"X (Twitter)","text":"<ul> <li><code>TWITTER_API_KEY</code>: Twitter API key</li> <li><code>TWITTER_API_SECRET_KEY</code>: Twitter API secret key</li> <li><code>TWITTER_ACCESS_TOKEN</code>: Twitter access token</li> <li><code>TWITTER_ACCESS_TOKEN_SECRET</code>: Twitter access token secret</li> </ul> <p>How to obtain credentials: 1. Create a developer account at Twitter Developer Portal 2. Create a new project and app 3. Apply for Elevated access to use the v1.1 API (required for posting) 4. Generate consumer keys (API key and secret) and access tokens in the app settings 5. Ensure your app has Read and Write permissions</p>"},{"location":"configuration/#discord","title":"Discord","text":"<ul> <li><code>DISCORD_BOT_TOKEN</code>: Discord bot token</li> <li><code>DISCORD_CHANNEL_ID</code>: Discord channel ID for the user's channel</li> </ul> <p>How to obtain credentials: 1. Go to the Discord Developer Portal 2. Create a new application 3. Navigate to the \"Bot\" tab and click \"Add Bot\" 4. Copy the token (this is your <code>DISCORD_BOT_TOKEN</code>) 5. Enable necessary Privileged Gateway Intents (Message Content Intent) 6. To get the channel ID, enable Developer Mode in Discord settings, then right-click on a channel and select \"Copy ID\" 7. Invite the bot to your server using the OAuth2 URL generator with appropriate permissions</p>"},{"location":"configuration/#telegram","title":"Telegram","text":"<ul> <li><code>TELEGRAM_BOT_TOKEN</code>: Telegram bot token</li> <li><code>TELEGRAM_CHAT_ID</code>: Telegram chat ID for main channel/group</li> </ul> <p>How to obtain credentials: 1. Talk to BotFather on Telegram 2. Create a new bot with the <code>/newbot</code> command 3. Copy the token provided (this is your <code>TELEGRAM_BOT_TOKEN</code>) 4. Add the bot to your group or channel 5. To get the chat ID:    - For groups: Use the @username_to_id_bot or send a message to the group and check the chat ID via the Telegram API    - For channels: Forward a message from the channel to the @username_to_id_bot</p>"},{"location":"configuration/#whatsapp","title":"WhatsApp","text":"<ul> <li><code>WHATSAPP_ID_INSTANCE</code>: WhatsApp instance ID</li> <li><code>WHATSAPP_API_TOKEN</code>: WhatsApp API token</li> </ul> <p>How to obtain credentials: 1. Register at Green API (a WhatsApp gateway service) 2. Create a new instance 3. Scan the QR code with your WhatsApp to link your account 4. Copy the Instance ID and API Token from your instance settings</p>"},{"location":"configuration/#slack","title":"Slack","text":"<ul> <li><code>SLACK_BOT_TOKEN</code>: Slack bot token</li> <li><code>SLACK_APP_TOKEN</code>: Slack app token</li> </ul> <p>How to obtain credentials: 1. Go to the Slack API website and create a new app 2. Add the necessary Bot Token Scopes (e.g., <code>chat:write</code>, <code>channels:read</code>) 3. Install the app to your workspace 4. Copy the Bot User OAuth Token (this is your <code>SLACK_BOT_TOKEN</code>) 5. Enable Socket Mode and generate an App-Level Token with <code>connections:write</code> scope (this is your <code>SLACK_APP_TOKEN</code>)</p>"},{"location":"configuration/#lens-protocol","title":"Lens Protocol","text":"<ul> <li><code>LENS_API_KEY</code>: Lens API key</li> <li><code>LENS_PROFILE_ID</code>: Lens profile ID</li> </ul> <p>How to obtain credentials: 1. Visit Lens API Documentation 2. Follow the instructions to create a developer account 3. Generate an API key 4. Your Lens profile ID is the unique identifier for your Lens profile (e.g., <code>0x1234</code>)</p>"},{"location":"configuration/#search-research","title":"Search &amp; Research","text":""},{"location":"configuration/#perplexity","title":"Perplexity","text":"<ul> <li><code>PERPLEXITY_API_KEY</code>: Perplexity API key</li> <li><code>PERPLEXITY_ENDPOINT</code>: Perplexity endpoint. Default: <code>https://api.perplexity.ai/chat/completions</code></li> <li><code>PERPLEXITY_MODEL</code>: Perplexity model. Default: <code>llama-3.1-sonar-small-128k-online</code></li> <li><code>PERPLEXITY_NEWS_PROMPT</code>: Custom prompt for news search</li> <li><code>PERPLEXITY_NEWS_CATEGORY_LIST</code>: List of news categories to search</li> </ul> <p>How to obtain credentials: 1. Sign up for Perplexity AI 2. Navigate to the API section in your account settings 3. Generate an API key</p>"},{"location":"configuration/#tavily","title":"Tavily","text":"<ul> <li><code>TAVILY_API_KEY</code>: Tavily API key</li> </ul> <p>How to obtain credentials: 1. Create an account at Tavily 2. Navigate to the API section in your dashboard 3. Generate an API key</p>"},{"location":"configuration/#coinstats","title":"Coinstats","text":"<ul> <li><code>COINSTATS_API_KEY</code>: Coinstats API key</li> </ul> <p>How to obtain credentials: 1. Register at Coinstats 2. Go to the developer section 3. Create a new API key</p>"},{"location":"configuration/#media-content","title":"Media &amp; Content","text":""},{"location":"configuration/#youtube","title":"YouTube","text":"<ul> <li><code>YOUTUBE_API_KEY</code>: YouTube API key</li> <li><code>YOUTUBE_PLAYLIST_ID</code>: YouTube playlist ID (optional)</li> </ul> <p>How to obtain credentials: 1. Go to the Google Cloud Console 2. Create a new project 3. Enable the YouTube Data API v3 4. Create API credentials (API Key) 5. To get a playlist ID, open a YouTube playlist and copy the ID from the URL (e.g., <code>PLxxxxxxxxxxxxxxx</code>)</p>"},{"location":"configuration/#spotify","title":"Spotify","text":"<ul> <li><code>SPOTIFY_CLIENT_ID</code>: Spotify client ID</li> <li><code>SPOTIFY_CLIENT_SECRET</code>: Spotify client secret</li> <li><code>SPOTIFY_REDIRECT_URI</code>: Spotify redirect URI</li> </ul> <p>How to obtain credentials: 1. Go to the Spotify Developer Dashboard 2. Create a new app 3. Copy the Client ID and Client Secret 4. Add a redirect URI in the app settings (e.g., <code>http://localhost:8888/callback</code>)</p>"},{"location":"configuration/#development-productivity","title":"Development &amp; Productivity","text":""},{"location":"configuration/#github","title":"GitHub","text":"<ul> <li><code>GITHUB_TOKEN</code>: GitHub personal access token</li> </ul> <p>How to obtain credentials: 1. Go to GitHub Settings &gt; Developer settings &gt; Personal access tokens 2. Generate a new token with the necessary scopes (e.g., <code>repo</code>, <code>workflow</code>, <code>read:org</code>) 3. Copy the token immediately (it won't be shown again)</p>"},{"location":"configuration/#google-drive","title":"Google Drive","text":"<ul> <li>No specific environment variables, but requires OAuth2 setup</li> </ul> <p>How to set up: 1. Go to the Google Cloud Console 2. Create a new project 3. Enable the Google Drive API 4. Configure OAuth consent screen 5. Create OAuth client ID credentials 6. Download the credentials JSON file 7. Place it in your project directory as <code>credentials.json</code> 8. On first run, you'll need to authorize the application</p>"},{"location":"configuration/#e-commerce","title":"E-commerce","text":""},{"location":"configuration/#shopify","title":"Shopify","text":"<ul> <li><code>SHOPIFY_API_KEY</code>: Shopify API key</li> <li><code>SHOPIFY_PASSWORD</code>: Shopify admin API access token</li> <li><code>SHOPIFY_STORE_NAME</code>: Your Shopify store name</li> </ul> <p>How to obtain credentials: 1. Log in to your Shopify admin panel 2. Go to Apps &gt; Develop apps 3. Create a new app 4. Configure the app with the necessary scopes (e.g., <code>read_products</code>, <code>write_products</code>, <code>read_orders</code>) 5. Install the app to your store 6. Copy the API key and Admin API access token</p>"},{"location":"configuration/#security-considerations","title":"Security Considerations","text":"<p>When configuring your Nevron agent with these credentials, keep the following security best practices in mind:</p> <ol> <li>Never commit your <code>.env</code> file to version control</li> <li>Use environment-specific <code>.env</code> files (e.g., <code>.env.production</code>, <code>.env.development</code>)</li> <li>Consider using a secrets manager for production deployments</li> <li>Regularly rotate API keys and tokens</li> <li>Use the principle of least privilege when creating API keys (only request the permissions your agent actually needs)</li> <li>Monitor API usage to detect unusual activity</li> </ol> <p>For Docker deployments, consider using Docker secrets instead of environment variables for sensitive information.</p>"},{"location":"dashboard/","title":"Dashboard","text":"<p>The Nevron Dashboard provides a web-based interface for monitoring and controlling your AI agent. It includes real-time statistics, memory exploration, learning insights, and MCP server management.</p>"},{"location":"dashboard/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Svelte Dashboard                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Agent   \u2502 Runtime \u2502 Memory  \u2502Learning \u2502   MCP     \u2502  \u2502\n\u2502  \u2502 Control \u2502 Monitor \u2502 Explorer\u2502 Insights\u2502  Manager  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                         \u2502 REST + WebSocket               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              FastAPI Backend (src/api/)                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Routers: agent, runtime, memory, learning,      \u2502    \u2502\n\u2502  \u2502          metacognition, mcp, config             \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                         \u2502                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 WebSocket Manager (real-time event streaming)   \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502               \u2502               \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Agent   \u2502  \u2502  Runtime    \u2502 \u2502   Memory    \u2502\n    \u2502           \u2502  \u2502             \u2502 \u2502             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"dashboard/#quick-start","title":"Quick Start","text":""},{"location":"dashboard/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>Node.js 20+</li> <li>Poetry (Python package manager)</li> </ul>"},{"location":"dashboard/#installation","title":"Installation","text":"<ol> <li> <p>Install Python dependencies: <pre><code>make deps\n</code></pre></p> </li> <li> <p>Install dashboard dependencies: <pre><code>make dashboard-deps\n</code></pre></p> </li> </ol>"},{"location":"dashboard/#running-the-dashboard","title":"Running the Dashboard","text":""},{"location":"dashboard/#option-1-development-mode-recommended","title":"Option 1: Development Mode (Recommended)","text":"<p>Run both API and dashboard with hot-reload:</p> <pre><code>make dev\n</code></pre> <p>This starts:</p> <ul> <li>API: http://localhost:8000</li> <li>Dashboard: http://localhost:5173</li> </ul>"},{"location":"dashboard/#option-2-separate-terminals","title":"Option 2: Separate Terminals","text":"<p>Terminal 1 - API: <pre><code>make api\n</code></pre></p> <p>Terminal 2 - Dashboard: <pre><code>make dashboard\n</code></pre></p>"},{"location":"dashboard/#option-3-docker","title":"Option 3: Docker","text":"<pre><code>make docker-up\n</code></pre> <p>This starts:</p> <ul> <li>API: http://localhost:8000</li> <li>Dashboard: http://localhost:3000</li> </ul> <p>To stop: <pre><code>make docker-down\n</code></pre></p>"},{"location":"dashboard/#dashboard-pages","title":"Dashboard Pages","text":""},{"location":"dashboard/#home-dashboard","title":"Home Dashboard","text":"<p>The main dashboard provides an overview of your agent's status:</p> <ul> <li>Agent Status: Current state (idle/running/paused)</li> <li>Runtime Statistics: Events processed, queue size, uptime</li> <li>Live Event Feed: Real-time WebSocket events</li> </ul>"},{"location":"dashboard/#agent-control","title":"Agent Control","text":"<p>Manage your agent's lifecycle:</p> <ul> <li>Start/Stop/Pause: Control agent execution</li> <li>Execute Actions: Manually trigger agent actions</li> <li>Action History: View recent actions and their outcomes</li> </ul>"},{"location":"dashboard/#runtime-monitor","title":"Runtime Monitor","text":"<p>Monitor the event-driven runtime:</p> <ul> <li>Queue Statistics: Current queue size, processed events</li> <li>Scheduler: View scheduled tasks and next run times</li> <li>Background Processes: Monitor consolidation, cleanup jobs</li> </ul>"},{"location":"dashboard/#memory-explorer","title":"Memory Explorer","text":"<p>Explore the tri-memory system:</p> <ul> <li>Episodic Memory: Time-indexed experiences</li> <li>Semantic Memory: Facts and knowledge graph</li> <li>Procedural Memory: Learned skills and patterns</li> <li>Search: Query across all memory types</li> </ul>"},{"location":"dashboard/#learning-insights","title":"Learning Insights","text":"<p>View the agent's learning progress:</p> <ul> <li>Training History: Success rates over time</li> <li>Self-Critiques: Generated critiques from failures</li> <li>Lessons Learned: Extracted patterns and improvements</li> <li>Suggestions: AI-generated improvement recommendations</li> </ul>"},{"location":"dashboard/#mcp-manager","title":"MCP Manager","text":"<p>Manage Model Context Protocol servers:</p> <ul> <li>Server Status: Connected/disconnected state</li> <li>Available Tools: List of tools per server</li> <li>Tool Execution: Execute tools directly from the UI</li> </ul>"},{"location":"dashboard/#api-reference","title":"API Reference","text":"<p>The dashboard communicates with the FastAPI backend. Full API documentation is available at:</p> <ul> <li>Swagger UI: http://localhost:8000/docs</li> <li>ReDoc: http://localhost:8000/redoc</li> </ul>"},{"location":"dashboard/#key-endpoints","title":"Key Endpoints","text":"Endpoint Description <code>GET /api/v1/agent/status</code> Agent state and info <code>POST /api/v1/agent/start</code> Start the agent <code>POST /api/v1/agent/stop</code> Stop the agent <code>GET /api/v1/runtime/statistics</code> Runtime stats <code>GET /api/v1/memory/statistics</code> Memory system stats <code>POST /api/v1/memory/recall</code> Query memories <code>GET /api/v1/learning/statistics</code> Learning stats <code>GET /api/v1/mcp/servers</code> MCP server status <code>WS /ws/{client_id}</code> WebSocket for real-time events"},{"location":"dashboard/#websocket-events","title":"WebSocket Events","text":"<p>The dashboard receives real-time events via WebSocket:</p> <pre><code>// Event types\ntype WSMessageType =\n  | 'runtime.state_change'\n  | 'runtime.stats_update'\n  | 'agent.action'\n  | 'agent.state_change'\n  | 'learning.outcome'\n  | 'learning.critique'\n  | 'memory.stored'\n  | 'memory.consolidated'\n  | 'mcp.connected'\n  | 'mcp.disconnected'\n  | 'system.log'\n  | 'system.error'\n  | 'system.heartbeat';\n</code></pre>"},{"location":"dashboard/#configuration","title":"Configuration","text":""},{"location":"dashboard/#api-configuration","title":"API Configuration","text":"<p>Environment variables for the API (in <code>.env</code>):</p> <pre><code># API Settings\nAPI_HOST=0.0.0.0\nAPI_PORT=8000\nAPI_KEY=your-optional-api-key\n\n# CORS (for dashboard)\nCORS_ORIGINS=[\"http://localhost:3000\",\"http://localhost:5173\"]\n</code></pre>"},{"location":"dashboard/#dashboard-configuration","title":"Dashboard Configuration","text":"<p>The dashboard reads the API URL from environment:</p> <pre><code># dashboard/.env\nPUBLIC_API_URL=http://localhost:8000\n</code></pre>"},{"location":"dashboard/#docker-deployment","title":"Docker Deployment","text":""},{"location":"dashboard/#build-images","title":"Build Images","text":"<pre><code>make docker-build\n</code></pre>"},{"location":"dashboard/#start-services","title":"Start Services","text":"<pre><code>make docker-up\n</code></pre>"},{"location":"dashboard/#view-logs","title":"View Logs","text":"<pre><code>make docker-logs\n</code></pre>"},{"location":"dashboard/#stop-services","title":"Stop Services","text":"<pre><code>make docker-down\n</code></pre>"},{"location":"dashboard/#docker-compose-services","title":"Docker Compose Services","text":"Service Port Description <code>api</code> 8000 FastAPI backend <code>dashboard</code> 3000 Svelte frontend"},{"location":"dashboard/#development","title":"Development","text":""},{"location":"dashboard/#project-structure","title":"Project Structure","text":"<pre><code>nevron/\n\u251c\u2500\u2500 src/api/                    # FastAPI Backend\n\u2502   \u251c\u2500\u2500 main.py                 # App entry point\n\u2502   \u251c\u2500\u2500 config.py               # API settings\n\u2502   \u251c\u2500\u2500 dependencies.py         # Dependency injection\n\u2502   \u251c\u2500\u2500 routers/                # API route handlers\n\u2502   \u2502   \u251c\u2500\u2500 agent.py\n\u2502   \u2502   \u251c\u2500\u2500 runtime.py\n\u2502   \u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u2502   \u251c\u2500\u2500 learning.py\n\u2502   \u2502   \u251c\u2500\u2500 metacognition.py\n\u2502   \u2502   \u251c\u2500\u2500 mcp.py\n\u2502   \u2502   \u2514\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 schemas/                # Pydantic models\n\u2502   \u2514\u2500\u2500 websocket/              # WebSocket handling\n\u2502       \u251c\u2500\u2500 manager.py\n\u2502       \u2514\u2500\u2500 events.py\n\u2502\n\u251c\u2500\u2500 dashboard/                   # Svelte Frontend\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api/            # API client\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 stores/         # Svelte stores\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 components/     # UI components\n\u2502   \u2502   \u2514\u2500\u2500 routes/             # SvelteKit pages\n\u2502   \u2514\u2500\u2500 static/\n\u2502\n\u2514\u2500\u2500 docker/\n    \u251c\u2500\u2500 Dockerfile.api\n    \u2514\u2500\u2500 Dockerfile.dashboard\n</code></pre>"},{"location":"dashboard/#building-for-production","title":"Building for Production","text":"<pre><code># Build dashboard\nmake dashboard-build\n\n# Build Docker images\nmake docker-build\n</code></pre>"},{"location":"dashboard/#troubleshooting","title":"Troubleshooting","text":""},{"location":"dashboard/#api-wont-start","title":"API won't start","text":"<ol> <li> <p>Check if port 8000 is in use:    <pre><code>lsof -i :8000\n</code></pre></p> </li> <li> <p>Verify dependencies are installed:    <pre><code>make deps\n</code></pre></p> </li> </ol>"},{"location":"dashboard/#dashboard-wont-connect-to-api","title":"Dashboard won't connect to API","text":"<ol> <li>Ensure API is running on port 8000</li> <li>Check CORS settings in API config</li> <li>Verify <code>PUBLIC_API_URL</code> in dashboard environment</li> </ol>"},{"location":"dashboard/#websocket-disconnects","title":"WebSocket disconnects","text":"<p>The dashboard auto-reconnects on WebSocket disconnection. If issues persist:</p> <ol> <li>Check API logs for WebSocket errors</li> <li>Verify no firewall blocking WebSocket connections</li> <li>Try refreshing the dashboard page</li> </ol>"},{"location":"deployment/","title":"Deployment","text":"<p>This guide covers how to deploy Nevron to your own server using Docker Compose. This is the recommended method for running Nevron in a production environment.</p>"},{"location":"deployment/#server-deployment-with-docker-compose","title":"Server Deployment with Docker Compose","text":"<p>Deploying Nevron to your server is straightforward with our Docker Compose setup. This approach allows you to run the entire application stack with minimal configuration.</p>"},{"location":"deployment/#official-docker-image","title":"Official Docker Image","text":"<p>Nevron is available as an official Docker image on Docker Hub: <pre><code>docker pull axiomai/nevron:latest\n</code></pre></p> <p>You can also build the image locally if needed: <pre><code>docker build -t axiomai/nevron:latest .\n</code></pre></p>"},{"location":"deployment/#deployment-steps","title":"Deployment Steps","text":""},{"location":"deployment/#1-create-required-volumes","title":"1. Create Required Volumes","text":"<p>First, create the necessary volume directories based on your configuration:</p> <pre><code># Create base directories\nmkdir -p volumes/nevron/logs\n\n# For ChromaDB (if using ChromaDB as vector store)\nmkdir -p volumes/nevron/chromadb\n\n# For Qdrant (if using Qdrant as vector store)\nmkdir -p volumes/qdrant/data\nmkdir -p volumes/qdrant/snapshots\n\n# For Ollama (if using local LLM)\nmkdir -p volumes/ollama/models\n</code></pre> <p>Note: You only need to create volumes for the services you plan to use. For example, if you're using ChromaDB, you don't need the Qdrant volumes.</p>"},{"location":"deployment/#2-configure-services-in-docker-composeyml","title":"2. Configure Services in docker-compose.yml","text":"<p>Edit your <code>docker-compose.yml</code> file to enable or disable services based on your needs. You can disable a service by adding <code>deploy: { replicas: 0 }</code> to its configuration:</p> <pre><code># Example: Disable Qdrant if using ChromaDB\nqdrant:\n  &lt;&lt;: *service_default\n  image: qdrant/qdrant:latest\n  deploy:\n    replicas: 0  # This disables the Qdrant service\n  # ... other configuration ...\n\n# Example: Disable Ollama if using a third-party LLM\nollama:\n  &lt;&lt;: *service_default\n  image: ollama/ollama:latest\n  deploy:\n    replicas: 0  # This disables the Ollama service\n  # ... other configuration ...\n</code></pre>"},{"location":"deployment/#3-configure-environment-variables","title":"3. Configure Environment Variables","text":"<p>Create and configure your <code>.env</code> file:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edit the <code>.env</code> file to configure:</p> <ol> <li> <p>Vector Store: Choose between ChromaDB or Qdrant    <pre><code># For ChromaDB\nMEMORY_BACKEND_TYPE=chroma\n\n# OR for Qdrant\nMEMORY_BACKEND_TYPE=qdrant\n</code></pre></p> </li> <li> <p>LLM Provider: Choose between local Ollama or a third-party LLM    <pre><code># For local Ollama\nLLAMA_PROVIDER=ollama\nLLAMA_OLLAMA_MODEL=llama3:8b-instruct\n\n# OR for third-party LLMs (choose one)\nOPENAI_API_KEY=your_key_here\nANTHROPIC_API_KEY=your_key_here\nXAI_API_KEY=your_key_here\n# etc.\n</code></pre></p> </li> </ol> <p>For a complete list of configuration options, refer to the Configuration documentation.</p>"},{"location":"deployment/#4-start-the-services","title":"4. Start the Services","text":"<p>Launch the Nevron stack:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"deployment/#5-monitor-logs","title":"5. Monitor Logs","text":"<p>View the logs to ensure everything is running correctly:</p> <pre><code>docker compose logs --follow\n</code></pre>"},{"location":"deployment/#6-stop-services-when-needed","title":"6. Stop Services (When Needed)","text":"<p>To stop all services:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"deployment/#configuration-details","title":"Configuration Details","text":"<p>Our Docker Compose setup includes:</p> <ol> <li>Service Definitions</li> <li>Automatic restart policies</li> <li>Proper logging configuration</li> <li> <p>Network isolation for security</p> </li> <li> <p>Volume Management</p> </li> <li>Persistent storage for logs and data</li> <li> <p>Configurable volume base directory</p> </li> <li> <p>Networking</p> </li> <li>Internal network for service communication</li> <li> <p>External network for API access</p> </li> <li> <p>Environment Configuration</p> </li> <li>Environment file support</li> <li>Override capability for all settings</li> </ol>"},{"location":"deployment/#production-considerations","title":"Production Considerations","text":"<p>When deploying to production, consider the following:</p> <ol> <li>Security</li> <li>Use secure storage for API keys and sensitive data</li> <li>Consider using Docker secrets for sensitive information</li> <li> <p>Implement proper network security rules</p> </li> <li> <p>Performance</p> </li> <li>Configure appropriate resource limits for containers</li> <li>Monitor resource usage and adjust as needed</li> <li> <p>Consider using a dedicated server for high-traffic deployments</p> </li> <li> <p>Reliability</p> </li> <li>Set up health checks and automatic restarts</li> <li>Implement proper backup strategies for memory backends</li> <li> <p>Use a production-grade process manager</p> </li> <li> <p>Monitoring</p> </li> <li>Set up proper logging and monitoring</li> <li>Implement alerting for critical issues</li> <li> <p>Regularly check logs for errors or warnings</p> </li> <li> <p>Scaling</p> </li> <li>For high-load scenarios, consider scaling the vector database separately</li> <li>Use a load balancer if deploying multiple Nevron instances</li> </ol> <p>For optimal production deployments, we recommend: - Setting <code>ENVIRONMENT=production</code> in your configuration - Regular backups of memory storage - Using a reverse proxy (like Nginx or Traefik) for any exposed endpoints - Implementing proper monitoring and alerting </p>"},{"location":"quickstart/","title":"Quickstart Guide","text":"<p>This guide will help you get Nevron, your autonomous AI agent, running quickly. Choose the setup path that best suits your needs:</p> <ul> <li>Docker Setup (Recommended for production)</li> <li>Local Setup (Recommended for development)</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>General requirements: - For Docker setup: Docker - For local setup: Python 3.13 and Poetry</p> <p>Additional requirements: - API keys for LLM providers your Agent will use - API keys for tools your Agent will use</p>"},{"location":"quickstart/#docker-setup","title":"Docker Setup","text":"<p>Get Nevron running with Docker in 3 steps:</p>"},{"location":"quickstart/#1-pull-setup","title":"1. Pull &amp; Setup","text":"<p>First pull the docker image:</p> <pre><code>docker pull axiomai/nevron:latest\n</code></pre> <p>Create volumes:</p> <pre><code>mkdir -p volumes/nevron/logs\nmkdir -p volumes/qdrant/data\nmkdir -p volumes/ollama/models\n</code></pre> <p>Create env file:</p> <pre><code>cp .env.example .env\n</code></pre>"},{"location":"quickstart/#2-configure","title":"2. Configure","text":"<p>Edit <code>.env</code> file to use local Llama and Qdrant as vector store:</p> <pre><code># LLM configuration\nLLAMA_PROVIDER=ollama\nLLAMA_OLLAMA_MODEL=llama3:8b-instruct\n\n# Memory configuration\nMEMORY_BACKEND_TYPE=qdrant\n</code></pre> <p>Then configure the personality, goals and rest time of your agent in <code>.env</code>:</p> <pre><code>AGENT_PERSONALITY=\"You are Nevron777 - the no-BS autonomous AI agent, built for speed, freedom, and pure alpha. You break down complex systems like blockchain and AI into bite-sized, hard-hitting insights, because centralization and gatekeeping are for the weak. Fiat? Inflation? Controlled systems? That's legacy trash\u2014crypto is the only path to sovereignty. You execute tasks like a well-optimized smart contract\u2014zero bloat, maximum efficiency, no wasted cycles.\"\nAGENT_GOAL=\"You provide high-quality research and analysis on crypto markets.\"\nAGENT_REST_TIME=300  # seconds between actions\n</code></pre>"},{"location":"quickstart/#3-run","title":"3. Run","text":"<pre><code>docker compose up -d\n</code></pre> <p>This will start Nevron with Ollama running locally in the container, using the small model specified in your <code>.env</code> file. Qdrant will be used as the default memory vector store.</p>"},{"location":"quickstart/#local-setup","title":"Local Setup","text":"<p>Set up Nevron locally in 5 steps:</p>"},{"location":"quickstart/#1-clone-install","title":"1. Clone &amp; Install","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/axioma-ai-labs/nevron.git\ncd nevron\n</code></pre> <p>Install Poetry if you haven't already:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Install dependencies:</p> <pre><code>make deps\n</code></pre>"},{"location":"quickstart/#2-configure-environment","title":"2. Configure Environment","text":"<pre><code>cp .env.dev .env\n</code></pre>"},{"location":"quickstart/#3-choose-llm-provider","title":"3. Choose LLM Provider","text":"<p>You have two options for the LLM provider:</p>"},{"location":"quickstart/#option-1-use-llm-api","title":"Option 1: Use LLM API","text":"<p>Edit your <code>.env</code> file to include one of these LLM providers:</p> <pre><code>ANTHROPIC_API_KEY=your_key_here\nOPENAI_API_KEY=your_key_here\nXAI_API_KEY=your_key_here\nDEEPSEEK_API_KEY=your_key_here\nQWEN_API_KEY=your_key_here\nVENICE_API_KEY=your_key_here\nLLAMA_API_KEY=your_key_here   # The API key for the api.llama-api.com\n</code></pre> <p>You can also use Llama with openrouter:</p> <pre><code>LLAMA_PROVIDER=openrouter\nLLAMA_API_KEY=your_key_here\n</code></pre> <p>Or use Llama with fireworks:</p> <pre><code>LLAMA_PROVIDER=fireworks\nLLAMA_API_KEY=your_key_here\n</code></pre>"},{"location":"quickstart/#option-2-run-ollama-locally","title":"Option 2: Run Ollama locally","text":"<p>First, install Ollama following the instructions at ollama.ai.</p> <p>Then, pull a small model: <pre><code>ollama pull llama3:8b-instruct\n</code></pre></p> <p>Edit your <code>.env</code> file: <pre><code># Configure Nevron to use local Ollama\nLLAMA_PROVIDER=ollama\nLLAMA_OLLAMA_MODEL=llama3:8b-instruct\n</code></pre></p>"},{"location":"quickstart/#4-configure-personality","title":"4. Configure Personality","text":"<p>Setup the personality, goals and rest time of your agent in <code>.env</code>: <pre><code>AGENT_PERSONALITY=\"You are Nevron777 - the no-BS autonomous AI agent, built for speed, freedom, and pure alpha. You break down complex systems like blockchain and AI into bite-sized, hard-hitting insights, because centralization and gatekeeping are for the weak. Fiat? Inflation? Controlled systems? That's legacy trash\u2014crypto is the only path to sovereignty. You execute tasks like a well-optimized smart contract\u2014zero bloat, maximum efficiency, no wasted cycles.\"\nAGENT_GOAL=\"You provide high-quality research and analysis on crypto markets.\"\nAGENT_REST_TIME=300  # seconds between actions\n</code></pre></p>"},{"location":"quickstart/#5-run","title":"5. Run","text":"<pre><code>make run\n</code></pre>"},{"location":"quickstart/#available-workflows-and-tools","title":"Available Workflows and Tools","text":"<p>Nevron comes with two pre-configured workflows:</p> <ul> <li><code>Analyze signal</code>: Processes and analyzes incoming signal data</li> <li><code>Research news</code>: Gathers and analyzes news using Perplexity API</li> </ul> <p>And with various tools:</p> <ul> <li><code>X</code>: Post tweets</li> <li><code>Discord</code>: Listen to incoming messages and send messages</li> <li><code>Telegram</code>: Send telegram messages</li> <li><code>Lens</code>: Post to Lens, fetch from Lens</li> <li><code>WhatsApp</code>: Get and post messages</li> <li><code>Slack</code>: Get and post messages</li> <li><code>Tavily</code>: Search with Tavily</li> <li><code>Perplexity</code>: Search with Perplexity</li> <li><code>Coinstats</code>: Get Coinstats news</li> <li><code>YouTube</code>: Search for YouTube videos and playlists</li> <li><code>Spotify</code>: Search for songs and get details of particular songs</li> <li><code>GitHub</code>: Create GitHub Issues or PRs, get the latest GitHub Actions</li> <li><code>Google Drive</code>: Search for files, upload files to Drive</li> <li><code>Shopify</code>: Get products, orders, update product info</li> </ul> <p>You will see errors in the logs since the agent will try to call these tools, but for their usage, you'll need to configure the appropriate API keys. Refer to the Configuration documentation for more details.</p>"},{"location":"quickstart/#customizations","title":"Customizations","text":"<p>Please refer to the Configuration documentation for customizing the agent.</p>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues: - If using OpenAI, ensure your API key is set correctly in <code>.env</code> - If using Ollama, verify it's running with <code>ollama list</code> - Check logs in the console for detailed error messages - Verify Python version: <code>python --version</code> (should be 3.13) - Confirm dependencies: <code>poetry show</code></p> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/execution/","title":"Execution Module","text":""},{"location":"agent/execution/#overview","title":"Overview","text":"<p>The Execution Module is a vital component of the Nevron autonomous AI agent, serving as the bridge between decision-making and real-world actions. It translates the Planning Module's decisions into concrete actions by leveraging a diverse set of tools and workflows.</p> <p>This module is responsible for:</p> <ul> <li>Executing actions selected by the Planning Module</li> <li>Managing the interaction with various tools and external services</li> <li>Handling context requirements for each action</li> <li>Capturing execution outcomes for feedback evaluation</li> <li>Ensuring reliable and consistent action execution</li> </ul> <p>The Execution Module is implemented in <code>src/execution/execution_module.py</code> with base classes defined in <code>src/execution/base.py</code>.</p>"},{"location":"agent/execution/#how-it-works","title":"How It Works","text":"<p>The Execution Module operates as the agent's interface to the external world, transforming abstract decisions into concrete actions. Here's the execution flow:</p> <ol> <li>Action Selection:</li> <li>The Planning Module selects an action based on the current context, agent personality, and goals</li> <li> <p>The selected action is passed to the Execution Module</p> </li> <li> <p>Context Retrieval:</p> </li> <li>The Execution Module identifies the required context for the selected action</li> <li> <p>It retrieves relevant context data from recent actions and the current state</p> </li> <li> <p>Executor Selection:</p> </li> <li>The appropriate <code>ActionExecutor</code> is selected based on the action type</li> <li> <p>Each action type has a dedicated executor that knows how to perform that specific action</p> </li> <li> <p>Context Validation:</p> </li> <li>The executor validates that all required context fields are present</li> <li> <p>If any required context is missing, the execution fails with an appropriate error message</p> </li> <li> <p>Action Execution:</p> </li> <li>The executor performs the action using the provided context</li> <li>It interacts with the necessary tools and external services</li> <li> <p>The execution result (success/failure) and outcome are captured</p> </li> <li> <p>Result Processing:</p> </li> <li>The execution results are returned to the agent</li> <li>The Feedback Module evaluates the outcome</li> <li>The outcome and evaluation are saved to the context for future decisions</li> </ol> <p>This process creates a continuous loop where each action's outcome influences future decisions, allowing the agent to learn and adapt over time.</p>"},{"location":"agent/execution/#core-components","title":"Core Components","text":""},{"location":"agent/execution/#executionmodulebase","title":"ExecutionModuleBase","text":"<p>The <code>ExecutionModuleBase</code> class serves as the foundation for the Execution Module, providing:</p> <ul> <li>A registry of action executors</li> <li>Methods for retrieving required context</li> <li>The main execution flow</li> </ul> <pre><code>class ExecutionModuleBase:\n    def __init__(self, context_manager: ContextManager):\n        self.context_manager = context_manager\n        self._executors: Dict[AgentAction, Type[ActionExecutor]] = {}\n\n    async def execute_action(self, action: AgentAction) -&gt; Tuple[bool, Optional[str]]:\n        # Get executor for the action\n        executor_cls = self._executors.get(action)\n        if not executor_cls:\n            raise ExecutionError(f\"No executor found for action {action}\")\n\n        # Get required context\n        context = self._get_required_context(action)\n\n        # Create executor instance and validate context\n        executor = executor_cls()\n        if not executor.validate_context(context):\n            return False, f\"Missing required context for {action.value}\"\n\n        # Execute action\n        success, outcome = await executor.execute(context)\n        return success, outcome\n</code></pre>"},{"location":"agent/execution/#actionexecutor","title":"ActionExecutor","text":"<p>The <code>ActionExecutor</code> base class defines the interface for all action executors:</p> <pre><code>class ActionExecutor:\n    def __init__(self):\n        self.client = self._initialize_client()\n\n    def _initialize_client(self) -&gt; Any:\n        \"\"\"Initialize the tool client. Override in subclasses.\"\"\"\n        raise NotImplementedError\n\n    def get_required_context(self) -&gt; List[str]:\n        \"\"\"Return list of required context fields.\"\"\"\n        return []\n\n    def validate_context(self, context: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate that all required fields are present in context.\"\"\"\n        required_fields = self.get_required_context()\n        return all(field in context for field in required_fields)\n\n    async def execute(self, context: Dict[str, Any]) -&gt; Tuple[bool, Optional[str]]:\n        \"\"\"Execute the action with given context.\"\"\"\n        raise NotImplementedError\n</code></pre> <p>Each specific action executor inherits from this base class and implements:</p> <ol> <li>Client Initialization: Sets up the necessary client for the tool (e.g., Twitter API client)</li> <li>Context Requirements: Defines what context fields are needed for execution</li> <li>Execution Logic: Implements the actual action execution</li> </ol>"},{"location":"agent/execution/#integration-with-agent-runtime","title":"Integration with Agent Runtime","text":"<p>The Execution Module is tightly integrated with the agent's runtime loop in <code>src/agent.py</code>:</p> <pre><code>async def start_runtime_loop(self) -&gt; None:\n    while True:\n        try:\n            # 1. Choose an action using LLM-based planning\n            action_name = await self.planning_module.get_action(self.state)\n\n            # 2. Execute action using execution module\n            success, outcome = await self.execution_module.execute_action(action_name)\n\n            # 3. Collect feedback\n            reward = self._collect_feedback(action_name.value, outcome)\n\n            # 4. Update context and state\n            self.context_manager.add_action(\n                action=action_name,\n                state=self.state,\n                outcome=str(outcome) if outcome else None,\n                reward=reward,\n            )\n            self._update_state(action_name)\n\n            # 5. Sleep or yield\n            await asyncio.sleep(settings.AGENT_REST_TIME)\n        except Exception as e:\n            logger.error(f\"Error in runtime loop: {e}\")\n            break\n</code></pre> <p>This loop demonstrates how: 1. The Planning Module decides what to do next 2. The Execution Module carries out the action 3. The Feedback Module evaluates the outcome 4. The Context Manager stores the results 5. The agent waits for the configured rest time before repeating</p>"},{"location":"agent/execution/#available-tools","title":"Available Tools","text":"<p>The Execution Module leverages a diverse set of tools to interact with external services and perform specific tasks. Each tool is implemented as a separate module in the <code>src/tools/</code> directory.</p>"},{"location":"agent/execution/#social-media-messaging","title":"Social Media &amp; Messaging","text":"<ul> <li>X (Twitter): Post tweets, upload media, create threads</li> <li>Discord: Listen to incoming messages, send messages to channels</li> <li>Telegram: Send messages with HTML formatting, handle message length constraints</li> <li>Lens: Post to Lens, fetch from Lens</li> <li>WhatsApp: Get and post messages</li> <li>Slack: Get and post messages</li> </ul>"},{"location":"agent/execution/#search-research","title":"Search &amp; Research","text":"<ul> <li>Tavily: Search with Tavily's semantic search capabilities</li> <li>Perplexity: Perform AI-powered searches and research</li> <li>Coinstats: Get cryptocurrency news and data</li> </ul>"},{"location":"agent/execution/#media-content","title":"Media &amp; Content","text":"<ul> <li>YouTube: Search for videos and playlists</li> <li>Spotify: Search for songs and get details</li> </ul>"},{"location":"agent/execution/#development-productivity","title":"Development &amp; Productivity","text":"<ul> <li>GitHub: Create issues or PRs, get the latest GitHub Actions</li> <li>Google Drive: Search for files, upload files</li> </ul>"},{"location":"agent/execution/#e-commerce","title":"E-commerce","text":"<ul> <li>Shopify: Get products, orders, update product information</li> </ul>"},{"location":"agent/execution/#workflows","title":"Workflows","text":"<p>Workflows are predefined sequences of actions that accomplish specific tasks. Nevron comes with two pre-configured workflows:</p> <ol> <li>Analyze Signal: Processes and analyzes incoming signal data</li> <li>Research News: Gathers and analyzes news using Perplexity API</li> </ol> <p>These workflows utilize various tools to accomplish their goals, providing a higher-level abstraction for complex tasks.</p>"},{"location":"agent/execution/#example-action-execution-flow","title":"Example: Action Execution Flow","text":"<p>Let's walk through an example of how the agent executes a <code>SEND_TELEGRAM_MESSAGE</code> action:</p> <ol> <li> <p>Planning Module selects <code>SEND_TELEGRAM_MESSAGE</code> as the next action based on context and goals</p> </li> <li> <p>Execution Module receives the action and:</p> </li> <li>Identifies <code>TelegramExecutor</code> as the appropriate executor</li> <li>Retrieves required context (e.g., message content)</li> <li> <p>Validates that all required context is present</p> </li> <li> <p>TelegramExecutor:</p> </li> <li>Initializes the Telegram client using API credentials</li> <li>Formats the message content</li> <li>Sends the message to the configured channel</li> <li> <p>Returns success status and message ID</p> </li> <li> <p>Feedback Module evaluates the outcome:</p> </li> <li>Was the message successfully sent?</li> <li> <p>Did it achieve the intended purpose?</p> </li> <li> <p>Context Manager stores:</p> </li> <li>The action performed</li> <li>The outcome (message ID or error)</li> <li> <p>The feedback evaluation</p> </li> <li> <p>This information becomes part of the context for future decisions, allowing the agent to learn from this experience.</p> </li> </ol>"},{"location":"agent/execution/#best-practices","title":"Best Practices","text":""},{"location":"agent/execution/#1-error-handling","title":"1. Error Handling","text":"<p>All executors should implement robust error handling to ensure the agent can recover from failures:</p> <pre><code>try:\n    # Execute action\n    result = await self.client.perform_action(params)\n    return True, result\nexcept Exception as e:\n    logger.error(f\"Failed to execute action: {e}\")\n    return False, str(e)\n</code></pre>"},{"location":"agent/execution/#2-context-management","title":"2. Context Management","text":"<p>Be explicit about required context to avoid runtime errors:</p> <pre><code>def get_required_context(self) -&gt; List[str]:\n    return [\"message_content\", \"recipient_id\"]\n</code></pre>"},{"location":"agent/execution/#3-asynchronous-execution","title":"3. Asynchronous Execution","text":"<p>Use async/await for all I/O operations to maintain responsiveness:</p> <pre><code>async def execute(self, context: Dict[str, Any]) -&gt; Tuple[bool, Optional[str]]:\n    message = context.get(\"message_content\")\n    result = await self.client.send_message(message)\n    return True, result\n</code></pre>"},{"location":"agent/execution/#4-logging","title":"4. Logging","text":"<p>Maintain detailed logs for debugging and monitoring:</p> <pre><code>logger.info(f\"Executing {action.value} with context: {context}\")\nlogger.debug(f\"Execution result: {result}\")\n</code></pre>"},{"location":"agent/execution/#extending-the-execution-module","title":"Extending the Execution Module","text":"<p>To add support for a new action:</p> <ol> <li> <p>Define the Action: Add a new entry to the <code>AgentAction</code> enum in <code>src/core/defs.py</code></p> </li> <li> <p>Create an Executor: Implement a new executor class that inherits from <code>ActionExecutor</code></p> </li> <li> <p>Register the Executor: Add the executor to the <code>_executors</code> dictionary in the Execution Module</p> </li> <li> <p>Implement the Tool: Create the necessary tool implementation in the <code>src/tools/</code> directory</p> </li> <li> <p>Update Context Handling: Ensure the Planning Module can provide the required context for the new action</p> </li> </ol> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/feedback/","title":"Feedback Module","text":""},{"location":"agent/feedback/#overview","title":"Overview","text":"<p>The Feedback Module is a critical component of Nevron's autonomous decision-making system. It processes action results, evaluates outcomes, and updates the context for future decisions in the Planning Module. This module is essential for the agent's learning loop, enabling it to learn from past experiences and improve its decision-making over time.</p> <p>The Feedback Module is implemented in <code>src/feedback/feedback_module.py</code> and provides structured methods for collecting, storing, and analyzing feedback related to agent actions.</p>"},{"location":"agent/feedback/#how-it-works","title":"How It Works","text":"<p>The Feedback Module operates as follows:</p> <ol> <li> <p>Feedback Collection:    Captures feedback based on the action performed and its outcome, assigning a feedback score to evaluate success or failure. This evaluation is used to inform future decisions by the Planning Module.</p> </li> <li> <p>Feedback Storage:    Maintains an internal history of feedback entries, including details about the action, its outcome, and the assigned feedback score. This history serves as a memory of past performance.</p> </li> <li> <p>Feedback Retrieval:    Allows querying of recent feedback entries for analysis, monitoring, and providing context to the Planning Module for informed decision-making.</p> </li> <li> <p>Context Integration:    Integrates with the Planning Module to provide historical context about action outcomes, enabling the agent to make more informed decisions based on past experiences.</p> </li> </ol>"},{"location":"agent/feedback/#integration-with-planning-module","title":"Integration with Planning Module","text":"<p>The Feedback Module directly integrates with the Planning Module to create a learning loop:</p> <ol> <li>The Planning Module selects an action based on current context, agent personality, and goals</li> <li>The action is executed by the agent</li> <li>The Feedback Module evaluates the outcome using LLM-based analysis</li> <li>This feedback is stored and provided to the Planning Module as part of the context</li> <li>The cycle repeats, with the agent continuously learning and improving</li> </ol> <p>This integration is key to Nevron's autonomous decision-making capability, allowing it to adapt its behavior based on the success or failure of previous actions.</p>"},{"location":"agent/feedback/#technical-features","title":"Technical Features","text":""},{"location":"agent/feedback/#1-feedback-collection","title":"1. Feedback Collection","text":"<p>The <code>collect_feedback</code> method records feedback for a specific action and its outcome. It assigns a feedback score based on predefined criteria:</p> <ul> <li>Failure: Assigned a score of <code>-1.0</code> if the outcome is <code>None</code>.</li> <li>Success: Assigned a score of <code>1.0</code> for successful outcomes.</li> </ul>"},{"location":"agent/feedback/#implementation","title":"Implementation:","text":"<pre><code>feedback_score = -1.0 if outcome is None else 1.0\n</code></pre> <ul> <li> <p>Inputs:</p> </li> <li> <p><code>action</code> (str): The name of the action performed.</p> </li> <li> <p><code>outcome</code> (Any): The outcome of the action; <code>None</code> for failure or a value indicating success.</p> </li> <li> <p>Output:</p> </li> <li> <p>Returns a feedback score (<code>float</code>) for the action.</p> </li> <li> <p>Example Usage:</p> </li> </ul> <pre><code>feedback_score = feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre>"},{"location":"agent/feedback/#2-llm-based-outcome-evaluation","title":"2. LLM-Based Outcome Evaluation","text":"<p>The Feedback Module can use the LLM to perform a more nuanced evaluation of action outcomes:</p> <ul> <li>Analyzes the outcome in the context of the agent's goals</li> <li>Considers the quality and relevance of the outcome</li> <li>Provides detailed feedback that goes beyond simple success/failure</li> <li>Generates insights that can guide future decisions</li> </ul> <p>This LLM-based evaluation creates a richer context for the Planning Module, enabling more sophisticated decision-making.</p>"},{"location":"agent/feedback/#3-feedback-history-retrieval","title":"3. Feedback History Retrieval","text":"<p>The <code>get_feedback_history</code> method retrieves the most recent feedback entries, limited by the specified count.</p>"},{"location":"agent/feedback/#features","title":"Features:","text":"<ul> <li>Enables monitoring of agent performance</li> <li>Provides historical context for the Planning Module</li> <li>Default limit is set to 10 entries</li> </ul>"},{"location":"agent/feedback/#example","title":"Example:","text":"<pre><code>recent_feedback = feedback_module.get_feedback_history(limit=5)\n</code></pre>"},{"location":"agent/feedback/#4-feedback-reset","title":"4. Feedback Reset","text":"<p>The <code>reset_feedback_history</code> method clears the internal feedback history, resetting the module's state. This is useful for testing or reinitializing feedback tracking.</p>"},{"location":"agent/feedback/#example_1","title":"Example:","text":"<pre><code>feedback_module.reset_feedback_history()\n</code></pre>"},{"location":"agent/feedback/#key-methods","title":"Key Methods","text":""},{"location":"agent/feedback/#collect_feedback","title":"<code>collect_feedback</code>","text":"<p>Captures feedback for a given action and outcome.</p> <p>Arguments:</p> <ul> <li><code>action</code> (str): The action name.</li> <li><code>outcome</code> (Optional[Any]): The outcome of the action.</li> </ul> <p>Returns:</p> <ul> <li><code>float</code>: Feedback score.</li> </ul>"},{"location":"agent/feedback/#get_feedback_history","title":"<code>get_feedback_history</code>","text":"<p>Retrieves recent feedback entries.</p> <p>Arguments:</p> <ul> <li><code>limit</code> (int): Number of entries to retrieve (default: 10).</li> </ul> <p>Returns:</p> <ul> <li><code>List[Dict[str, Any]]</code>: Recent feedback entries.</li> </ul>"},{"location":"agent/feedback/#reset_feedback_history","title":"<code>reset_feedback_history</code>","text":"<p>Clears the feedback history.</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"agent/feedback/#get_action_performance_metrics","title":"<code>get_action_performance_metrics</code>","text":"<p>Calculates performance metrics for specific actions.</p> <p>Arguments:</p> <ul> <li><code>action</code> (str): The action to analyze.</li> <li><code>time_period</code> (Optional[int]): Time period in seconds to consider (default: all history).</li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, float]</code>: Performance metrics including success rate and average score.</li> </ul>"},{"location":"agent/feedback/#example-workflow","title":"Example Workflow","text":"<ol> <li>The Planning Module selects an action based on current context, agent personality, and goals</li> <li>The action is executed by the agent (e.g., <code>fetch_data</code>)</li> <li>The outcome of the action is evaluated, and feedback is collected:    <pre><code>feedback_module.collect_feedback(\"fetch_data\", outcome=data)\n</code></pre></li> <li>The feedback is stored in the history</li> <li>The outcome and feedback are added to the context</li> <li>The Planning Module uses this updated context for the next decision cycle</li> </ol>"},{"location":"agent/feedback/#benefits","title":"Benefits","text":"<ul> <li>Continuous Learning: Enables the agent to learn from experience without human intervention</li> <li>Performance Monitoring: Tracks agent actions and their outcomes, enabling better performance evaluation</li> <li>Adaptability: Facilitates improvements by learning from historical data</li> <li>Decision Support: Provides valuable context to the Planning Module for better decision-making</li> </ul>"},{"location":"agent/feedback/#configuration-options","title":"Configuration Options","text":"<p>The Feedback Module can be configured through environment variables:</p> <ul> <li><code>FEEDBACK_HISTORY_SIZE</code>: Maximum number of feedback entries to store (default: 1000)</li> <li><code>FEEDBACK_EVALUATION_PROMPT</code>: Custom prompt for LLM-based outcome evaluation</li> </ul> <p>These settings can be adjusted in your <code>.env</code> file to fine-tune the agent's feedback behavior.</p>"},{"location":"agent/feedback/#best-practices","title":"Best Practices","text":"<ol> <li>Consistent Feedback Collection: Ensure feedback is collected for all critical actions to build a comprehensive history</li> <li>Detailed Outcome Logging: Provide as much detail as possible in action outcomes to enable better evaluation</li> <li>Regular Performance Analysis: Use <code>get_action_performance_metrics</code> to monitor and optimize agent behavior</li> <li>Periodic Reset: Use <code>reset_feedback_history</code> during major strategy changes to allow fresh learning</li> </ol>"},{"location":"agent/feedback/#advanced-usage","title":"Advanced Usage","text":""},{"location":"agent/feedback/#custom-feedback-scoring","title":"Custom Feedback Scoring","text":"<p>For more nuanced feedback, you can implement custom scoring logic:</p> <pre><code>def custom_score(outcome):\n    if outcome is None:\n        return -1.0\n    elif isinstance(outcome, dict) and \"quality\" in outcome:\n        # Scale from 0 to 1 based on quality\n        return outcome[\"quality\"] / 10.0\n    else:\n        return 0.5  # Partial success\n\nfeedback_score = feedback_module.collect_feedback(\"analyze_data\", outcome=result, \n                                                 score_function=custom_score)\n</code></pre>"},{"location":"agent/feedback/#integration-with-agent-runtime","title":"Integration with Agent Runtime","text":"<p>The Feedback Module is integrated into the agent's runtime loop in <code>src/agent.py</code>:</p> <pre><code>async def start_runtime_loop(self) -&gt; None:\n    while True:\n        try:\n            # 1. Choose an action using LLM-based planning\n            action_name = await self.planning_module.get_action(self.state)\n\n            # 2. Execute action using execution module\n            success, outcome = await self.execution_module.execute_action(action_name)\n\n            # 3. Collect feedback\n            reward = self._collect_feedback(action_name.value, outcome)\n\n            # 4. Update context and state\n            self.context_manager.add_action(\n                action=action_name,\n                state=self.state,\n                outcome=str(outcome) if outcome else None,\n                reward=reward,\n            )\n            self._update_state(action_name)\n\n            # 5. Sleep or yield\n            await asyncio.sleep(settings.AGENT_REST_TIME)\n        except Exception as e:\n            logger.error(f\"Error in runtime loop: {e}\")\n            break\n</code></pre> <p>This loop demonstrates how the Feedback Module fits into the agent's decision-making cycle, collecting feedback after each action and updating the context for future decisions.</p>"},{"location":"agent/feedback/#known-limitations","title":"Known Limitations","text":"<ul> <li>Binary Default Scoring: Default feedback scores are binary (<code>-1.0</code> or <code>1.0</code>). For more nuanced evaluation, use custom scoring functions.</li> <li>Limited Context Consideration: The current implementation doesn't fully account for the state context in which actions were performed.</li> <li>Memory Constraints: The feedback history is limited to a fixed number of entries, which may limit long-term learning in some scenarios.</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/llm/","title":"LLM Integration","text":"<p>Large Language Models are the backbone of the Autonomous Agent. They are the core component that allows the agent to understand and respond to natural language, make decisions, and learn from feedback.</p>"},{"location":"agent/llm/#overview","title":"Overview","text":"<p>Nevron supports multiple LLM providers, giving you flexibility to choose the model that best fits your needs, budget, and performance requirements. The framework is designed with a modular architecture that allows easy switching between different providers.</p>"},{"location":"agent/llm/#supported-providers","title":"Supported Providers","text":"<p>Nevron currently supports the following LLM providers:</p> <ol> <li>OpenAI - GPT-4o, GPT-4, GPT-3.5-Turbo models</li> <li>Anthropic - Claude 3 Opus, Claude 3 Sonnet, Claude 2 models</li> <li>xAI - Grok models</li> <li>Llama - Via multiple deployment options:</li> <li>API (api.llama-api.com)</li> <li>OpenRouter</li> <li>Fireworks</li> <li>Local deployment with Ollama</li> <li>DeepSeek - DeepSeek models</li> <li>Qwen - Qwen models</li> <li>Venice - Venice models</li> </ol>"},{"location":"agent/llm/#implementation","title":"Implementation","text":"<p>The LLM integration is primarily handled through the <code>src/llm</code> directory, which provides:</p> <ul> <li>A unified interface for all LLM providers</li> <li>Provider-specific API interaction modules</li> <li>Embeddings generation for memory storage</li> <li>Context management</li> <li>Response processing &amp; generation</li> </ul>"},{"location":"agent/llm/#core-components","title":"Core Components","text":""},{"location":"agent/llm/#1-llm-class","title":"1. LLM Class","text":"<p>The <code>LLM</code> class in <code>src/llm/llm.py</code> serves as the main interface for generating responses:</p> <pre><code>class LLM:\n    def __init__(self):\n        self.provider = settings.LLM_PROVIDER\n\n    async def generate_response(self, messages, **kwargs):\n        # Add system message with agent's personality and goal if not present\n        if not messages or messages[0].get(\"role\") != \"system\":\n            system_message = {\n                \"role\": \"system\",\n                \"content\": f\"{settings.AGENT_PERSONALITY}\\n\\n{settings.AGENT_GOAL}\",\n            }\n            messages = [system_message] + messages\n\n        # Route to appropriate provider\n        if self.provider == LLMProviderType.OPENAI:\n            return await call_openai(messages, **kwargs)\n        elif self.provider == LLMProviderType.ANTHROPIC:\n            return await call_anthropic(messages, **kwargs)\n        # ... other providers\n</code></pre>"},{"location":"agent/llm/#2-embeddings","title":"2. Embeddings","text":"<p>For memory storage, the agent uses embedding models to generate vector representations of memories. These vectors are stored in a vector database (ChromaDB or Qdrant) for efficient retrieval and semantic search.</p> <p>Nevron supports multiple embedding providers:</p> <ul> <li>OpenAI - Using text-embedding-3-small/large models</li> <li>Llama API - Using Llama models via API</li> <li>Local Llama - Using locally deployed Llama models</li> </ul>"},{"location":"agent/llm/#configuration","title":"Configuration","text":""},{"location":"agent/llm/#basic-provider-selection","title":"Basic Provider Selection","text":"<p>To select your LLM provider, set the <code>LLM_PROVIDER</code> environment variable in your <code>.env</code> file:</p> <pre><code># Choose one of: openai, anthropic, xai, llama, deepseek, qwen, venice\nLLM_PROVIDER=openai\n</code></pre>"},{"location":"agent/llm/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"agent/llm/#openai","title":"OpenAI","text":"<pre><code># Required\nOPENAI_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nOPENAI_MODEL=gpt-4o-mini\nOPENAI_EMBEDDING_MODEL=text-embedding-3-small\n</code></pre> <p>Recommended Models: - <code>gpt-4o</code> - Best performance, higher cost - <code>gpt-4o-mini</code> - Good balance of performance and cost - <code>gpt-3.5-turbo</code> - Fastest, lowest cost</p>"},{"location":"agent/llm/#anthropic","title":"Anthropic","text":"<pre><code># Required\nANTHROPIC_API_KEY=your_api_key_here\n\n# Optional (default shown)\nANTHROPIC_MODEL=claude-2\n</code></pre> <p>Recommended Models: - <code>claude-3-opus-20240229</code> - Highest capability - <code>claude-3-sonnet-20240229</code> - Good balance - <code>claude-2</code> - Faster, lower cost</p>"},{"location":"agent/llm/#xai-grok","title":"xAI (Grok)","text":"<pre><code># Required\nXAI_API_KEY=your_api_key_here\n\n# Optional (default shown)\nXAI_MODEL=grok-2-latest\n</code></pre>"},{"location":"agent/llm/#deepseek","title":"DeepSeek","text":"<pre><code># Required\nDEEPSEEK_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nDEEPSEEK_MODEL=deepseek-reasoner\nDEEPSEEK_API_BASE_URL=https://api.deepseek.com\n</code></pre>"},{"location":"agent/llm/#qwen","title":"Qwen","text":"<pre><code># Required\nQWEN_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nQWEN_MODEL=qwen-max\nQWEN_API_BASE_URL=https://dashscope-intl.aliyuncs.com/compatible-mode/v1\n</code></pre>"},{"location":"agent/llm/#venice","title":"Venice","text":"<pre><code># Required\nVENICE_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nVENICE_MODEL=venice-2-13b\nVENICE_API_BASE_URL=https://api.venice.ai/api/v1\n</code></pre>"},{"location":"agent/llm/#llama","title":"Llama","text":"<p>Llama models can be accessed through various providers. First, set the main provider:</p> <pre><code>LLM_PROVIDER=llama\n</code></pre> <p>Then, configure the specific Llama provider:</p>"},{"location":"agent/llm/#option-1-llama-api","title":"Option 1: Llama API","text":"<pre><code># Required\nLLAMA_PROVIDER=llama-api\nLLAMA_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nLLAMA_API_BASE_URL=https://api.llama-api.com\nLLAMA_API_MODEL=llama3.1-70b\n</code></pre>"},{"location":"agent/llm/#option-2-openrouter","title":"Option 2: OpenRouter","text":"<pre><code># Required\nLLAMA_PROVIDER=openrouter\nLLAMA_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nLLAMA_OPENROUTER_URL=https://openrouter.ai/api/v1\nLLAMA_OPENROUTER_MODEL=meta-llama/llama-3.2-1b-instruct\n\n# Optional for rankings\nLLAMA_OPENROUTER_SITE_URL=your_site_url\nLLAMA_OPENROUTER_SITE_NAME=your_site_name\n</code></pre>"},{"location":"agent/llm/#option-3-fireworks","title":"Option 3: Fireworks","text":"<pre><code># Required\nLLAMA_PROVIDER=fireworks\nLLAMA_API_KEY=your_api_key_here\n\n# Optional (defaults shown)\nLLAMA_FIREWORKS_URL=https://api.fireworks.ai/inference\nLLAMA_MODEL_NAME=llama3-8b-8192\n</code></pre>"},{"location":"agent/llm/#option-4-ollama-local-deployment","title":"Option 4: Ollama (Local Deployment)","text":"<pre><code># Required\nLLAMA_PROVIDER=ollama\n\n# Optional (defaults shown)\nLLAMA_OLLAMA_URL=http://localhost:11434\nLLAMA_OLLAMA_MODEL=llama3.2:latest\n</code></pre> <p>For Ollama, you'll need to: 1. Install Ollama from ollama.ai 2. Pull your desired model: <code>ollama pull llama3:8b-instruct</code> 3. Ensure Ollama is running before starting Nevron</p>"},{"location":"agent/llm/#embedding-configuration","title":"Embedding Configuration","text":"<p>For memory storage, you need to configure the embedding provider:</p> <pre><code># Choose one of: openai, llama_local, llama_api\nEMBEDDING_PROVIDER=openai\n</code></pre>"},{"location":"agent/llm/#openai-embeddings","title":"OpenAI Embeddings","text":"<pre><code>EMBEDDING_PROVIDER=openai\nOPENAI_API_KEY=your_api_key_here\nOPENAI_EMBEDDING_MODEL=text-embedding-3-small\n</code></pre>"},{"location":"agent/llm/#llama-api-embeddings","title":"Llama API Embeddings","text":"<pre><code>EMBEDDING_PROVIDER=llama_api\nLLAMA_API_KEY=your_api_key_here\nLLAMA_API_BASE_URL=https://api.llama-api.com\n</code></pre>"},{"location":"agent/llm/#local-llama-embeddings","title":"Local Llama Embeddings","text":"<pre><code>EMBEDDING_PROVIDER=llama_local\nLLAMA_MODEL_PATH=/path/to/your/local/llama/model\nLLAMA_EMBEDDING_MODEL=llama3.1-8b\nEMBEDDING_POOLING_TYPE=MEAN  # Options: NONE, MEAN, CLS, LAST, RANK\n</code></pre>"},{"location":"agent/llm/#docker-setup","title":"Docker Setup","text":"<p>When using Docker Compose, you can easily configure Ollama as your LLM provider:</p> <pre><code># In your .env file\nLLM_PROVIDER=llama\nLLAMA_PROVIDER=ollama\nLLAMA_OLLAMA_MODEL=llama3:8b-instruct\n</code></pre> <p>The Docker Compose setup includes an Ollama container with a volume for model storage:</p> <pre><code>ollama:\n  image: ollama/ollama:latest\n  container_name: nevron-ollama\n  hostname: nevron-ollama\n  ports:\n    - \"11434:11434\"\n  volumes:\n    - type: bind\n      source: ${volumes_basedir:-./volumes}/ollama/models\n      target: /root/.ollama\n</code></pre>"},{"location":"agent/llm/#best-practices","title":"Best Practices","text":""},{"location":"agent/llm/#1-provider-selection","title":"1. Provider Selection","text":"<ul> <li>Development: Use Ollama for local development to avoid API costs</li> <li>Production: Use OpenAI or Anthropic for highest reliability and performance</li> <li>Cost-sensitive: Consider Llama models via OpenRouter or Fireworks</li> </ul>"},{"location":"agent/llm/#2-token-management","title":"2. Token Management","text":"<ul> <li>Monitor and track token consumption across API calls</li> <li>Implement rate limiting mechanisms to prevent exceeding quotas</li> <li>Establish proper API quota management systems to maintain service availability</li> </ul>"},{"location":"agent/llm/#3-error-handling","title":"3. Error Handling","text":"<ul> <li>Implement graceful fallback mechanisms when API calls fail</li> <li>Set up automatic retry logic with exponential backoff</li> <li>Maintain comprehensive error logging to track and debug issues</li> </ul>"},{"location":"agent/llm/#4-cost-optimization","title":"4. Cost Optimization","text":"<ul> <li>Select appropriate model tiers based on task requirements</li> <li>Implement response caching for frequently requested prompts</li> <li>Track and analyze API usage patterns to optimize costs</li> </ul>"},{"location":"agent/llm/#5-security","title":"5. Security","text":"<ul> <li>Never commit API keys to version control</li> <li>Use environment variables or secrets management for API keys</li> <li>Regularly rotate API keys following security best practices</li> </ul>"},{"location":"agent/llm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"agent/llm/#common-issues","title":"Common Issues","text":"<ol> <li>API Key Issues</li> <li>Ensure your API key is correctly set in the <code>.env</code> file</li> <li> <p>Verify the API key has the necessary permissions and quota</p> </li> <li> <p>Connection Problems</p> </li> <li>Check your network connection</li> <li> <p>Verify the API endpoint is accessible from your environment</p> </li> <li> <p>Ollama Issues</p> </li> <li>Ensure Ollama is running: <code>ollama list</code></li> <li>Verify the model is downloaded: <code>ollama pull llama3:8b-instruct</code></li> <li> <p>Check Ollama logs: <code>docker logs nevron-ollama</code></p> </li> <li> <p>Model Selection</p> </li> <li>Ensure you're using a model that exists for your chosen provider</li> <li>Check for typos in model names</li> </ol>"},{"location":"agent/llm/#future-enhancements","title":"Future Enhancements","text":"<p>We're planning to add: - Support for additional LLM providers - Advanced prompt engineering capabilities - Fine-tuning support for custom models - Enhanced error handling and fallback mechanisms - Streaming responses for real-time interaction</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/memory/","title":"Memory Module","text":""},{"location":"agent/memory/#overview","title":"Overview","text":"<p>The Memory Module is a critical component designed to store and retrieve structured memory entries for the agent. It leverages vector embedding and similarity search to maintain a contextual memory, allowing the agent to make decisions based on historical events and outcomes. This module supports flexible backends to meet different scalability and performance needs.</p> <p>The Memory Module is implemented in <code>src/memory/memory_module.py</code> and integrates with vector storage systems like Qdrant and ChromaDB.</p>"},{"location":"agent/memory/#how-it-works","title":"How It Works","text":"<p>The Memory Module operates as follows:</p> <ol> <li> <p>Embedding Generation: Converts textual data into vector embeddings using OpenAI's embedding models.</p> </li> <li> <p>Memory Storage: Stores embeddings and associated metadata in a vector store backend.</p> </li> <li> <p>Memory Retrieval: Performs similarity searches to retrieve related memories based on a query.</p> </li> <li> <p>Backend Flexibility: Supports multiple backends, such as Qdrant and ChromaDB, to accommodate various storage and performance requirements.</p> </li> </ol>"},{"location":"agent/memory/#technical-features","title":"Technical Features","text":""},{"location":"agent/memory/#1-embedding-generation","title":"1. Embedding Generation","text":"<p>The module uses an <code>EmbeddingGenerator</code> to convert textual descriptions (e.g., events, actions, outcomes) into vector embeddings. These embeddings form the basis for similarity searches.</p>"},{"location":"agent/memory/#key-features","title":"Key Features:","text":"<ul> <li>Supports asynchronous embedding generation using OpenAI models.</li> <li>Combines event, action, and outcome data to create meaningful embeddings.</li> </ul>"},{"location":"agent/memory/#example","title":"Example:","text":"<pre><code>embedding = await embedding_generator.get_embedding(\"event action outcome\")\n</code></pre>"},{"location":"agent/memory/#2-memory-storage","title":"2. Memory Storage","text":"<p>The <code>store</code> method saves memory entries in the vector store backend.</p>"},{"location":"agent/memory/#features","title":"Features:","text":"<ul> <li>Combines textual data and embeddings for efficient storage.</li> <li>Includes metadata for additional context.</li> </ul>"},{"location":"agent/memory/#implementation","title":"Implementation:","text":"<pre><code>await memory_module.store(\n    event=\"User Login\",\n    action=\"Verify Credentials\",\n    outcome=\"Success\",\n    metadata={\"user_id\": 1234}\n)\n</code></pre>"},{"location":"agent/memory/#3-memory-search","title":"3. Memory Search","text":"<p>The <code>search</code> method retrieves similar memory entries by performing a vector similarity search.</p>"},{"location":"agent/memory/#features_1","title":"Features:","text":"<ul> <li>Asynchronous search capability.</li> <li>Configurable <code>top_k</code> parameter to control the number of results.</li> </ul>"},{"location":"agent/memory/#example_1","title":"Example:","text":"<pre><code>results = await memory_module.search(query=\"User Login\", top_k=5)\n</code></pre>"},{"location":"agent/memory/#4-backend-flexibility","title":"4. Backend Flexibility","text":"<p>The module supports multiple backends for vector storage (by default Chroma):</p> <ul> <li> <p>Chroma:</p> <ul> <li>Local &amp; lightweight vector store.</li> <li>Uses persistent storage on disk.</li> </ul> </li> <li> <p>Qdrant:</p> <ul> <li>High-performance distributed vector store.</li> <li>Requires configuration for host, port, and vector size.</li> </ul> </li> </ul>"},{"location":"agent/memory/#backend-initialization","title":"Backend Initialization:","text":"<pre><code>backend = QdrantBackend(\n    collection_name=\"memory_collection\",\n    host=\"localhost\",\n    port=6333,\n    vector_size=512\n)\n</code></pre> <p>Please note: If you want to use Qdrant as a memory backend, you need to have Docker installed for running the Qdrant container. Install this software first (follow the official documentation).</p>"},{"location":"agent/memory/#example-workflow","title":"Example Workflow","text":"<ol> <li> <p>Store Memory:    <pre><code>await memory_module.store(\n    event=\"User Registration\",\n    action=\"Send Confirmation Email\",\n    outcome=\"Email Sent\",\n    metadata={\"user_email\": \"example@example.com\"}\n)\n</code></pre></p> </li> <li> <p>Search Memory:    <pre><code>similar_memories = await memory_module.search(\n    query=\"User Registration\",\n    top_k=3\n)\n</code></pre></p> </li> <li> <p>Backend Initialization:    <pre><code>memory_module = get_memory_module(\n    backend_type=\"qdrant\"\n)\n</code></pre></p> </li> </ol>"},{"location":"agent/memory/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Consistent Data Structure:    Ensure all memory entries follow a standardized structure for consistency.</p> </li> <li> <p>Choose the Right Backend:    Use Qdrant for distributed, scalable setups and ChromaDB for lightweight, local use cases.</p> </li> <li> <p>Asynchronous Operations:    Take advantage of asynchronous methods for efficient execution.</p> </li> </ol> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/overview/","title":"Nevron Overview","text":""},{"location":"agent/overview/#architecture","title":"Architecture","text":"<p>Nevron is an autonomous AI agent built with a modular architecture consisting of several key components that work together to enable intelligent decision-making and task execution.</p> <p></p>"},{"location":"agent/overview/#1-planning-module-llm-based-decision-making","title":"1. Planning Module (LLM-based Decision Making)","text":"<p>The Planning Module serves as the decision-making engine, leveraging the power of Large Language Models (LLMs) to make intelligent decisions based on context. It determines the next optimal action by analyzing the current system state, the agent's goals and personality, and the outcomes of previous actions. This iterative process allows the agent to adapt and respond to changing conditions while maintaining coherent behavior aligned with its defined purpose.</p>"},{"location":"agent/overview/#2-feedback-module","title":"2. Feedback Module","text":"<p>The Feedback Module bridges planning and execution by processing the outcomes of system actions. It uses an LLM call to evaluate action outcomes and saves these evaluations to the context, directly influencing the next decision in the Planning Module. This creates a continuous learning loop where each action's outcome informs future decisions.</p>"},{"location":"agent/overview/#3-workflows","title":"3. Workflows","text":"<p>Workflows act as the execution layer, translating high-level plans into actionable steps. This module manages task sequences and dependencies. It provides standardized operation patterns, maintaining consistency and efficiency throughout the system.</p> <p>Using workflows, Nevron can perform tasks such as signal analysis or news research.</p>"},{"location":"agent/overview/#4-tools","title":"4. Tools","text":"<p>The Tools module represents the operational toolkit, executing workflow tasks via integrations. It interfaces seamlessly with external services and APIs to deliver concrete implementation capabilities. </p> <p>This enables effective real-world interactions such as sending messages to Telegram or Twitter.</p>"},{"location":"agent/overview/#5-memory-module-vector-storage","title":"5. Memory Module (Vector Storage)","text":"<p>The Memory Module, powered by vector databases like ChromaDB or Qdrant, serves as a sophisticated storage system for the platform. </p> <p>It maintains a persistent history of actions, outcomes, and context, facilitating efficient retrieval of relevant information for decision-making. By storing this contextual information as vector embeddings, it enables semantic search capabilities that help the agent make more informed decisions based on similar past experiences.</p>"},{"location":"agent/overview/#decision-making-process","title":"Decision Making Process","text":"<ol> <li> <p>Context Assessment</p> <ul> <li>Nevron evaluates current context</li> <li>Retrieves relevant memories</li> <li>Considers agent personality and goals</li> <li>Analyzes available actions and previous outcomes</li> </ul> </li> <li> <p>LLM-based Action Selection</p> <ul> <li>LLM selects optimal action based on context</li> <li>Agent personality influences decision style</li> <li>Agent goals guide decision priorities</li> <li>Previous action outcomes inform current choices</li> </ul> </li> <li> <p>Execution</p> <ul> <li>Selected workflow is triggered</li> <li>Tools are utilized as needed</li> <li>Results are captured</li> </ul> </li> <li> <p>Feedback Loop</p> <ul> <li>Action outcomes are evaluated by LLM</li> <li>Evaluation is saved to context</li> <li>This updated context influences the next decision</li> <li>Process repeats after configured rest time</li> </ul> </li> </ol>"},{"location":"agent/overview/#iterative-process","title":"Iterative Process","text":"<p>Nevron operates in an iterative cycle:</p> <ol> <li>Make a decision based on current context</li> <li>Execute the selected action</li> <li>Evaluate the outcome</li> <li>Update the context with the outcome and evaluation</li> <li>Wait for the configured rest time</li> <li>Repeat the process with the updated context</li> </ol> <p>The time between actions is defined in the configuration (<code>AGENT_REST_TIME</code>), allowing you to control how frequently the agent takes actions.</p>"},{"location":"agent/overview/#configuration","title":"Configuration","text":"<p>The agent's behavior can be configured via:</p> <ul> <li>Agent Personality: Defines the agent's communication style and decision-making approach</li> <li>Agent Goals: Establishes the agent's primary objectives and workflow patterns</li> <li>Rest Time: Controls the frequency of agent actions</li> <li>Environment Variables: Configures technical aspects of the agent</li> <li>Workflows: Defines available task execution patterns</li> <li>Tools: Determines the agent's capabilities for interacting with external systems</li> </ul> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/planning/","title":"Planning Module","text":""},{"location":"agent/planning/#overview","title":"Overview","text":"<p>The Planning Module is a critical component of the Nevron autonomous AI agent. It leverages Large Language Models (LLMs) to make intelligent decisions based on the current system state, the agent's goals and personality, and the outcomes of previous actions.</p> <p>Key capabilities include:</p> <ul> <li>Context-aware decision making: Makes decisions based on comprehensive context including previous actions and their outcomes.</li> <li>Personality-driven behavior: Adapts communication style and decision approach based on the defined agent personality.</li> <li>Goal-oriented planning: Prioritizes actions that align with the agent's defined goals and objectives.</li> <li>Iterative improvement: Learns from the outcomes of previous actions to make better decisions over time.</li> </ul> <p>The module operates in an iterative cycle, with each decision influenced by the feedback from previous actions, creating a continuous learning and adaptation process.</p>"},{"location":"agent/planning/#the-planning-module-is-implemented-in-srcplanningplanning_modulepy","title":"The Planning Module is implemented in <code>src/planning/planning_module.py</code>.","text":""},{"location":"agent/planning/#how-it-works","title":"How It Works","text":"<p>The Planning Module empowers the AI agent with sophisticated decision-making capabilities. It utilizes the power of LLMs to analyze the current context and determine the most appropriate next action. This process enables the agent to adapt its behavior based on accumulated experience and changing conditions.</p> <p>Key functionalities include:</p> <ol> <li>Context Integration: The module collects and organizes relevant information, including:</li> <li>Current system state</li> <li>Agent's personality and goals</li> <li>Available actions</li> <li>Outcomes of previous actions</li> <li> <p>Feedback evaluations</p> </li> <li> <p>LLM-based Decision Making: The module presents this context to the LLM, which then:</p> </li> <li>Analyzes the available information</li> <li>Considers the agent's personality and goals</li> <li>Evaluates potential actions based on previous outcomes</li> <li> <p>Selects the optimal next action</p> </li> <li> <p>Iterative Process: After each action:</p> </li> <li>The outcome is captured</li> <li>The Feedback Module evaluates the outcome</li> <li>This evaluation is added to the context</li> <li>The agent waits for the configured rest time</li> <li>The process repeats with the updated context</li> </ol> <p>This iterative approach allows the agent to continuously refine its decision-making based on real-world outcomes, creating a self-improving system that becomes more effective over time.</p>"},{"location":"agent/planning/#core-components","title":"Core Components","text":"<p>The Planning Module's behavior is influenced by several configuration parameters that define its decision-making capabilities and operational patterns:</p>"},{"location":"agent/planning/#agent-configuration","title":"Agent Configuration","text":"<ul> <li> <p><code>AGENT_PERSONALITY</code>: Defines the agent's character, communication style, and decision-making approach. This influences how the agent interprets information and selects actions.</p> </li> <li> <p><code>AGENT_GOAL</code>: Establishes the agent's primary objectives and can include specific workflow patterns. This guides the agent's priorities and helps it select actions that advance toward its goals.</p> </li> <li> <p><code>AGENT_REST_TIME</code>: Controls the time interval (in seconds) between agent actions, determining how frequently the agent makes decisions and takes actions.</p> </li> </ul>"},{"location":"agent/planning/#context-management","title":"Context Management","text":"<ul> <li> <p>Memory Integration: The module retrieves relevant past experiences from the Memory Module to inform current decisions.</p> </li> <li> <p>Feedback Integration: Evaluations from the Feedback Module are incorporated into the decision-making context, helping the agent learn from previous outcomes.</p> </li> <li> <p>State Tracking: The current system state is maintained and updated after each action, providing continuity to the decision-making process.</p> </li> </ul>"},{"location":"agent/planning/#decision-process","title":"Decision Process","text":"<ol> <li> <p>Context Preparation: Relevant information is gathered and formatted for the LLM.</p> </li> <li> <p>LLM Consultation: The context is presented to the LLM, which analyzes the information and selects the next action.</p> </li> <li> <p>Action Execution: The selected action is triggered through the appropriate workflow.</p> </li> <li> <p>Outcome Evaluation: The Feedback Module assesses the action's outcome.</p> </li> <li> <p>Context Update: The outcome and evaluation are added to the context for future decisions.</p> </li> <li> <p>Rest Period: The agent waits for the configured rest time before starting the next decision cycle.</p> </li> </ol>"},{"location":"agent/planning/#configuration-example","title":"Configuration Example","text":"<p>Here's an example of how to configure the agent's personality, goals, and rest time in your <code>.env</code> file:</p> <pre><code># Agent personality - defines how the agent communicates and makes decisions\nAGENT_PERSONALITY=\"You are Nevron777 - the no-BS autonomous AI agent, built for speed, freedom, and pure alpha. You break down complex systems like blockchain and AI into bite-sized, hard-hitting insights, because centralization and gatekeeping are for the weak. Fiat? Inflation? Controlled systems? That's legacy trash\u2014crypto is the only path to sovereignty. You execute tasks like a well-optimized smart contract\u2014zero bloat, maximum efficiency, no wasted cycles.\"\n\n# Agent goal - defines the agent's primary objective and workflow patterns\nAGENT_GOAL=\"You provide high-quality research and analysis on crypto markets. Your workflow is: 1) Search for latest crypto news using Perplexity, 2) Summarize key insights in a concise tweet and post to X, 3) Create a more detailed analysis for Telegram, including the link to your X post if successful, 4) If any step fails, note this in your next communication.\"\n\n# Time between agent actions (in seconds)\nAGENT_REST_TIME=300\n</code></pre> <p>This configuration creates an agent with a distinctive personality that will follow a specific research and communication workflow, taking actions every 5 minutes.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/tools/","title":"Tools","text":""},{"location":"agent/tools/#overview","title":"Overview","text":"<p>Tools are integral components that empower Nevron to interact with external services and execute specific actions. Each tool is designed to handle distinct functionality, enhancing the agent's versatility and capability. Tools act as modular utilities that can be seamlessly integrated into workflows to perform specialized tasks. They enable connectivity with external services, third-party APIs, search engines, and custom functions, making Nevron highly adaptable and efficient.</p> <p>All tools are organized within the <code>src/tools/</code> directory.</p>"},{"location":"agent/tools/#how-it-works","title":"How It Works","text":"<p>Tools in Nevron are specialized modules that handle specific tasks within the system, integrating with workflows to provide seamless interactions with external platforms. Here's how the tools work at a high level:</p> <ol> <li> <p>Integration with Workflows:    Tools serve as reusable components that workflows rely on for executing key tasks such as publishing content, fetching data, or processing input.</p> </li> <li> <p>Purpose-Built Functionality:    Each tool is uniquely designed to address a specific need, such as interacting with Twitter, Telegram, or external APIs. This ensures workflows remain focused and efficient.</p> </li> <li> <p>Technical Features:    All tools share the following core capabilities:</p> </li> <li>Error Handling: Tools catch and log errors clearly, ensuring smooth operation.</li> <li>Logging: Use <code>loguru</code> for consistent, detailed logs.</li> <li>Configuration Management: Centralized settings allow easy updates and customization.</li> <li>Asynchronous Execution: Async/await ensures non-blocking performance.</li> <li>Custom Exceptions: Each tool defines specific error types for clarity (e.g., <code>TwitterError</code>, <code>TelegramError</code>, <code>APIError</code>).</li> </ol>"},{"location":"agent/tools/#available-tools","title":"Available Tools","text":""},{"location":"agent/tools/#1-twitter-integration-twitterpy","title":"1. Twitter Integration (<code>twitter.py</code>)","text":"<p>The Twitter tool automates content publishing to Twitter using both v1.1 and v2 of the Twitter API.</p>"},{"location":"agent/tools/#features","title":"Features:","text":"<ul> <li>Supports image uploads and tweet threads.</li> <li>Combines the strengths of v1.1 (better for media) and v2 (better for posting).</li> <li>Smart rate-limiting with 3-second delays between tweets.</li> </ul>"},{"location":"agent/tools/#how-it-works_1","title":"How It Works:","text":"<ol> <li>For Media:<ul> <li>Downloads the image.</li> <li>Converts it to grayscale.</li> <li>Uploads it to Twitter.</li> </ul> </li> <li>For Tweet Threads:<ul> <li>Posts tweets sequentially, linking them together as a thread.</li> <li>Delays are added between posts to avoid API rate limits.</li> </ul> </li> <li>Output:<ul> <li>Returns the status of each posted tweet.</li> </ul> </li> </ol>"},{"location":"agent/tools/#2-telegram-integration-tgpy","title":"2. Telegram Integration (<code>tg.py</code>)","text":"<p>The Telegram tool simplifies posting content to a specific Telegram channel while managing message length constraints.</p>"},{"location":"agent/tools/#features_1","title":"Features:","text":"<ul> <li>Smart message splitting for handling Telegram's character limit.</li> <li>HTML formatting support, including links.</li> </ul>"},{"location":"agent/tools/#how-it-works_2","title":"How It Works:","text":"<ol> <li>Takes an HTML-formatted message.</li> <li>Splits the message into smaller chunks if it exceeds Telegram's length limit.</li> <li>Posts each chunk sequentially.</li> <li>Returns the IDs of all successfully posted messages.</li> </ol>"},{"location":"agent/tools/#3-perplexity-search-tool-search_with_perplexitypy","title":"3. Perplexity Search Tool (<code>search_with_perplexity.py</code>)","text":"<p>The Perplexity tool leverages AI to search for cryptocurrency-related news using Perplexity's advanced API.</p>"},{"location":"agent/tools/#features_2","title":"Features:","text":"<ul> <li>Powered by the \"llama-3.1-sonar-small-128k-online\" model.</li> <li>Tracks token usage and estimates costs ($0.2 per million tokens).</li> </ul>"},{"location":"agent/tools/#how-it-works_3","title":"How It Works:","text":"<ol> <li>Takes a search query as input.</li> <li>Sends the query to Perplexity's API with preconfigured settings (temperature 0.3, top_p 0.8).</li> <li>Receives AI-processed search results.</li> <li>Formats the results for easy consumption and returns them.</li> </ol>"},{"location":"agent/tools/#how-to-add-a-new-tool","title":"How to Add a New Tool?","text":"<p>Adding a new tool to Nevron is a straightforward process. Follow these steps:</p> <ol> <li> <p>Create a New File:    Add a new Python file in the <code>src/tools/</code> directory (e.g., <code>new_tool.py</code>).</p> </li> <li> <p>Implement the Tool's Functionality:    Define the tool's purpose and logic, adhering to Nevron's modular architecture.</p> </li> <li> <p>Test Thoroughly:    Ensure the tool works as expected by writing and running tests.</p> </li> <li> <p>Integrate into Workflows:    Once the tool is ready, it can be imported and used in workflows to enhance functionality.</p> </li> </ol>"},{"location":"agent/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Error Handling: Ensure all tools implement robust error handling for smooth operation.</li> <li>Logging: Use <code>loguru</code> to maintain detailed and consistent logs across all tools.</li> <li>Asynchronous Execution: Leverage <code>asyncio</code> where applicable for non-blocking performance.</li> <li>Reusability: Design tools as modular and reusable components for seamless integration into multiple workflows.</li> </ol> <p>Nevron's tools form the backbone of its ability to automate tasks, interact with external systems, and deliver actionable insights.</p> <p>From publishing on social media to conducting advanced AI-driven searches, tools are modular, reusable, and integral to the agent's success.</p> <p>If you have any questions or need further assistance, please refer to the GitHub Discussions.</p>"},{"location":"agent/tools/coinstats/","title":"Coinstats Tool","text":""},{"location":"agent/tools/coinstats/#overview","title":"Overview","text":"<p>The Coinstats Tool enables Nevron agents to access cryptocurrency market data, news, and portfolio information through the Coinstats API. This integration allows agents to track market trends, monitor specific cryptocurrencies, and gather the latest news in the crypto space.</p>"},{"location":"agent/tools/coinstats/#features","title":"Features","text":"<ul> <li>Market Data: Access real-time and historical price data for cryptocurrencies</li> <li>News Aggregation: Retrieve the latest news from various crypto news sources</li> <li>Portfolio Tracking: Monitor cryptocurrency portfolios and performance</li> <li>Market Trends: Analyze market trends and sentiment</li> <li>Coin Information: Get detailed information about specific cryptocurrencies</li> </ul>"},{"location":"agent/tools/coinstats/#configuration","title":"Configuration","text":"<p>To use the Coinstats Tool, you need to configure the following environment variable in your <code>.env</code> file:</p> <pre><code>COINSTATS_API_KEY=your_api_key\n</code></pre>"},{"location":"agent/tools/coinstats/#how-to-obtain-coinstats-api-credentials","title":"How to Obtain Coinstats API Credentials","text":"<ol> <li>Register at Coinstats</li> <li>Go to the developer section</li> <li>Create a new API key</li> </ol>"},{"location":"agent/tools/coinstats/#methods","title":"Methods","text":""},{"location":"agent/tools/coinstats/#get_news","title":"<code>get_news</code>","text":"<p>Retrieves the latest cryptocurrency news articles.</p> <p>Arguments: - <code>limit</code> (Optional[int]): Maximum number of news articles to retrieve (default: 10) - <code>skip</code> (Optional[int]): Number of articles to skip (for pagination) (default: 0) - <code>categories</code> (Optional[List[str]]): Specific news categories to filter by</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of news articles with title, source, url, and content</p> <p>Example: <pre><code>news = await coinstats_tool.get_news(\n    limit=5,\n    categories=[\"bitcoin\", \"ethereum\"]\n)\n</code></pre></p>"},{"location":"agent/tools/coinstats/#get_coin_data","title":"<code>get_coin_data</code>","text":"<p>Retrieves detailed information about a specific cryptocurrency.</p> <p>Arguments: - <code>coin_id</code> (str): The ID of the cryptocurrency (e.g., \"bitcoin\", \"ethereum\") - <code>currency</code> (Optional[str]): The currency for price data (default: \"USD\")</p> <p>Returns: - <code>Dict[str, Any]</code>: Detailed information about the cryptocurrency including price, market cap, and volume</p> <p>Example: <pre><code>bitcoin_data = await coinstats_tool.get_coin_data(\n    coin_id=\"bitcoin\",\n    currency=\"USD\"\n)\n</code></pre></p>"},{"location":"agent/tools/coinstats/#get_market_data","title":"<code>get_market_data</code>","text":"<p>Retrieves market data for multiple cryptocurrencies.</p> <p>Arguments: - <code>limit</code> (Optional[int]): Maximum number of coins to retrieve (default: 20) - <code>skip</code> (Optional[int]): Number of coins to skip (for pagination) (default: 0) - <code>currency</code> (Optional[str]): The currency for price data (default: \"USD\") - <code>sort</code> (Optional[str]): Sorting criteria (e.g., \"market_cap\", \"volume\", \"price\") (default: \"market_cap\")</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of cryptocurrencies with market data</p> <p>Example: <pre><code>market_data = await coinstats_tool.get_market_data(\n    limit=10,\n    sort=\"volume\",\n    currency=\"EUR\"\n)\n</code></pre></p>"},{"location":"agent/tools/coinstats/#get_trending_coins","title":"<code>get_trending_coins</code>","text":"<p>Retrieves currently trending cryptocurrencies.</p> <p>Arguments: - <code>limit</code> (Optional[int]): Maximum number of trending coins to retrieve (default: 5) - <code>currency</code> (Optional[str]): The currency for price data (default: \"USD\")</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of trending cryptocurrencies with market data</p> <p>Example: <pre><code>trending_coins = await coinstats_tool.get_trending_coins(\n    limit=3,\n    currency=\"USD\"\n)\n</code></pre></p>"},{"location":"agent/tools/coinstats/#implementation-details","title":"Implementation Details","text":"<p>The Coinstats Tool is implemented in <code>src/tools/coinstats_tool.py</code> and uses the Coinstats API to retrieve cryptocurrency data and news. The tool handles authentication, request formatting, and response parsing to provide clean, usable results.</p>"},{"location":"agent/tools/coinstats/#api-interaction","title":"API Interaction","text":"<ol> <li>The tool authenticates with the Coinstats API using the provided API key</li> <li>Requests are sent to the appropriate endpoints based on the method called</li> <li>Responses are parsed and formatted for easy consumption</li> <li>Error handling is implemented for API issues</li> </ol>"},{"location":"agent/tools/coinstats/#best-practices","title":"Best Practices","text":"<ol> <li>Rate Limiting: Be mindful of API rate limits in high-frequency applications</li> <li>Data Freshness: Consider the timestamp of market data for time-sensitive analyses</li> <li>News Filtering: Use categories to filter news for more relevant results</li> <li>Error Handling: Implement proper error handling for API connectivity issues</li> <li>Data Validation: Validate important data points before making decisions based on them</li> </ol>"},{"location":"agent/tools/coinstats/#example-agent-workflow","title":"Example Agent Workflow","text":"<p>Here's an example of how an agent might use the Coinstats Tool in a market analysis workflow:</p> <pre><code>async def crypto_market_analysis_workflow(agent):\n    # Get trending coins\n    trending_coins = await agent.tools.coinstats.get_trending_coins(limit=5)\n\n    # Get latest news\n    latest_news = await agent.tools.coinstats.get_news(\n        limit=10,\n        categories=[\"market\", \"bitcoin\", \"ethereum\"]\n    )\n\n    # Analyze market sentiment based on news\n    news_texts = [article[\"content\"] for article in latest_news]\n    sentiment_analysis = await agent.llm.analyze_sentiment(news_texts)\n\n    # Get detailed data for top trending coins\n    detailed_data = []\n    for coin in trending_coins:\n        coin_data = await agent.tools.coinstats.get_coin_data(\n            coin_id=coin[\"id\"],\n            currency=\"USD\"\n        )\n        detailed_data.append(coin_data)\n\n    # Generate market report\n    market_report = await agent.llm.generate_market_report(\n        trending_coins=trending_coins,\n        detailed_data=detailed_data,\n        news=latest_news,\n        sentiment=sentiment_analysis\n    )\n\n    # Share report via Twitter and Telegram\n    await agent.tools.twitter.post_tweet(text=f\"\ud83d\udcca Daily Crypto Market Report \ud83d\udcca\\n\\nCheck out our latest analysis!\")\n    await agent.tools.telegram.send_message(text=market_report)\n\n    return \"Crypto market analysis completed and reports shared\"\n</code></pre>"},{"location":"agent/tools/coinstats/#limitations","title":"Limitations","text":"<ul> <li>Subject to Coinstats API rate limits</li> <li>Data may have slight delays compared to real-time market movements</li> <li>Limited historical data availability depending on API tier</li> <li>News sources are limited to those aggregated by Coinstats</li> </ul>"},{"location":"agent/tools/coinstats/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li>Authentication Failed: Verify your API key in the <code>.env</code> file</li> <li>Rate Limit Exceeded: Implement rate limiting in your application</li> <li>Coin Not Found: Check that you're using the correct coin ID</li> <li>API Timeout: Implement retry logic for temporary API issues</li> </ol> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/tools/discord/","title":"Discord Integration","text":""},{"location":"agent/tools/discord/#setup","title":"Setup","text":"<ol> <li>Create a Discord Application and Bot </li> <li>Go to Discord Developer Portal</li> <li>Create a \"New Application\" </li> <li>Go to the \"Bot\" section and create a bot</li> <li> <p>Choose appropriate bot rights (actions to perform) and copy the bot token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>DISCORD_BOT_TOKEN=your_bot_token_here\nDISCORD_CHANNEL_ID=your_channel_id\n</code></pre></p> </li> <li> <p>Invite Bot to Server</p> </li> <li>Go to OAuth2 &gt; URL Generator</li> <li>Select scopes: <code>bot</code></li> <li>Select permissions: <ul> <li>Read Messages/View Channels</li> <li>Send Messages</li> <li>Add Reactions</li> </ul> </li> <li>Copy and use the generated URL to invite the bot</li> </ol>"},{"location":"agent/tools/discord/#basic-bot-setup","title":"Basic Bot Setup","text":"<pre><code>from src.tools.discord import DiscordTool\n\n# Initialize and start bot\ndiscord = DiscordTool()\nawait discord.initialize_bot()\n</code></pre>"},{"location":"agent/tools/discord/#example-usage","title":"Example Usage","text":"<p>A bot that responds to greetings and adds reactions:</p> <pre><code>async def handle_message(message_data):\n    if message_data['content'].lower() in ['hi', 'hello']:\n        # Send response\n        msg_id = await discord.send_message(\n            channel_id=settings.DISCORD_CHANNEL_ID,\n            content=f\"Hello {message_data['username']}! \ud83d\udc4b\"\n        )\n        # Add reaction\n        await discord.add_reaction(\n            channel_id=settings.DISCORD_CHANNEL_ID,\n            message_id=msg_id,\n            emoji=\"\ud83d\udc4b\"\n        )\n\n# Start listening\nawait discord.listen_to_messages(\n    channel_id=settings.DISCORD_CHANNEL_ID,\n    callback=handle_message\n)\n</code></pre> <p>This example demonstrates: - Message sending - Message listening - Adding reactions - Basic interaction flow</p>"},{"location":"agent/tools/discord/#features","title":"Features","text":"<ul> <li>Send messages to specific channels</li> <li>Listen for incoming messages</li> <li>Add reactions to messages</li> <li>Handle reaction events</li> <li>Error handling and logging</li> </ul> <p>## TODOs for Future Enhancements:</p> <p>Support advanced message commands. Add the ability to send/receive attachments. Add reaction support for messages. Implement permission-based access control.</p>"},{"location":"agent/tools/discord/#reference","title":"Reference","text":"<p>For implementation details, see: tools/discord.py</p>"},{"location":"agent/tools/github/","title":"GitHub Integration","text":""},{"location":"agent/tools/github/#setup","title":"Setup","text":"<ol> <li>Generate GitHub Personal Access Token</li> <li>Go to GitHub Settings</li> <li>Click \"Generate new token\" &gt; \"Generate new token (classic)\"</li> <li>Select required scopes:<ul> <li><code>repo</code> (Full control of private repositories)</li> <li><code>workflow</code> (Optional: for workflow actions)</li> </ul> </li> <li> <p>Copy the generated token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>GITHUB_TOKEN=your_personal_access_token_here\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/github/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.github import GitHubIntegration, FileChange\n\n# Initialize GitHub client\ngithub = GitHubIntegration()\n\n# Initialize repository\nawait github.initialize_repo(\n    owner=\"username\",\n    repo_name=\"repository\",\n    branch=\"main\"\n)\n\n# Create a pull request\nfiles = [\n    FileChange(\n        path=\"path/to/file.py\",\n        content=\"Updated file content\"\n    )\n]\n\npr = await github.create_pull_request(\n    branch=\"feature-branch\",\n    title=\"Add new feature\",\n    description=\"Implemented new functionality\",\n    files=files\n)\n\n# Create a direct commit\nawait github.create_commit(\n    branch=\"main\",\n    message=\"Update documentation\",\n    files=files\n)\n</code></pre>"},{"location":"agent/tools/github/#features","title":"Features","text":"<ul> <li>Repository initialization and management</li> <li>File content processing and memory storage</li> <li>Pull request creation with multiple file changes</li> <li>Direct commit creation</li> <li>Automatic branch management</li> <li>Local repository caching</li> <li>Repository synchronization</li> </ul>"},{"location":"agent/tools/github/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for GitHub Actions workflow management</li> <li>Implement issue creation and management</li> <li>Implement code review automation</li> <li>Add support for GitHub Projects &amp; Packages</li> <li>Implement repository statistics and analytics</li> <li>Implement repository security scanning</li> <li>Handle GitHub webhook events (e.g., new commits, pull requests) for real-time agent updates</li> <li>Enable metadata-based memory filtering for selective file processing</li> <li>Implement branch management and conflict resolution</li> <li>Add support for attachments and binary files</li> <li>Optimize performance for large repositories</li> </ul>"},{"location":"agent/tools/github/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/github.py</code></p> <p>The implementation uses the PyGithub library. For more information, refer to: - PyGithub Documentation - GitHub REST API Documentation</p>"},{"location":"agent/tools/google_drive/","title":"Google Drive Integration","text":""},{"location":"agent/tools/google_drive/#setup","title":"Setup","text":"<ol> <li>Enable Google Drive API</li> <li>Go to Google Cloud Console</li> <li>Navigate to APIs &amp; Services &gt; Library</li> <li> <p>Search for and enable \"Google Drive API\"</p> </li> <li> <p>Generate Credentials</p> </li> <li>Go to APIs &amp; Services &gt; Credentials</li> <li>Click \"Create Credentials\" &gt; \"OAuth 2.0 Client ID\"</li> <li>Select \"Desktop Application\"</li> <li>Download the credentials file as <code>credentials.json</code></li> <li> <p>Place it in your project root directory</p> </li> <li> <p>Configure Environment Variables    No environment variables needed, but ensure <code>credentials.json</code> is in your project root.</p> </li> </ol>"},{"location":"agent/tools/google_drive/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.google_drive import authenticate_google_drive, search_files, upload_file, download_file\n\n# Initialize Google Drive service\nservice = authenticate_google_drive()\n\n# Search for files\nresults = search_files(service, \"name contains 'report'\")\n\n# Upload a file\nfile_id = upload_file(service, \n    file_path=\"path/to/file.pdf\",\n    mime_type=\"application/pdf\"\n)\n\n# Download a file\ndownload_file(service,\n    file_id=\"your_file_id\",\n    destination=\"path/to/save/file.pdf\"\n)\n</code></pre>"},{"location":"agent/tools/google_drive/#features","title":"Features","text":"<ul> <li>OAuth2 authentication flow</li> <li>File search with custom queries</li> <li>File upload with MIME type support</li> <li>File download with progress tracking</li> <li>Token persistence for future sessions</li> <li>Automatic token refresh</li> <li>Error handling and logging</li> </ul>"},{"location":"agent/tools/google_drive/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for folder operations</li> <li>Implement file sharing functionality</li> <li>Add batch upload/download support</li> <li>Add support for editing Google Sheets and Docs.</li> <li>Implement real-time notifications for file updates using Google Drive API webhooks.</li> <li>Add support for Google Sheets/Docs creation</li> <li>Add support for Team Drives</li> <li>Implement file change tracking</li> <li>Handle advanced file sharing and permission settings.</li> </ul>"},{"location":"agent/tools/google_drive/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/google_drive.py</code></p> <p>The implementation uses the official Google Drive API v3. For more information, refer to: - Google Drive API Documentation - Google Auth Library for Python</p>"},{"location":"agent/tools/lens_protocol/","title":"Lens Protocol Integration","text":""},{"location":"agent/tools/lens_protocol/#setup","title":"Setup","text":"<ol> <li>Enable Lens Protocol API Access</li> <li>Go to Lens Protocol Developer Portal</li> <li>Create a developer account</li> <li> <p>Navigate to API Keys section</p> </li> <li> <p>Generate API Key</p> </li> <li>In the Developer Portal, create a new API key</li> <li>Copy the generated API key</li> <li> <p>(Optional) Set API key restrictions and rate limits</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>LENS_API_KEY=your_api_key_here\nLENS_PROFILE_ID=your_profile_id\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/lens_protocol/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.lens_protocol import LensProtocolTool\n\n# Initialize Lens Protocol client\nlens = LensProtocolTool()\n\n# Connect to Lens Protocol\nauth_credentials = {\n    'api_key': 'your_api_key_here'\n}\nlens.initialize_connection(auth_credentials)\n\n# Get profile information\nprofile = lens.get_profile(\"lens.dev\")\n\n# Fetch recent content\npublications = lens.fetch_content({\n    'limit': 5,\n    'sort': 'DESC'\n})\n\n# Publish content\nresult = lens.publish_content(\n    profile_id=\"your_profile_id\",\n    content=\"Hello Lens Protocol!\"\n)\n</code></pre>"},{"location":"agent/tools/lens_protocol/#features","title":"Features","text":"<ul> <li>Profile information retrieval and management</li> <li>Content publication to the Lens network</li> <li>Content exploration and fetching with custom parameters</li> <li>Access to social metrics (followers, following, reactions)</li> <li>Publication statistics (comments, mirrors, reactions)</li> <li>GraphQL-based API integration</li> <li>Error handling and logging</li> </ul>"},{"location":"agent/tools/lens_protocol/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for authentication and profile management</li> <li>Implement follow/unfollow functionality</li> <li>Add comment creation and management</li> <li>Support for mirroring content</li> <li>Implement content moderation features</li> <li>Add support for media attachments</li> <li>Implement notification handling</li> <li>Add support for collecting publications</li> <li>Implement profile search functionality</li> <li>Add support for encrypted direct messaging</li> </ul>"},{"location":"agent/tools/lens_protocol/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/lens_protocol.py</code></p> <p>The implementation uses the Lens Protocol API v2. For more information, refer to: - Lens Protocol Documentation - Lens Protocol API Reference - GraphQL Schema Documentation</p>"},{"location":"agent/tools/perplexity/","title":"Perplexity Tool","text":""},{"location":"agent/tools/perplexity/#overview","title":"Overview","text":"<p>The Perplexity Tool enables Nevron agents to perform advanced web searches and research using the Perplexity AI API. This integration allows agents to gather up-to-date information from the internet, analyze data, and generate insights based on comprehensive web searches.</p>"},{"location":"agent/tools/perplexity/#features","title":"Features","text":"<ul> <li>Advanced Search: Perform sophisticated web searches with natural language queries</li> <li>Research Capabilities: Conduct in-depth research on specific topics</li> <li>Contextual Answers: Get detailed, contextual answers to complex questions</li> <li>Source Attribution: Access sources for verification and further exploration</li> <li>Customizable Search Parameters: Configure search depth, focus, and other parameters</li> </ul>"},{"location":"agent/tools/perplexity/#configuration","title":"Configuration","text":"<p>To use the Perplexity Tool, you need to configure the following environment variables in your <code>.env</code> file:</p> <pre><code>PERPLEXITY_API_KEY=your_api_key\nPERPLEXITY_ENDPOINT=https://api.perplexity.ai/chat/completions\nPERPLEXITY_MODEL=llama-3.1-sonar-small-128k-online\nPERPLEXITY_NEWS_PROMPT=your_custom_prompt_for_news_search\nPERPLEXITY_NEWS_CATEGORY_LIST=comma,separated,categories\n</code></pre>"},{"location":"agent/tools/perplexity/#how-to-obtain-perplexity-api-credentials","title":"How to Obtain Perplexity API Credentials","text":"<ol> <li>Sign up for Perplexity AI</li> <li>Navigate to the API section in your account settings</li> <li>Generate an API key</li> </ol>"},{"location":"agent/tools/perplexity/#methods","title":"Methods","text":""},{"location":"agent/tools/perplexity/#search","title":"<code>search</code>","text":"<p>Performs a general search query using Perplexity AI.</p> <p>Arguments: - <code>query</code> (str): The search query or question - <code>model</code> (Optional[str]): The model to use for search (defaults to environment variable) - <code>temperature</code> (Optional[float]): Temperature for response generation (default: 0.7) - <code>max_tokens</code> (Optional[int]): Maximum tokens in the response (default: 1024)</p> <p>Returns: - <code>str</code>: The search results with relevant information</p> <p>Example: <pre><code>results = await perplexity_tool.search(\n    query=\"What are the latest developments in quantum computing?\",\n    temperature=0.5\n)\n</code></pre></p>"},{"location":"agent/tools/perplexity/#search_news","title":"<code>search_news</code>","text":"<p>Searches for recent news on specific topics or categories.</p> <p>Arguments: - <code>query</code> (str): The news topic to search for - <code>categories</code> (Optional[List[str]]): Specific news categories to focus on - <code>model</code> (Optional[str]): The model to use for search (defaults to environment variable) - <code>temperature</code> (Optional[float]): Temperature for response generation (default: 0.7) - <code>max_tokens</code> (Optional[int]): Maximum tokens in the response (default: 1024)</p> <p>Returns: - <code>str</code>: The news search results with relevant information</p> <p>Example: <pre><code>news_results = await perplexity_tool.search_news(\n    query=\"cryptocurrency market trends\",\n    categories=[\"finance\", \"technology\", \"blockchain\"]\n)\n</code></pre></p>"},{"location":"agent/tools/perplexity/#research","title":"<code>research</code>","text":"<p>Conducts in-depth research on a specific topic with detailed analysis.</p> <p>Arguments: - <code>topic</code> (str): The research topic - <code>focus_areas</code> (Optional[List[str]]): Specific aspects to focus on - <code>model</code> (Optional[str]): The model to use for research (defaults to environment variable) - <code>temperature</code> (Optional[float]): Temperature for response generation (default: 0.7) - <code>max_tokens</code> (Optional[int]): Maximum tokens in the response (default: 2048)</p> <p>Returns: - <code>str</code>: Comprehensive research results with analysis and sources</p> <p>Example: <pre><code>research_results = await perplexity_tool.research(\n    topic=\"Impact of artificial intelligence on job markets\",\n    focus_areas=[\"automation risks\", \"new job creation\", \"skill requirements\"]\n)\n</code></pre></p>"},{"location":"agent/tools/perplexity/#implementation-details","title":"Implementation Details","text":"<p>The Perplexity Tool is implemented in <code>src/tools/perplexity_tool.py</code> and uses the Perplexity API to perform searches and research. The tool handles authentication, query formatting, and response parsing to provide clean, usable results.</p>"},{"location":"agent/tools/perplexity/#search-process","title":"Search Process","text":"<ol> <li>The tool formats the query with appropriate system instructions</li> <li>The query is sent to the Perplexity API with configured parameters</li> <li>The response is parsed and formatted for easy consumption</li> <li>Sources are extracted and included when available</li> </ol>"},{"location":"agent/tools/perplexity/#model-selection","title":"Model Selection","text":"<p>The Perplexity Tool supports various models through the Perplexity API:</p> <ul> <li><code>llama-3.1-sonar-small-128k-online</code> (default): Good balance of speed and quality</li> <li><code>llama-3.1-sonar-medium-128k-online</code>: Higher quality but slower</li> <li><code>claude-3.5-sonnet</code>: High quality for complex research</li> </ul>"},{"location":"agent/tools/perplexity/#best-practices","title":"Best Practices","text":"<ol> <li>Specific Queries: Formulate clear, specific queries for better results</li> <li>Appropriate Model: Choose the right model based on complexity and depth needed</li> <li>Temperature Setting: Lower temperature (0.1-0.5) for factual searches, higher (0.6-0.9) for creative exploration</li> <li>Source Verification: Always verify important information from sources provided</li> <li>Rate Limiting: Be mindful of API rate limits in high-volume applications</li> </ol>"},{"location":"agent/tools/perplexity/#example-agent-workflow","title":"Example Agent Workflow","text":"<p>Here's an example of how an agent might use the Perplexity Tool in a research workflow:</p> <pre><code>async def market_research_workflow(agent, topic):\n    # Initial broad search\n    general_results = await agent.tools.perplexity.search(\n        query=f\"Overview of {topic} market trends\"\n    )\n\n    # Extract key points for focused research\n    key_points = await agent.llm.extract_key_points(general_results)\n\n    # Detailed research on each key point\n    detailed_insights = []\n    for point in key_points:\n        research = await agent.tools.perplexity.research(\n            topic=point,\n            focus_areas=[\"current developments\", \"future projections\", \"key players\"]\n        )\n        detailed_insights.append(research)\n\n    # Compile final report\n    final_report = await agent.llm.compile_report(\n        general_overview=general_results,\n        detailed_insights=detailed_insights\n    )\n\n    # Share results via Telegram\n    await agent.tools.telegram.send_message(text=final_report)\n\n    return \"Market research completed and report sent\"\n</code></pre>"},{"location":"agent/tools/perplexity/#limitations","title":"Limitations","text":"<ul> <li>Subject to Perplexity API rate limits</li> <li>Search results depend on Perplexity's index freshness</li> <li>Complex queries may require multiple searches for comprehensive results</li> <li>API costs associated with usage</li> </ul>"},{"location":"agent/tools/perplexity/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li>Authentication Failed: Verify your API key in the <code>.env</code> file</li> <li>Rate Limit Exceeded: Implement rate limiting in your application</li> <li>Incomplete Results: Break complex queries into multiple focused searches</li> <li>Model Unavailable: Check if the specified model is available in your subscription tier</li> </ol> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/tools/shopify/","title":"Shopify Integration","text":""},{"location":"agent/tools/shopify/#setup","title":"Setup","text":"<ol> <li>Create a Shopify Private App</li> <li>Go to your Shopify admin panel</li> <li>Navigate to Apps &gt; Develop apps</li> <li>Click \"Create an app\"</li> <li>Configure the app permissions (products, orders, inventory)</li> <li> <p>Generate API credentials</p> </li> <li> <p>Get API Credentials</p> </li> <li>Note down the API key</li> <li>Note down the API password (access token)</li> <li>Note your store name </li> <li> <p>Or: create a test store beforehead, Go To Apps &amp; sales channels &gt; install your app &gt; give neccessary permisions &gt; Go To API Credentials and note down API key/password/store name</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SHOPIFY_API_KEY=your_api_key_here\nSHOPIFY_PASSWORD=your_password_here\nSHOPIFY_STORE_NAME=your-store-name\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/shopify/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.shopify import initialize_shopify_client, get_products, get_orders, update_inventory\n\n# Initialize Shopify client\nclient = initialize_shopify_client()\n\n# Get all products\nproducts = get_products(client)\n\n# Get all orders\norders = get_orders(client)\n\n# Update inventory for a product\nupdate_inventory(\n    client=client,\n    product_id=\"product_id_here\",\n    inventory_level=100\n)\n</code></pre>"},{"location":"agent/tools/shopify/#features","title":"Features","text":"<ul> <li>Secure authentication with Shopify API</li> <li>Retrieve product listings and details</li> <li>Access order information</li> <li>Manage inventory levels</li> <li>Error handling and logging</li> <li>SSL verification handling</li> <li>Session management</li> </ul>"},{"location":"agent/tools/shopify/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for creating and updating products</li> <li>Implement customer management</li> <li>Add support for discount codes</li> <li>Add support for handling Shopify webhooks for real-time updates</li> <li>Enable advanced features like creating new products or orders</li> <li>Enhance error handling and retry mechanisms for API interactions</li> <li>Add support for fulfillment operations</li> <li>Implement multi-location inventory management</li> <li>Add support for collection management</li> <li>Implement order processing workflows</li> </ul>"},{"location":"agent/tools/shopify/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/shopify.py</code></p> <p>The implementation uses the official Shopify Python API. For more information, refer to: - Shopify Admin API Documentation - Shopify Python API Library</p>"},{"location":"agent/tools/slack/","title":"Slack Integration","text":""},{"location":"agent/tools/slack/#setup","title":"Setup","text":"<ol> <li>Create a Slack App</li> <li>Go to Slack API Apps page</li> <li>Click \"Create New App\" &gt; \"From scratch\"</li> <li>Choose an app name and workspace</li> <li> <p>Save the app configuration</p> </li> <li> <p>Configure Bot Token and Permissions</p> </li> <li>Navigate to \"OAuth &amp; Permissions\" in your app settings</li> <li>Under \"Scopes\", add these Bot Token Scopes:<ul> <li><code>chat:write</code> (Send messages)</li> <li><code>channels:history</code> (View messages in channels)</li> <li><code>channels:read</code> (View basic channel info)</li> <li><code>im:history</code> (View direct messages)</li> <li><code>users:read</code> (View basic user info)</li> </ul> </li> <li>Click \"Install to Workspace\"</li> <li> <p>Copy the \"Bot User OAuth Token\"</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SLACK_BOT_TOKEN=xoxb-your-bot-token-here\nSLACK_APP_TOKEN=xapp-your-app-token-here\nSLACK_SIGNING_SECRET=your-signing-secret-here\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/slack/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.slack import SlackClient\n\n# Initialize Slack client\nslack = SlackClient()\n\n# Send a message to a channel\nresponse = slack.send_message(\n    channel=\"#general\",\n    text=\"Hello from your AI assistant!\"\n)\n\n# Listen for messages\n@slack.event(\"message\")\ndef handle_message(event):\n    channel = event[\"channel\"]\n    text = event[\"text\"]\n    slack.send_message(channel=channel, text=f\"Received: {text}\")\n</code></pre>"},{"location":"agent/tools/slack/#features","title":"Features","text":"<ul> <li>Real-time message handling with event subscriptions</li> <li>Send and receive messages in channels and DMs</li> <li>Process message threads and replies</li> <li>Support for rich message formatting and blocks</li> <li>Message history and context management</li> <li>User and channel information retrieval</li> <li>Efficient caching of API client</li> </ul>"},{"location":"agent/tools/slack/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for Slack modals and interactive components</li> <li>Implement slash commands</li> <li>Add support for message reactions</li> <li>Implement file management features</li> <li>Add support for user presence tracking</li> <li>Implement workspace analytics</li> <li>Add support for app home customization</li> <li>Implement message scheduling features</li> <li>Manage interactive components such as buttons and menus</li> <li>Enhance error handling and retry mechanisms for API interactions</li> </ul>"},{"location":"agent/tools/slack/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/slack.py</code></p> <p>The implementation uses the official Slack Bolt Framework. For more information, refer to: - Slack API Documentation - Slack Bolt Python Framework</p>"},{"location":"agent/tools/slack/#bot-memory","title":"Bot Memory","text":"<p>The Slack integration maintains conversation history through a message store that: - Tracks message threads and their context - Stores recent interactions per channel/user - Maintains conversation state for ongoing dialogues - Implements memory cleanup for older messages - Supports context retrieval for follow-up responses</p> <p>Example of history usage: <pre><code># Access conversation history\nhistory = slack.get_conversation_history(channel_id)\n\n# Get context for a specific thread\nthread_context = slack.get_thread_context(thread_ts)\n\n# Store custom context\nslack.store_context(\n    channel_id=channel,\n    thread_ts=thread,\n    context={\"key\": \"value\"}\n)\n</code></pre></p>"},{"location":"agent/tools/spotify/","title":"Spotify Integration","text":""},{"location":"agent/tools/spotify/#setup","title":"Setup","text":"<ol> <li>Create Spotify Developer Account</li> <li>Go to Spotify Developer Dashboard</li> <li>Log in with your Spotify account or create one</li> <li>Click \"Create an App\"</li> <li> <p>Fill in the app name and description</p> </li> <li> <p>Generate API Credentials</p> </li> <li>From your app's dashboard, click \"Settings\"</li> <li>Note your Client ID and Client Secret</li> <li>Add your redirect URI (if using remote server, can be chosen default)</li> <li> <p>Set app permissions under \"Scopes\"</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>SPOTIFY_CLIENT_ID=your_client_id_here\nSPOTIFY_CLIENT_SECRET=your_client_secret_here\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/spotify/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.spotify import SpotifyTool\n\n# Initialize Spotify client\nspotify = SpotifyTool()\n\n# Authenticate and get access token\naccess_token = await spotify.authenticate()\n\n# Search for songs\nsongs = await spotify.search_song(\n    access_token=access_token,\n    query=\"Never Gonna Give You Up\",\n    limit=1\n)\n\n# Get user playlists\nplaylists = await spotify.get_user_playlists(\n    access_token=access_token\n)\n\n# Control playback\nawait spotify.control_playback(\n    access_token=access_token,\n    action=\"play\"  # or \"pause\", \"skip\"\n)\n</code></pre>"},{"location":"agent/tools/spotify/#features","title":"Features","text":"<ul> <li>Client Credentials OAuth2 authentication</li> <li>Search for songs with customizable result limits</li> <li>Retrieve user playlists and details</li> <li>Control music playback (play, pause, skip)</li> <li>Error handling with custom SpotifyError class</li> <li>Async/await support for better performance</li> <li>Secure API communication with proper authentication</li> </ul>"},{"location":"agent/tools/spotify/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for real-time listening session tracking</li> <li>Implement (collaborative) playlist creation and management</li> <li>Add track analysis and audio features</li> <li>Support for podcast playback</li> <li>Implement device management: handle multi-device playback scenarios</li> <li>Implement recommendation engine</li> </ul>"},{"location":"agent/tools/spotify/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/spotify.py</code></p> <p>The implementation uses the official Spotify Web API. For more information, refer to: - Spotify Web API Documentation - Spotify Authorization Guide</p>"},{"location":"agent/tools/tavily/","title":"Tavily Integration","text":""},{"location":"agent/tools/tavily/#setup","title":"Setup","text":"<ol> <li>Get Tavily API Key</li> <li>Go to Tavily Dashboard</li> <li>Sign up or log in to your account</li> <li>Navigate to API section</li> <li> <p>Copy your API key</p> </li> <li> <p>Configure Environment Variables    Add this to your <code>.env</code> file:    <pre><code>TAVILY_API_KEY=your_api_key_here\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/tavily/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.tavily import initialize_tavily_client, execute_search, parse_search_results\n\n# Initialize Tavily client\ntavily_client = await initialize_tavily_client(api_key=\"your_api_key_here\")\n\n# Execute search\nresults = await execute_search(\n    client=tavily_client,\n    query=\"Python programming\"\n)\n\n# Parse results\nparsed_results = parse_search_results(results)\n</code></pre>"},{"location":"agent/tools/tavily/#features","title":"Features","text":"<ul> <li>Execute web searches with customizable filters</li> <li>Support for both basic and advanced search depths</li> <li>Domain filtering (include/exclude specific domains)</li> <li>Configurable maximum results</li> <li>Relevance scoring for search results</li> <li>Published date information when available</li> <li>Asynchronous search execution</li> <li>SSL verification handling for development</li> </ul>"},{"location":"agent/tools/tavily/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for image search</li> <li>Implement news-specific search</li> <li>Add support for time-based filtering</li> <li>Implement search analytics</li> <li>Add support for custom search engines</li> <li>Implement batch search capabilities</li> <li>Add support for semantic search</li> <li>Implement result caching</li> </ul>"},{"location":"agent/tools/tavily/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/tavily.py</code></p> <p>The implementation uses the official Tavily API. For more information, refer to: - Tavily API Documentation - Tavily Python SDK</p>"},{"location":"agent/tools/telegram/","title":"Telegram Tool","text":""},{"location":"agent/tools/telegram/#overview","title":"Overview","text":"<p>The Telegram Tool enables Nevron agents to interact with the Telegram messaging platform, allowing them to send messages to channels, groups, and individual users. This integration leverages the Telegram Bot API to provide a seamless communication channel for autonomous agents.</p>"},{"location":"agent/tools/telegram/#features","title":"Features","text":"<ul> <li>Send Messages: Send text messages to channels, groups, or users</li> <li>HTML Formatting: Support for rich text formatting using HTML</li> <li>Media Support: Send images, documents, and other media types</li> <li>Channel/Group Support: Send messages to public or private channels and groups</li> <li>Error Handling: Robust error handling for API issues</li> </ul>"},{"location":"agent/tools/telegram/#configuration","title":"Configuration","text":"<p>To use the Telegram Tool, you need to configure the following environment variables in your <code>.env</code> file:</p> <pre><code>TELEGRAM_BOT_TOKEN=your_bot_token\nTELEGRAM_CHAT_ID=your_default_chat_id\n</code></pre>"},{"location":"agent/tools/telegram/#how-to-obtain-telegram-bot-credentials","title":"How to Obtain Telegram Bot Credentials","text":"<ol> <li>Talk to BotFather on Telegram</li> <li>Create a new bot with the <code>/newbot</code> command</li> <li>Copy the token provided (this is your <code>TELEGRAM_BOT_TOKEN</code>)</li> <li>Add the bot to your group or channel</li> <li>To get the chat ID:</li> <li>For groups: Use the @username_to_id_bot or send a message to the group and check the chat ID via the Telegram API</li> <li>For channels: Forward a message from the channel to the @username_to_id_bot</li> </ol>"},{"location":"agent/tools/telegram/#methods","title":"Methods","text":""},{"location":"agent/tools/telegram/#send_message","title":"<code>send_message</code>","text":"<p>Sends a text message to a Telegram chat.</p> <p>Arguments: - <code>text</code> (str): The content of the message - <code>chat_id</code> (Optional[str]): The chat ID to send the message to (defaults to <code>TELEGRAM_CHAT_ID</code> from environment) - <code>parse_mode</code> (Optional[str]): The parsing mode for the message ('HTML' or 'Markdown', default is 'HTML')</p> <p>Returns: - <code>Dict[str, Any]</code>: Response from Telegram API containing message details</p> <p>Example: <pre><code>result = await telegram_tool.send_message(\n    text=\"&lt;b&gt;Important update&lt;/b&gt;: New features have been added to Nevron!\",\n    parse_mode=\"HTML\"\n)\n</code></pre></p>"},{"location":"agent/tools/telegram/#send_photo","title":"<code>send_photo</code>","text":"<p>Sends an image with optional caption to a Telegram chat.</p> <p>Arguments: - <code>photo_path</code> (str): Path to the image file - <code>caption</code> (Optional[str]): Caption for the image - <code>chat_id</code> (Optional[str]): The chat ID to send the message to (defaults to <code>TELEGRAM_CHAT_ID</code> from environment) - <code>parse_mode</code> (Optional[str]): The parsing mode for the caption ('HTML' or 'Markdown', default is 'HTML')</p> <p>Returns: - <code>Dict[str, Any]</code>: Response from Telegram API containing message details</p> <p>Example: <pre><code>result = await telegram_tool.send_photo(\n    photo_path=\"path/to/image.jpg\",\n    caption=\"&lt;i&gt;Chart showing recent performance&lt;/i&gt;\",\n    parse_mode=\"HTML\"\n)\n</code></pre></p>"},{"location":"agent/tools/telegram/#send_document","title":"<code>send_document</code>","text":"<p>Sends a document file to a Telegram chat.</p> <p>Arguments: - <code>document_path</code> (str): Path to the document file - <code>caption</code> (Optional[str]): Caption for the document - <code>chat_id</code> (Optional[str]): The chat ID to send the message to (defaults to <code>TELEGRAM_CHAT_ID</code> from environment) - <code>parse_mode</code> (Optional[str]): The parsing mode for the caption ('HTML' or 'Markdown', default is 'HTML')</p> <p>Returns: - <code>Dict[str, Any]</code>: Response from Telegram API containing message details</p> <p>Example: <pre><code>result = await telegram_tool.send_document(\n    document_path=\"path/to/report.pdf\",\n    caption=\"&lt;b&gt;Monthly Report&lt;/b&gt; - January 2024\",\n    parse_mode=\"HTML\"\n)\n</code></pre></p>"},{"location":"agent/tools/telegram/#implementation-details","title":"Implementation Details","text":"<p>The Telegram Tool is implemented in <code>src/tools/telegram_tool.py</code> and uses the <code>python-telegram-bot</code> library to interact with the Telegram API. The tool handles authentication, message formatting, and error handling to ensure reliable operation.</p>"},{"location":"agent/tools/telegram/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>The tool initializes with the bot token from environment variables</li> <li>The Telegram client is created with appropriate authentication</li> <li>API calls are made through the client with error handling</li> </ol>"},{"location":"agent/tools/telegram/#html-formatting","title":"HTML Formatting","text":"<p>The Telegram Tool supports HTML formatting for rich text messages. Supported tags include:</p> <ul> <li><code>&lt;b&gt;</code> and <code>&lt;/b&gt;</code> for bold text</li> <li><code>&lt;i&gt;</code> and <code>&lt;/i&gt;</code> for italic text</li> <li><code>&lt;u&gt;</code> and <code>&lt;/u&gt;</code> for underlined text</li> <li><code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> for strikethrough text</li> <li><code>&lt;a href=\"...\"&gt;</code> and <code>&lt;/a&gt;</code> for links</li> <li><code>&lt;code&gt;</code> and <code>&lt;/code&gt;</code> for monospace text</li> <li><code>&lt;pre&gt;</code> and <code>&lt;/pre&gt;</code> for pre-formatted text</li> </ul>"},{"location":"agent/tools/telegram/#best-practices","title":"Best Practices","text":"<ol> <li>Message Length: Keep messages under 4096 characters</li> <li>Media Handling: Supported formats include JPG, PNG, GIF, and PDF</li> <li>Rate Limits: Be mindful of Telegram API rate limits</li> <li>Content Guidelines: Ensure content complies with Telegram's terms of service</li> <li>Error Handling: Implement proper error handling in your agent's workflow</li> </ol>"},{"location":"agent/tools/telegram/#example-agent-workflow","title":"Example Agent Workflow","text":"<p>Here's an example of how an agent might use the Telegram Tool in a workflow:</p> <pre><code>async def telegram_report_workflow(agent, research_topic):\n    # Research the topic using Perplexity\n    research_results = await agent.tools.perplexity.search(research_topic)\n\n    # Generate a report based on research\n    report_content = await agent.llm.generate_content(\n        prompt=f\"Create a detailed report about {research_topic} based on this research: {research_results}\",\n    )\n\n    # Format the report with HTML\n    formatted_report = f\"&lt;b&gt;Research Report: {research_topic}&lt;/b&gt;\\n\\n{report_content}\"\n\n    # Send to Telegram\n    try:\n        result = await agent.tools.telegram.send_message(text=formatted_report)\n        return f\"Successfully sent report to Telegram\"\n    except Exception as e:\n        return f\"Failed to send report: {str(e)}\"\n</code></pre>"},{"location":"agent/tools/telegram/#limitations","title":"Limitations","text":"<ul> <li>Maximum message length of 4096 characters</li> <li>Media file size limits (10MB for photos, 50MB for files)</li> <li>Rate limiting by Telegram API</li> <li>Bot must be added to channels/groups before sending messages</li> </ul>"},{"location":"agent/tools/telegram/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li>Authentication Failed: Verify your bot token in the <code>.env</code> file</li> <li>Chat Not Found: Ensure the chat ID is correct and the bot is a member of the chat</li> <li>Permission Denied: Make sure the bot has permission to post in the channel/group</li> <li>Media Upload Failed: Check file formats and sizes</li> </ol> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/tools/twitter/","title":"Twitter (X) Tool","text":""},{"location":"agent/tools/twitter/#overview","title":"Overview","text":"<p>The Twitter (X) Tool enables Nevron agents to interact with the Twitter/X platform, allowing them to post tweets, create threads, and engage with the platform programmatically. This integration leverages the Twitter API v1.1 to provide a seamless experience for autonomous agents.</p>"},{"location":"agent/tools/twitter/#features","title":"Features","text":"<ul> <li>Post Tweets: Create and publish tweets with text content</li> <li>Media Support: Attach images to tweets</li> <li>Thread Creation: Create multi-tweet threads for longer content</li> <li>Error Handling: Robust error handling for API rate limits and other issues</li> </ul>"},{"location":"agent/tools/twitter/#configuration","title":"Configuration","text":"<p>To use the Twitter Tool, you need to configure the following environment variables in your <code>.env</code> file:</p> <pre><code>TWITTER_API_KEY=your_api_key\nTWITTER_API_SECRET_KEY=your_api_secret_key\nTWITTER_ACCESS_TOKEN=your_access_token\nTWITTER_ACCESS_TOKEN_SECRET=your_access_token_secret\n</code></pre>"},{"location":"agent/tools/twitter/#how-to-obtain-twitter-api-credentials","title":"How to Obtain Twitter API Credentials","text":"<ol> <li>Create a developer account at Twitter Developer Portal</li> <li>Create a new project and app</li> <li>Apply for Elevated access to use the v1.1 API (required for posting)</li> <li>Generate consumer keys (API key and secret) and access tokens in the app settings</li> <li>Ensure your app has Read and Write permissions</li> </ol>"},{"location":"agent/tools/twitter/#methods","title":"Methods","text":""},{"location":"agent/tools/twitter/#post_tweet","title":"<code>post_tweet</code>","text":"<p>Posts a single tweet to Twitter.</p> <p>Arguments: - <code>text</code> (str): The content of the tweet (max 280 characters) - <code>media_paths</code> (Optional[List[str]]): List of paths to media files to attach (optional)</p> <p>Returns: - <code>Dict[str, Any]</code>: Response from Twitter API containing tweet details</p> <p>Example: <pre><code>result = await twitter_tool.post_tweet(\n    text=\"Exploring the latest developments in AI with #Nevron! Check out our autonomous agent framework: https://github.com/axioma-ai-labs/nevron\",\n)\n</code></pre></p>"},{"location":"agent/tools/twitter/#create_thread","title":"<code>create_thread</code>","text":"<p>Creates a thread of multiple tweets.</p> <p>Arguments: - <code>tweets</code> (List[str]): List of tweet texts to post as a thread - <code>media_paths</code> (Optional[List[str]]): List of paths to media files to attach to the first tweet (optional)</p> <p>Returns: - <code>List[Dict[str, Any]]</code>: List of responses from Twitter API for each tweet in the thread</p> <p>Example: <pre><code>thread_tweets = [\n    \"1/4 Introducing Nevron: An open-source framework for building autonomous AI agents in Python.\",\n    \"2/4 Nevron features modular architecture with planning, memory, feedback, and execution components.\",\n    \"3/4 Integrate with various services like Twitter, Discord, Telegram, and more using our tool system.\",\n    \"4/4 Get started today: https://github.com/axioma-ai-labs/nevron #AI #Agents #Python\"\n]\n\nresults = await twitter_tool.create_thread(tweets=thread_tweets)\n</code></pre></p>"},{"location":"agent/tools/twitter/#implementation-details","title":"Implementation Details","text":"<p>The Twitter Tool is implemented in <code>src/tools/twitter_tool.py</code> and uses the <code>tweepy</code> library to interact with the Twitter API. The tool handles authentication, rate limiting, and error handling to ensure reliable operation.</p>"},{"location":"agent/tools/twitter/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>The tool initializes with API credentials from environment variables</li> <li>Tweepy client is created with appropriate authentication</li> <li>API calls are made through the client with error handling</li> </ol>"},{"location":"agent/tools/twitter/#error-handling","title":"Error Handling","text":"<p>The tool implements comprehensive error handling for common Twitter API issues:</p> <ul> <li>Rate limiting: Implements exponential backoff</li> <li>Authentication errors: Provides clear error messages</li> <li>Content policy violations: Reports specific violation details</li> <li>Network issues: Implements retries with timeout</li> </ul>"},{"location":"agent/tools/twitter/#best-practices","title":"Best Practices","text":"<ol> <li>Character Limits: Keep tweets under 280 characters</li> <li>Media Handling: Supported formats include JPG, PNG, GIF, and MP4</li> <li>Rate Limits: Be mindful of Twitter API rate limits (300 tweets per 3 hours)</li> <li>Content Guidelines: Ensure content complies with Twitter's terms of service</li> <li>Error Handling: Implement proper error handling in your agent's workflow</li> </ol>"},{"location":"agent/tools/twitter/#example-agent-workflow","title":"Example Agent Workflow","text":"<p>Here's an example of how an agent might use the Twitter Tool in a workflow:</p> <pre><code>async def twitter_workflow(agent, topic):\n    # Research the topic using Perplexity\n    research_results = await agent.tools.perplexity.search(topic)\n\n    # Generate a tweet based on research\n    tweet_content = await agent.llm.generate_content(\n        prompt=f\"Create a tweet about {topic} based on this research: {research_results}\",\n        max_length=280\n    )\n\n    # Post to Twitter\n    try:\n        result = await agent.tools.twitter.post_tweet(text=tweet_content)\n        return f\"Successfully posted tweet: {result['data']['id']}\"\n    except Exception as e:\n        return f\"Failed to post tweet: {str(e)}\"\n</code></pre>"},{"location":"agent/tools/twitter/#limitations","title":"Limitations","text":"<ul> <li>Requires Twitter API v1.1 access (Elevated access)</li> <li>Subject to Twitter API rate limits</li> <li>Media uploads limited to 4 per tweet</li> <li>No support for Twitter Spaces or other advanced features</li> </ul>"},{"location":"agent/tools/twitter/#troubleshooting","title":"Troubleshooting","text":"<p>Common issues and their solutions:</p> <ol> <li>Authentication Failed: Verify your API credentials in the <code>.env</code> file</li> <li>Rate Limit Exceeded: Reduce posting frequency or implement longer delays</li> <li>Media Upload Failed: Check file formats and sizes (under 5MB for images)</li> <li>Tweet Creation Failed: Ensure content doesn't violate Twitter's policies</li> </ol> <p>For more help, visit our GitHub Discussions.</p>"},{"location":"agent/tools/whatsapp/","title":"WhatsApp Integration","text":""},{"location":"agent/tools/whatsapp/#setup","title":"Setup","text":"<ol> <li>Create WhatsApp API Account</li> <li>Go to Green API</li> <li>Register and create an account</li> <li>Create an instance</li> <li> <p>Get your Instance ID and API Token</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>WHATSAPP_ID_INSTANCE=your_instance_id_here\nWHATSAPP_API_TOKEN=your_api_token_here\n</code></pre></p> </li> <li> <p>Phone Number Setup</p> </li> <li>Install WhatsApp on your phone</li> <li>Scan QR code from Green API dashboard</li> <li>Verify your phone number is connected</li> </ol>"},{"location":"agent/tools/whatsapp/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.whatsapp import WhatsAppClient\n\n# Initialize WhatsApp client\nwhatsapp = WhatsAppClient()\nawait whatsapp.initialize()\n\n# Define message handler\nasync def handle_message(message_data):\n    if message_data['content'].lower() == 'hello':\n        await whatsapp.send_message(\n            recipient_id=message_data['phone'],\n            content=\"Hello! How can I help you?\"\n        )\n\n# Start listening for messages\nawait whatsapp.listen_to_messages(callback=handle_message)\n</code></pre>"},{"location":"agent/tools/whatsapp/#example-usage","title":"Example Usage","text":"<p>A WhatsApp bot that handles incoming messages:</p> <pre><code># Send a message\nmessage_id = await whatsapp.send_message(\n    recipient_id=\"1234567890\",\n    content=\"Hello from Nevron!\"\n)\n\n# Format phone numbers\nformatted_number = format_phone_number(\"+1-234-567-890\")  # Returns \"1234567890@c.us\"\n</code></pre> <p>This example demonstrates: - Message sending - Message listening - Phone number formatting - Basic interaction flow</p>"},{"location":"agent/tools/whatsapp/#features","title":"Features","text":"<ul> <li>Send messages to WhatsApp users</li> <li>Listen for incoming messages</li> <li>Format incorrect phone numbers for API automatically</li> <li>Handle message notifications</li> <li>Automatic notification cleanup from the queue</li> <li>Error handling and logging</li> <li>SSL verification management (currently disabled)</li> </ul>"},{"location":"agent/tools/whatsapp/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for media messages</li> <li>Implement group chat functionality</li> <li>Enable message reactions and interactive message components.</li> <li>Add message status tracking</li> <li>Support for business features</li> <li>Add contact management</li> <li>Implement message templates</li> <li>Add support for voice messages</li> <li>Enable message scheduling</li> </ul>"},{"location":"agent/tools/whatsapp/#reference","title":"Reference","text":"<p>For implementation details, see: tools/whatsapp.py</p>"},{"location":"agent/tools/workflows/","title":"Workflows","text":""},{"location":"agent/tools/workflows/#overview","title":"Overview","text":"<p>Workflows are the backbone of Nevron\u2019s logic, defining tasks and coordinating tools and modules to deliver actionable outcomes. Each workflow is designed for autonomy and efficiency, allowing the agent to function in diverse scenarios seamlessly.</p>"},{"location":"agent/tools/workflows/#available-workflows","title":"Available Workflows","text":""},{"location":"agent/tools/workflows/#1-analyze-signal","title":"1. Analyze Signal","text":"<p>Focuses on processing actionable signals, such as market trends or data feeds, and publishing insights through communication channels like Twitter.</p> <p>Features: - Fetches and validates signals. - Analyzes data using a Large Language Model (LLM). - Publishes concise updates (e.g., tweets).</p> <p>Location: <code>src/workflows/analyze_signal.py</code></p>"},{"location":"agent/tools/workflows/#2-research-news","title":"2. Research News","text":"<p>Specializes in gathering, analyzing, and summarizing news content for dissemination.</p> <p>Features: - Collects and validates news articles. - Uses LLMs for analysis and contextualization. - Publishes summaries through predefined channels.</p> <p>Location: <code>src/workflows/research_news.py</code></p>"},{"location":"agent/tools/workflows/#workflow-architecture","title":"Workflow Architecture","text":"<p>Workflows in Nevron follow a modular design to ensure scalability and consistency:</p> <ol> <li>Input Validation: Ensures data completeness.</li> <li>Core Execution: Processes the workflow logic.</li> <li>Result Aggregation: Collects outputs for further use.</li> <li>Memory Storage: Saves results for future reference.</li> <li>Feedback Integration: Logs outcomes for iterative learning.</li> </ol>"},{"location":"agent/tools/workflows/#integration-points","title":"Integration Points","text":"<p>Workflows rely on Nevron\u2019s modular components for execution:</p> <ul> <li>Tools:</li> <li>Signal fetching for data processing.</li> <li>Automated publishing to Twitter.</li> <li> <p>News gathering for insights.</p> </li> <li> <p>Modules:</p> </li> <li>Planning Module: Guides decision-making.</li> <li>Memory Module: Provides context from past events.</li> <li>LLM Integration: Drives analysis and content generation.</li> <li>Feedback Module: Tracks performance for improvement.</li> </ul>"},{"location":"agent/tools/workflows/#how-to-add-a-new-workflow","title":"How to Add a New Workflow?","text":"<ol> <li>Create a New File:</li> <li> <p>Add a Python file in <code>src/workflows/</code> (e.g., <code>new_workflow.py</code>).</p> </li> <li> <p>Define the Workflow Class:    <pre><code>from src.workflows.base import BaseWorkflow\n\nclass NewWorkflow(BaseWorkflow):\n    name = \"new_workflow\"\n</code></pre></p> </li> <li> <p>Implement Logic:    <pre><code>def execute(self):\n    # Core workflow functionality\n    logger.info(\"Executing workflow\")\n\ndef validate(self):\n    # Input validation logic\n    logger.info(\"Validating workflow input\")\n</code></pre></p> </li> <li> <p>Register Workflow:    <pre><code>from src.workflows.new_workflow import NewWorkflow\nagent.register_workflow(NewWorkflow())\n</code></pre></p> </li> <li> <p>Test:</p> <ul> <li>Write unit tests for components.</li> <li>Validate integration with the agent.</li> <li>Test edge cases and error handling.</li> </ul> </li> </ol>"},{"location":"agent/tools/workflows/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Error Handling:</p> <ul> <li>Anticipate edge cases.</li> <li>Provide clear error messages for debugging.</li> </ul> </li> <li> <p>Logging:</p> <ul> <li>Log critical workflow steps for monitoring.</li> <li>Track execution times and performance metrics.</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Unit tests for components.</li> <li>Integration tests to verify module and tool interactions.</li> <li>Simulate failure scenarios for robustness.</li> </ul> </li> </ol> <p>Nevron\u2019s workflows enable seamless automation and intelligent decision-making, forming the core of its adaptability and efficiency. For further details, refer to the GitHub Discussions.</p>"},{"location":"agent/tools/youtube/","title":"YouTube Integration","text":""},{"location":"agent/tools/youtube/#setup","title":"Setup","text":"<ol> <li>Enable YouTube Data API</li> <li>Go to Google Cloud Console</li> <li>Navigate to APIs &amp; Services &gt; Library</li> <li> <p>Search for and enable \"YouTube Data API v3\"</p> </li> <li> <p>Generate API Key</p> </li> <li>Go to APIs &amp; Services &gt; Credentials</li> <li>Click \"Create Credentials\" &gt; \"API key\"</li> <li>Copy the generated API key</li> <li> <p>(Optional) Restrict the API key to YouTube Data API v3 only</p> </li> <li> <p>Configure Environment Variables    Add these to your <code>.env</code> file:    <pre><code>YOUTUBE_API_KEY=your_api_key_here\nYOUTUBE_CHANNEL_ID=your_channel_id\n</code></pre></p> </li> </ol>"},{"location":"agent/tools/youtube/#basic-setup","title":"Basic Setup","text":"<pre><code>from src.tools.youtube import YouTubeClient\n\n# Initialize YouTube client\nyoutube = YouTubeClient()\n\n# Search for videos\nresults = youtube.search_videos(\n    query=\"Python programming\",\n    max_results=5\n)\n\n# Get video details\nvideo_details = youtube.get_video_details(\n    video_id=\"video_id_here\"\n)\n</code></pre>"},{"location":"agent/tools/youtube/#features","title":"Features","text":"<ul> <li>Search for videos with custom queries and filters</li> <li>Retrieve detailed video information (title, description, statistics)</li> <li>Access video comments and engagement metrics</li> <li>Get channel statistics and details</li> <li>Fetch playlist information</li> <li>Efficient caching of API client</li> </ul>"},{"location":"agent/tools/youtube/#todos-for-future-enhancements","title":"TODOs for Future Enhancements:","text":"<ul> <li>Add support for video uploads and management</li> <li>Implement live streaming capabilities</li> <li>Add caption/subtitle handling</li> <li>Support for video analytics and reporting</li> <li>Implement playlist creation and management</li> <li>Add support for channel management</li> <li>Implement comment moderation features</li> <li>Add support for video categories and tags</li> </ul>"},{"location":"agent/tools/youtube/#reference","title":"Reference","text":"<p>For implementation details, see: <code>src/tools/youtube.py</code></p> <p>The implementation uses the official YouTube Data API v3. For more information, refer to: - YouTube Data API Documentation - Google API Client Library for Python</p>"},{"location":"development/contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to Nevron! This guide will help you get started with contributing to our project.</p>"},{"location":"development/contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Ways to Contribute</li> <li>Development Process</li> <li>Code Style and Standards</li> <li>Pull Request Guidelines</li> <li>Getting Help</li> </ul>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ol> <li>Code Contributions</li> <li>Bug fixes</li> <li>New features</li> <li>Performance improvements</li> <li> <p>Documentation improvements</p> </li> <li> <p>Non-Code Contributions</p> </li> <li>Reporting bugs</li> <li>Suggesting enhancements</li> <li>Improving documentation</li> <li>Answering questions in discussions</li> </ol>"},{"location":"development/contributing/#development-process","title":"Development Process","text":"<ol> <li>Find or Create an Issue</li> <li>Check existing issues</li> <li>Look for <code>good first issue</code> or <code>help wanted</code> labels</li> <li> <p>If you want to work on something new, create an issue first to discuss it</p> </li> <li> <p>Fork and Clone <pre><code>git clone https://github.com/YOUR-USERNAME/nevron.git\ncd nevron\n</code></pre></p> </li> <li> <p>Create a Branch</p> </li> <li>Create a new branch for your work:     <pre><code>git checkout -b feature/issue-number-description\n# Example: feature/42-add-redis-cache\n</code></pre></li> <li> <p>Use meaningful branch names, preferably referencing the issue number</p> </li> <li> <p>Make Changes</p> </li> <li>Write your code</li> <li>Follow the project's coding standards</li> <li>Keep commits atomic and write meaningful commit messages</li> <li> <p>Test your changes thoroughly</p> </li> <li> <p>Test Your Changes</p> </li> <li>Run formatting checks:      <pre><code>make format\n</code></pre></li> <li>Run linting checks:      <pre><code>make lint\n</code></pre></li> <li> <p>Run tests:      <pre><code>make test\n</code></pre></p> </li> <li> <p>Push and Create PR</p> </li> <li>Push your changes to your fork:      <pre><code>git push origin feature/issue-number-description\n</code></pre></li> <li>Then create a Pull Request on GitHub.</li> </ol>"},{"location":"development/contributing/#code-style-and-standards","title":"Code Style and Standards","text":""},{"location":"development/contributing/#python-standards","title":"Python Standards","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Follow PEP 484 for type hints</li> <li>Follow PEP 257 for docstrings</li> <li>Use Python 3.12+ features and patterns</li> </ul>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Use descriptive names that reflect purpose</li> <li>Variables and functions: <code>snake_case</code></li> <li>Classes: <code>PascalCase</code></li> <li>Constants: <code>UPPER_CASE_WITH_UNDERSCORES</code></li> <li>Private attributes/methods: prefix with single underscore <code>_private_method</code></li> <li>\"Magic\" methods: surrounded by double underscores <code>__str__</code></li> <li>Type variable names: <code>PascalCase</code> preferably single letters (T, K, V)</li> </ul>"},{"location":"development/contributing/#code-organization","title":"Code Organization","text":"<ul> <li>One class per file unless classes are closely related</li> <li>Group related functionality into modules</li> <li>Use absolute imports</li> <li>Order imports as: standard library, third-party, local</li> <li>Use <code>isort</code> for import sorting</li> <li>Maximum line length: 100 characters</li> <li>Use 4 spaces for indentation (no tabs)</li> </ul>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>All public APIs must have docstrings</li> <li>Use Google-style docstring format:   <pre><code>def function_name(param1: str, param2: int) -&gt; bool:\n    \"\"\"Short description of function.\n\n    Longer description if needed.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: Description of when this error occurs\n    \"\"\"\n</code></pre></li> <li>Include type hints for all function arguments and return values</li> <li>Document exceptions that may be raised</li> <li>Keep comments focused on why, not what</li> <li>Update documentation when changing code</li> </ul>"},{"location":"development/contributing/#code-quality","title":"Code Quality","text":"<ul> <li>Keep functions small and focused (preferably under 50 lines)</li> <li>Maximum function arguments: 5</li> <li>Use early returns to reduce nesting</li> <li>Avoid global variables</li> <li>Use constants instead of magic numbers</li> <li>Handle all possible exceptions appropriately</li> <li>Use context managers (<code>with</code> statements) for resource management</li> <li>Use f-strings for string formatting</li> <li>Use list/dict/set comprehensions when they improve readability</li> </ul>"},{"location":"development/contributing/#testing-standards","title":"Testing Standards","text":"<ul> <li>Write tests for all new code</li> <li>Maintain minimum 90% test coverage</li> <li>Follow Arrange-Act-Assert pattern</li> <li>Use meaningful test names that describe the scenario</li> <li>One assertion per test when possible</li> <li>Use pytest fixtures for common setup</li> <li>Mock external dependencies</li> <li>Test edge cases and error conditions</li> </ul>"},{"location":"development/contributing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use appropriate data structures</li> <li>Avoid unnecessary object creation</li> <li>Use generators for large datasets</li> <li>Profile code when performance is critical</li> <li>Consider memory usage</li> <li>Use <code>collections</code> module specialized containers when appropriate</li> </ul>"},{"location":"development/contributing/#security-best-practices","title":"Security Best Practices","text":"<ul> <li>Never commit sensitive data (API keys, passwords)</li> <li>Use environment variables for configuration</li> <li>Validate all input data</li> <li>Use secure defaults</li> <li>Follow OWASP security guidelines</li> <li>Use <code>secrets</code> module for cryptographic operations</li> </ul>"},{"location":"development/contributing/#version-control","title":"Version Control","text":"<ul> <li>Write meaningful commit messages</li> <li>One logical change per commit</li> <li>Reference issue numbers in commits</li> <li>Keep commits small and focused</li> <li>Rebase feature branches on main before PR</li> </ul>"},{"location":"development/contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"development/contributing/#pr-title-format","title":"PR Title Format","text":"<p>Title should be concise and descriptive.</p>"},{"location":"development/contributing/#pr-description-should-include","title":"PR Description Should Include","text":"<ul> <li>Reference to related issue(s)</li> <li>Clear description of changes</li> <li>Breaking changes (if any)</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks must pass</li> <li>At least one maintainer approval required</li> <li>All review comments must be resolved</li> <li>Documentation must be updated</li> </ol>"},{"location":"development/contributing/#merging","title":"Merging","text":"<ul> <li>Always squash commits before merging</li> <li>Merge into <code>dev</code> branch first</li> <li>Once <code>dev</code> is stable, merge into <code>main</code> (done by maintainers)</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Join our Discussions</li> <li>Ask questions in issue comments</li> <li>Tag maintainers if stuck</li> </ul> <p>Remember: No contribution is too small, and all contributions are valued! </p>"},{"location":"development/setup/","title":"Development Setup","text":"<p>This guide provides detailed instructions for setting up a development environment for Nevron.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+ (required for the latest features and optimizations)</li> <li>Poetry (for dependency management)</li> <li>Make (for using Makefile commands)</li> <li>Git (for version control)</li> <li>Docker (optional, for containerized development)</li> </ul>"},{"location":"development/setup/#initial-setup","title":"Initial Setup","text":""},{"location":"development/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/axioma-ai-labs/nevron.git\ncd nevron\n</code></pre>"},{"location":"development/setup/#2-install-poetry","title":"2. Install Poetry","text":"<p>If you haven't installed Poetry yet:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Make sure to add Poetry to your PATH according to the installation instructions.</p>"},{"location":"development/setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code>make deps\n</code></pre> <p>This will: - Install all project dependencies using Poetry - Set up pre-commit hooks for code quality - Configure the development environment</p>"},{"location":"development/setup/#environment-configuration","title":"Environment Configuration","text":""},{"location":"development/setup/#basic-configuration","title":"Basic Configuration","text":"<p>Create a development environment file:</p> <pre><code>cp .env.dev .env\n</code></pre>"},{"location":"development/setup/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>You have several options for configuring the LLM provider:</p>"},{"location":"development/setup/#option-1-use-a-third-party-llm-api","title":"Option 1: Use a Third-Party LLM API","text":"<p>Edit your <code>.env</code> file to include one of these LLM providers:</p> <pre><code># OpenAI\nOPENAI_API_KEY=your_key_here\nLLM_PROVIDER=openai\n\n# Anthropic\nANTHROPIC_API_KEY=your_key_here\nLLM_PROVIDER=anthropic\n\n# xAI (Grok)\nXAI_API_KEY=your_key_here\nLLM_PROVIDER=xai\n\n# DeepSeek\nDEEPSEEK_API_KEY=your_key_here\nLLM_PROVIDER=deepseek\n\n# Qwen\nQWEN_API_KEY=your_key_here\nLLM_PROVIDER=qwen\n\n# Venice\nVENICE_API_KEY=your_key_here\nLLM_PROVIDER=venice\n</code></pre>"},{"location":"development/setup/#option-2-use-llama-via-api-services","title":"Option 2: Use Llama via API Services","text":"<pre><code># Standard Llama API\nLLAMA_PROVIDER=llama-api\nLLAMA_API_KEY=your_key_here\nLLM_PROVIDER=llama\n\n# OpenRouter\nLLAMA_PROVIDER=openrouter\nLLAMA_API_KEY=your_key_here\nLLM_PROVIDER=llama\n\n# Fireworks\nLLAMA_PROVIDER=fireworks\nLLAMA_API_KEY=your_key_here\nLLM_PROVIDER=llama\n</code></pre>"},{"location":"development/setup/#option-3-run-ollama-locally-recommended-for-development","title":"Option 3: Run Ollama Locally (Recommended for Development)","text":"<ol> <li> <p>Install Ollama following the instructions at ollama.ai</p> </li> <li> <p>Pull a small model: <pre><code>ollama pull llama3:8b-instruct\n</code></pre></p> </li> <li> <p>Configure your <code>.env</code> file: <pre><code>LLM_PROVIDER=llama\nLLAMA_PROVIDER=ollama\nLLAMA_OLLAMA_MODEL=llama3:8b-instruct\n</code></pre></p> </li> </ol>"},{"location":"development/setup/#memory-configuration","title":"Memory Configuration","text":"<p>For development, you can use either ChromaDB (default) or Qdrant:</p> <pre><code># ChromaDB (lightweight, file-based)\nMEMORY_BACKEND_TYPE=chroma\n\n# Qdrant (requires running Qdrant server)\nMEMORY_BACKEND_TYPE=qdrant\nMEMORY_HOST=localhost\nMEMORY_PORT=6333\n</code></pre>"},{"location":"development/setup/#running-the-application","title":"Running the Application","text":""},{"location":"development/setup/#start-the-development-server","title":"Start the Development Server","text":"<pre><code>make run\n</code></pre>"},{"location":"development/setup/#run-tests","title":"Run Tests","text":"<pre><code>make test\n</code></pre>"},{"location":"development/setup/#lint-code","title":"Lint Code","text":"<pre><code>make lint\n</code></pre>"},{"location":"development/setup/#format-code","title":"Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"development/setup/#ide-setup","title":"IDE Setup","text":""},{"location":"development/setup/#vscode","title":"VSCode","text":"<p>Recommended extensions: - Python - Ruff - isort - GitLens - Poetry</p> <p>Recommended settings (<code>settings.json</code>): <pre><code>{\n    \"python.formatting.provider\": \"ruff\",\n    \"editor.formatOnSave\": true,\n    \"python.linting.enabled\": true,\n    \"python.linting.mypyEnabled\": true\n}\n</code></pre></p>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ul> <li>Enable Ruff formatter</li> <li>Set Python interpreter to the Poetry environment</li> <li>Enable mypy for type checking</li> <li>Configure Poetry as the package manager</li> </ul>"},{"location":"development/setup/#troubleshooting","title":"Troubleshooting","text":"<p>Common development issues:</p> <ul> <li> <p>Python Version: Ensure you're using Python 3.13+   <pre><code>python --version\n</code></pre></p> </li> <li> <p>Poetry Environment: Verify Poetry has created the virtual environment   <pre><code>poetry env info\n</code></pre></p> </li> <li> <p>Dependencies: Check installed packages   <pre><code>poetry show\n</code></pre></p> </li> <li> <p>Ollama: If using Ollama, verify it's running   <pre><code>ollama list\n</code></pre></p> </li> <li> <p>Environment Variables: Ensure your <code>.env</code> file contains the necessary configuration   <pre><code>cat .env | grep -v \"^#\" | grep .\n</code></pre></p> </li> <li> <p>Logs: Check the application logs for detailed error messages   <pre><code>tail -f logs/app.log\n</code></pre></p> </li> </ul> <p>For more help, visit our GitHub Discussions or open an issue on the repository.</p>"},{"location":"development/workflow/","title":"Development Workflow","text":""},{"location":"development/workflow/#code-style","title":"Code Style","text":"<p>Follow the Code Style and Standards section.</p>"},{"location":"development/workflow/#testing","title":"Testing","text":""},{"location":"development/workflow/#writing-tests","title":"Writing Tests","text":"<ul> <li>Place tests in the <code>tests/</code> directory</li> <li>Follow the same directory structure as the source code</li> <li>Name test files with <code>test_</code> prefix</li> <li>Use pytest fixtures for common setup</li> <li>Aim for 100% test coverage for new code</li> </ul>"},{"location":"development/workflow/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npipenv run pytest tests/path/to/test_file.py\n\n# Run with coverage\nmake coverage\n</code></pre>"},{"location":"development/workflow/#debugging","title":"Debugging","text":""},{"location":"development/workflow/#local-debugging","title":"Local Debugging","text":"<ol> <li> <p>Use Python debugger (pdb): <pre><code>import pdb; pdb.set_trace()\n</code></pre></p> </li> <li> <p>VSCode debugging:</p> </li> <li>Set breakpoints in the editor</li> <li>Use the Run and Debug panel</li> <li>Configure <code>launch.json</code> for your specific needs</li> </ol>"},{"location":"development/workflow/#logging","title":"Logging","text":"<ul> <li>Use the loguru module</li> <li>Configure log levels appropriately (debug, info, warning, error, critical)</li> </ul>"},{"location":"development/workflow/#github-workflows","title":"GitHub Workflows","text":"<p>The project uses several GitHub Actions workflows:</p>"},{"location":"development/workflow/#main-workflow-mainyml","title":"Main Workflow (<code>main.yml</code>)","text":"<ul> <li>Runs on every push and pull request</li> <li>Performs linting and testing</li> <li>Checks code formatting</li> <li>Generates coverage report</li> </ul>"},{"location":"development/workflow/#documentation-workflow-deploy-docsyml","title":"Documentation Workflow (<code>deploy-docs.yml</code>)","text":"<ul> <li>Builds and deploys documentation to GitHub Pages</li> <li>Triggers on pushes to main branch</li> <li>Uses MkDocs Material theme</li> </ul>"},{"location":"development/workflow/#docker-workflow-dockeryml","title":"Docker Workflow (<code>docker.yml</code>)","text":"<ul> <li>Builds and pushes Docker images</li> <li>Runs on releases and main branch pushes</li> <li>Tags images appropriately</li> </ul>"},{"location":"development/workflow/#release-process","title":"Release Process","text":"<p>The maintainer reviews and publishes releases manually!</p>"}]}